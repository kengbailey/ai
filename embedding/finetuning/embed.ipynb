{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget '<https://arxiv.org/pdf/2402.04177.pdf>'  -O \"Scaling_Laws_for_Downstream_Task_Performance_of_Large_Language_Models.pdf\"\n",
    "!wget '<https://arxiv.org/pdf/2403.06563.pdf>' -O \"Unraveling_the_Mystery_of_Scaling_Laws.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "%pip install accelerate\n",
    "%pip install -i https://pypi.org/simple/ bitsandbytes\n",
    "%pip install sentence-transformers\n",
    "\n",
    "%pip install llama-index llama-index-llms-huggingface llama-index-embeddings-huggingface llama-index-readers-web\n",
    "\n",
    "%pip install llama-index-embeddings-openai\n",
    "%pip install llama-index-embeddings-adapter\n",
    "%pip install llama-index-finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Download PDF Files\n",
    "training_file_name = \"lyft_2021.pdf\"\n",
    "validation_file_name = \"uber_2021.pdf\"\n",
    "\n",
    "working_dir = \"./\"\n",
    "\n",
    "# !wget 'https://arxiv.org/pdf/2402.04177.pdf'  -O \"Scaling_Laws_for_Downstream_Task_Performance_of_Large_Language_Models.pdf\"\n",
    "# !wget 'https://arxiv.org/pdf/2403.06563.pdf' -O \"Unraveling_the_Mystery_of_Scaling_Laws.pdf\"\n",
    "\n",
    "TRAIN_FILES = [os.path.join(working_dir, training_file_name)]\n",
    "TRAIN_CORPUS_FPATH = \"./train_corpus.json\"\n",
    "\n",
    "VAL_FILES = [os.path.join(working_dir, validation_file_name)]\n",
    "VAL_CORPUS_FPATH = \"./val_corpus.json\"\n",
    "\n",
    "print(f\"Train files: {TRAIN_FILES}\")\n",
    "print(f\"Val files: {VAL_FILES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "def load_corpus(files):\n",
    "    reader = SimpleDirectoryReader(input_files=files)\n",
    "    docs = reader.load_data()\n",
    "    parser = SentenceSplitter()\n",
    "    nodes = parser.get_nodes_from_documents(docs, show_progress=True)\n",
    "    print(f\"Parsed {len(nodes)} nodes\")\n",
    "    return nodes\n",
    "\n",
    "train_nodes = load_corpus(TRAIN_FILES)\n",
    "val_nodes = load_corpus(VAL_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_conf = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "#We recreate template (or messages) and ensure that they have the correct format, as per,\n",
    "#https://github.com/run-llama/llama_index/issues/9277#issuecomment-1837545398 for zephyr-7b-beta\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        if message.role == 'system':\n",
    "            prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "        elif message.role == 'user':\n",
    "            prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "        elif message.role == 'assistant':\n",
    "            prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "\n",
    "    # ensure we start with a system prompt, insert blank if needed\n",
    "    if not prompt.startswith(\"<|system|>\\n\"):\n",
    "        prompt = \"<|system|>\\n</s>\\n\" + prompt\n",
    "\n",
    "    # add final assistant prompt\n",
    "    prompt = prompt + \"<|assistant|>\\n\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load synthetic data generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huggingface_llm(model_name=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "                    tokenizer_name=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "                    context_window=3900,\n",
    "                    max_new_tokens=256,\n",
    "                    quantization_config = quantization_conf\n",
    "                   ):\n",
    "    llm = HuggingFaceLLM(\n",
    "        model_name=model_name,\n",
    "        tokenizer_name=tokenizer_name,\n",
    "        query_wrapper_prompt=PromptTemplate(\"<|system|>\\n</s>\\n<|user|>\\n{query_str}</s>\\n<|assistant|>\\n\"),\n",
    "        context_window=context_window,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        model_kwargs={\"quantization_config\": quantization_config},\n",
    "        # tokenizer_kwargs={},\n",
    "        generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "        messages_to_prompt=messages_to_prompt,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    return llm\n",
    "\n",
    "llm = huggingface_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic data - Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "\n",
    "train_dataset = generate_qa_embedding_pairs(train_nodes, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune embedding model w/ synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.finetuning import EmbeddingAdapterFinetuneEngine\n",
    "from llama_index.core.embeddings import resolve_embed_model\n",
    "import torch\n",
    "\n",
    "def embedding_model(model=\"local:avsolatorio/GIST-large-Embedding-v0\",\n",
    "                    model_output_path=\"model_output_test\",\n",
    "                    bias=True,\n",
    "                    no_of_epochs=4,\n",
    "                    verbose=True,\n",
    "                    optimizer=torch.optim.AdamW,\n",
    "                    optimizer_params={\"lr\": 0.01}\n",
    "                   ):\n",
    "\n",
    "    base_embed_model = resolve_embed_model(model)\n",
    "    finetune_engine = EmbeddingAdapterFinetuneEngine(\n",
    "        train_dataset,\n",
    "        base_embed_model,\n",
    "        model_output_path=model_output_path,\n",
    "        bias=bias,\n",
    "        epochs=no_of_epochs,\n",
    "        verbose=verbose,\n",
    "        optimizer_class=optimizer,\n",
    "        optimizer_params=optimizer_params\n",
    "    )\n",
    "\n",
    "    return finetune_engine\n",
    "\n",
    "finetune_engine = embedding_model()\n",
    "finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_embed_model = finetune_engine.get_finetuned_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic data - Evaluation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = generate_qa_embedding_pairs(val_nodes, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Embedding Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.schema import TextNode\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    dataset,\n",
    "    embed_model,\n",
    "    top_k=5,\n",
    "    verbose=False,\n",
    "):\n",
    "    corpus = dataset.corpus\n",
    "    queries = dataset.queries\n",
    "    relevant_docs = dataset.relevant_docs\n",
    "\n",
    "    nodes = [TextNode(id_=id_, text=text) for id_, text in corpus.items()]\n",
    "    index = VectorStoreIndex(\n",
    "        nodes, embed_model=embed_model, show_progress=True\n",
    "    )\n",
    "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
    "\n",
    "    eval_results = []\n",
    "    for query_id, query in tqdm(queries.items()):\n",
    "        retrieved_nodes = retriever.retrieve(query)\n",
    "        retrieved_ids = [node.node.node_id for node in retrieved_nodes]\n",
    "        expected_id = relevant_docs[query_id][0]\n",
    "        is_hit = expected_id in retrieved_ids  # assume 1 relevant doc\n",
    "\n",
    "        eval_result = {\n",
    "            \"is_hit\": is_hit,\n",
    "            \"retrieved\": retrieved_ids,\n",
    "            \"expected\": expected_id,\n",
    "            \"query\": query_id,\n",
    "        }\n",
    "        eval_results.append(eval_result)\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open AI ADA Embedding Model(Gold Standard)\n",
    "\n",
    "ada = OpenAIEmbedding(api_key=\"sk-\")\n",
    "ada_val_results = evaluate(val_dataset, ada)\n",
    "\n",
    "df_ada = pd.DataFrame(ada_val_results)\n",
    "hit_rate_ada = df_ada[\"is_hit\"].mean()\n",
    "hit_rate_ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pretained GIST Embedding Model\n",
    "\n",
    "GIST_model = \"local:avsolatorio/GIST-large-Embedding-v0\"\n",
    "GIST_val_results = evaluate(val_dataset, GIST_model)\n",
    "df_embed_models = pd.DataFrame(GIST_val_results)\n",
    "hit_rate_bge = df_embed_models[\"is_hit\"].mean()\n",
    "hit_rate_bge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fine Tuned GIST Embedding Model\n",
    "\n",
    "embed_model = finetune_engine.get_finetuned_model()\n",
    "\n",
    "val_results_finetuned = evaluate(val_dataset, embed_model)\n",
    "df_embed_models_finetuned = pd.DataFrame(val_results_finetuned)\n",
    "hit_rate_bge_finetuned = df_embed_models[\"is_hit\"].mean()\n",
    "hit_rate_bge_finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Results\n",
    "\n",
    "print(f\"hit_rate_bge_pretrained: {hit_rate_bge}\\nhit_rate_finetuned: {hit_rate_bge_finetuned}\\nhit_rate_ada {hit_rate_ada}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
