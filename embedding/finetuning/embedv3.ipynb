{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning Embedding Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "%pip install accelerate\n",
    "%pip install -i https://pypi.org/simple/ bitsandbytes\n",
    "%pip install sentence-transformers\n",
    "\n",
    "%pip install llama-index llama-index-llms-huggingface llama-index-embeddings-huggingface llama-index-readers-web\n",
    "\n",
    "%pip install llama-index-embeddings-openai\n",
    "%pip install llama-index-embeddings-adapter\n",
    "%pip install llama-index-finetuning\n",
    "%pip install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data\n",
    "\n",
    "We are using the 10-K report for both Uber and Lyft for 2021. Train data is the 10-K report for Lyft and validation data is the 10-K report for Uber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILES = [\"./Scaling_Laws_for_Downstream_Task_Performance_of_Large_Language_Models.pdf\"]\n",
    "VAL_FILES = [\"./Unraveling_the_Mystery_of_Scaling_Laws.pdf\"]\n",
    "\n",
    "TRAIN_CORPUS_FPATH = \"./train_corpus.json\"\n",
    "VAL_CORPUS_FPATH = \"./val_corpus.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load PDF reports and chunk text using LLamaIndex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import MetadataMode\n",
    "\n",
    "def load_corpus(files):\n",
    "    reader = SimpleDirectoryReader(input_files=files)\n",
    "    docs = reader.load_data()\n",
    "    parser = SentenceSplitter()\n",
    "    nodes = parser.get_nodes_from_documents(docs, show_progress=True)\n",
    "    print(f\"Parsed {len(nodes)} nodes\")\n",
    "    return nodes\n",
    "\n",
    "train_nodes = load_corpus(TRAIN_FILES)\n",
    "val_nodes = load_corpus(VAL_FILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Dataset\n",
    "\n",
    "We will generate a synthetic dataset using Ollama and the `zephyr-7b-beta` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?llama_index.llms.ollama.Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"mattw/huggingfaceh4_zephyr-7b-beta:latest\", \n",
    "    base_url=\"http://localhost:11434\",\n",
    "    temperature=0.7,\n",
    "    context_window=3900,\n",
    "    additional_kwargs = {\"top_k\": 50, \"top_p\": 0.95},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] Generate synthetic dataset using HF LLM directly (slower)\n",
    "\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "\n",
    "quantization_conf = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "#We recreate template (or messages) and ensure that they have the correct format, as per,\n",
    "#https://github.com/run-llama/llama_index/issues/9277#issuecomment-1837545398 for zephyr-7b-beta\n",
    "\n",
    "def messages_to_prompt(messages):\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        if message.role == 'system':\n",
    "            prompt += f\"<|system|>\\n{message.content}</s>\\n\"\n",
    "        elif message.role == 'user':\n",
    "            prompt += f\"<|user|>\\n{message.content}</s>\\n\"\n",
    "        elif message.role == 'assistant':\n",
    "            prompt += f\"<|assistant|>\\n{message.content}</s>\\n\"\n",
    "\n",
    "    # ensure we start with a system prompt, insert blank if needed\n",
    "    if not prompt.startswith(\"<|system|>\\n\"):\n",
    "        prompt = \"<|system|>\\n</s>\\n\" + prompt\n",
    "\n",
    "    # add final assistant prompt\n",
    "    prompt = prompt + \"<|assistant|>\\n\"\n",
    "    return prompt\n",
    "\n",
    "def huggingface_llm(model_name=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "                    tokenizer_name=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "                    context_window=3900,\n",
    "                    max_new_tokens=256,\n",
    "                    quantization_config = quantization_conf\n",
    "                   ):\n",
    "    llm = HuggingFaceLLM(\n",
    "        model_name=model_name,\n",
    "        tokenizer_name=tokenizer_name,\n",
    "        query_wrapper_prompt=PromptTemplate(\"<|system|>\\n</s>\\n<|user|>\\n{query_str}</s>\\n<|assistant|>\\n\"),\n",
    "        context_window=context_window,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        model_kwargs={\"quantization_config\": quantization_config},\n",
    "        # tokenizer_kwargs={},\n",
    "        generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "        messages_to_prompt=messages_to_prompt,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "\n",
    "    return llm\n",
    "\n",
    "llm = huggingface_llm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Optional] Load synthetic dataset from JSON file\n",
    "from llama_index.core.evaluation import EmbeddingQAFinetuneDataset\n",
    "\n",
    "train_dataset = EmbeddingQAFinetuneDataset.from_json(\"train_dataset.json\")\n",
    "val_dataset = EmbeddingQAFinetuneDataset.from_json(\"val_dataset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.finetuning import generate_qa_embedding_pairs\n",
    "\n",
    "train_dataset = generate_qa_embedding_pairs(\n",
    "    llm=llm, nodes=train_nodes, verbose=False\n",
    ")\n",
    "val_dataset = generate_qa_embedding_pairs(\n",
    "    llm=llm, nodes=val_nodes, verbose=False\n",
    ")\n",
    "\n",
    "train_dataset.save_json(\"train_dataset.json\")\n",
    "val_dataset.save_json(\"val_dataset.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune Embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.finetuning import SentenceTransformersFinetuneEngine\n",
    "\n",
    "finetune_engine = SentenceTransformersFinetuneEngine(\n",
    "    train_dataset,\n",
    "    model_id=\"BAAI/bge-small-en\",\n",
    "    model_output_path=\"bge_ft_SL\",\n",
    "    val_dataset=val_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetune_engine.finetune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = finetune_engine.get_finetuned_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Finetuned Model\n",
    "\n",
    "Compare the new embedding model with the original and an OpenAI ada embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.schema import TextNode\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate(\n",
    "    dataset,\n",
    "    embed_model,\n",
    "    top_k=5,\n",
    "    verbose=False,\n",
    "):\n",
    "    corpus = dataset.corpus\n",
    "    queries = dataset.queries\n",
    "    relevant_docs = dataset.relevant_docs\n",
    "\n",
    "    nodes = [TextNode(id_=id_, text=text) for id_, text in corpus.items()]\n",
    "    index = VectorStoreIndex(\n",
    "        nodes, embed_model=embed_model, show_progress=True\n",
    "    )\n",
    "    retriever = index.as_retriever(similarity_top_k=top_k)\n",
    "\n",
    "    eval_results = []\n",
    "    for query_id, query in tqdm(queries.items()):\n",
    "        retrieved_nodes = retriever.retrieve(query)\n",
    "        retrieved_ids = [node.node.node_id for node in retrieved_nodes]\n",
    "        expected_id = relevant_docs[query_id][0]\n",
    "        is_hit = expected_id in retrieved_ids  # assume 1 relevant doc\n",
    "\n",
    "        eval_result = {\n",
    "            \"is_hit\": is_hit,\n",
    "            \"retrieved\": retrieved_ids,\n",
    "            \"expected\": expected_id,\n",
    "            \"query\": query_id,\n",
    "        }\n",
    "        eval_results.append(eval_result)\n",
    "    return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Eval\n",
    "\n",
    "ada = OpenAIEmbedding(api_key='sk-')\n",
    "ada_val_results = evaluate(val_dataset, ada)\n",
    "\n",
    "df_ada = pd.DataFrame(ada_val_results)\n",
    "\n",
    "hit_rate_ada = df_ada[\"is_hit\"].mean()\n",
    "hit_rate_ada # 0.8 // 0.8913 // 0.8695 // 0.93478(HF) // 0.9347826086956522(HF) \n",
    "# // 0.9347826086956522(HF) // 0.8695652173913043(O-B) // 0.9347826086956522(OL) // 0.9130434782608695(OL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BGE Pretrained Eval\n",
    "\n",
    "bge = \"local:BAAI/bge-small-en\"\n",
    "bge_val_results = evaluate(train_dataset, bge)\n",
    "\n",
    "df_bge = pd.DataFrame(bge_val_results)\n",
    "\n",
    "hit_rate_bge = df_bge[\"is_hit\"].mean()\n",
    "hit_rate_bge # 0.8222 // 0.956521 // 0.82608 // 0.8695(HF) // 0.8695652173913043(HF) \n",
    "# // 0.8695652173913043(HF) // 0.8260869565217391(O-B) // 0.8913043478260869(OL) // 0.8043478260869565(OL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FT BGE Embedding Eval\n",
    "val_results_finetuned = evaluate(val_dataset, embed_model)\n",
    "df_finetuned = pd.DataFrame(val_results_finetuned)\n",
    "hit_rate_finetuned = df_finetuned[\"is_hit\"].mean()\n",
    "hit_rate_finetuned # 0.93333 // 0.97826 // 0.9130434782608695 // 0.95652(HF) // 0.95652173(HF) \n",
    "# // 0.9565217391304348(HF) // 0.9782608695652174(O-B) // 0.8695652173913043(OL) // 0.9130434782608695(OL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
