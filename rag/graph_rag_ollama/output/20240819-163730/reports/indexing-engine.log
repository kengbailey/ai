16:37:30,673 graphrag.config.read_dotenv INFO Loading pipeline .env file
16:37:30,686 graphrag.index.cli INFO using default configuration: {
    "llm": {
        "api_key": "REDACTED, length 51",
        "type": "openai_chat",
        "model": "llama3.1:8b",
        "max_tokens": 4000,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "request_timeout": 180.0,
        "api_base": "http://192.168.8.118:11434/v1",
        "api_version": null,
        "proxy": null,
        "cognitive_services_endpoint": null,
        "deployment_name": null,
        "model_supports_json": true,
        "tokens_per_minute": 0,
        "requests_per_minute": 0,
        "max_retries": 10,
        "max_retry_wait": 10.0,
        "sleep_on_rate_limit_recommendation": true,
        "concurrent_requests": 25
    },
    "parallelization": {
        "stagger": 0.3,
        "num_threads": 50
    },
    "async_mode": "threaded",
    "root_dir": ".\\graph_rag_ollama\\",
    "reporting": {
        "type": "file",
        "base_dir": "output/${timestamp}/reports",
        "storage_account_blob_url": null
    },
    "storage": {
        "type": "file",
        "base_dir": "output/${timestamp}/artifacts",
        "storage_account_blob_url": null
    },
    "cache": {
        "type": "file",
        "base_dir": "cache",
        "storage_account_blob_url": null
    },
    "input": {
        "type": "file",
        "file_type": "text",
        "base_dir": "input",
        "storage_account_blob_url": null,
        "encoding": "utf-8",
        "file_pattern": ".*\\.txt$",
        "file_filter": null,
        "source_column": null,
        "timestamp_column": null,
        "timestamp_format": null,
        "text_column": "text",
        "title_column": null,
        "document_attribute_columns": []
    },
    "embed_graph": {
        "enabled": false,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 597832,
        "strategy": null
    },
    "embeddings": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_embedding",
            "model": "text-embedding-3-small",
            "max_tokens": 4000,
            "temperature": 0,
            "top_p": 1,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": null,
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": null,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "batch_size": 16,
        "batch_max_tokens": 8191,
        "target": "required",
        "skip": [],
        "vector_store": null,
        "strategy": null
    },
    "chunks": {
        "size": 200,
        "overlap": 20,
        "group_by_columns": [
            "id"
        ],
        "strategy": null,
        "encoding_model": null
    },
    "snapshots": {
        "graphml": false,
        "raw_entities": false,
        "top_level_nodes": false
    },
    "entity_extraction": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "llama3.1:8b",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://192.168.8.118:11434/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/entity_extraction.txt",
        "entity_types": [
            "organization",
            "person",
            "geo",
            "event"
        ],
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "summarize_descriptions": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "llama3.1:8b",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://192.168.8.118:11434/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/summarize_descriptions.txt",
        "max_length": 500,
        "strategy": null
    },
    "community_reports": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "llama3.1:8b",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://192.168.8.118:11434/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "prompt": "prompts/community_report.txt",
        "max_length": 2000,
        "max_input_length": 8000,
        "strategy": null
    },
    "claim_extraction": {
        "llm": {
            "api_key": "REDACTED, length 51",
            "type": "openai_chat",
            "model": "llama3.1:8b",
            "max_tokens": 4000,
            "temperature": 0.0,
            "top_p": 1.0,
            "n": 1,
            "request_timeout": 180.0,
            "api_base": "http://192.168.8.118:11434/v1",
            "api_version": null,
            "proxy": null,
            "cognitive_services_endpoint": null,
            "deployment_name": null,
            "model_supports_json": true,
            "tokens_per_minute": 0,
            "requests_per_minute": 0,
            "max_retries": 10,
            "max_retry_wait": 10.0,
            "sleep_on_rate_limit_recommendation": true,
            "concurrent_requests": 25
        },
        "parallelization": {
            "stagger": 0.3,
            "num_threads": 50
        },
        "async_mode": "threaded",
        "enabled": false,
        "prompt": "prompts/claim_extraction.txt",
        "description": "Any claims or facts that could be relevant to information discovery.",
        "max_gleanings": 1,
        "strategy": null,
        "encoding_model": null
    },
    "cluster_graph": {
        "max_cluster_size": 10,
        "strategy": null
    },
    "umap": {
        "enabled": false
    },
    "local_search": {
        "text_unit_prop": 0.5,
        "community_prop": 0.1,
        "conversation_history_max_turns": 5,
        "top_k_entities": 10,
        "top_k_relationships": 10,
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "llm_max_tokens": 2000
    },
    "global_search": {
        "temperature": 0.0,
        "top_p": 1.0,
        "n": 1,
        "max_tokens": 12000,
        "data_max_tokens": 12000,
        "map_max_tokens": 1000,
        "reduce_max_tokens": 2000,
        "concurrency": 32
    },
    "encoding_model": "cl100k_base",
    "skip_workflows": []
}
16:37:30,718 graphrag.index.create_pipeline_config INFO skipping workflows 
16:37:30,720 graphrag.index.run INFO Running pipeline
16:37:30,720 graphrag.index.storage.file_pipeline_storage INFO Creating file storage at graph_rag_ollama\output\20240819-163730\artifacts
16:37:30,721 graphrag.index.input.load_input INFO loading input from root_dir=input
16:37:30,721 graphrag.index.input.load_input INFO using file storage for input
16:37:30,722 graphrag.index.storage.file_pipeline_storage INFO search graph_rag_ollama\input for files matching .*\.txt$
16:37:30,723 graphrag.index.input.text INFO found text files from input, found [('lex_fridman_438.txt', {})]
16:37:30,726 graphrag.index.input.text INFO Found 1 files, loading 1
16:37:30,729 graphrag.index.workflows.load INFO Workflow Run Order: ['create_base_text_units', 'create_base_extracted_entities', 'create_summarized_entities', 'create_base_entity_graph', 'create_final_entities', 'create_final_nodes', 'create_final_communities', 'join_text_units_to_entity_ids', 'create_final_relationships', 'join_text_units_to_relationship_ids', 'create_final_community_reports', 'create_final_text_units', 'create_base_documents', 'create_final_documents']
16:37:30,729 graphrag.index.run INFO Final # of rows loaded: 1
16:37:30,912 graphrag.index.run INFO Running workflow: create_base_text_units...
16:37:30,912 graphrag.index.run INFO dependencies for create_base_text_units: []
16:37:30,918 datashaper.workflow.workflow INFO executing verb orderby
16:37:30,921 datashaper.workflow.workflow INFO executing verb zip
16:37:30,924 datashaper.workflow.workflow INFO executing verb aggregate_override
16:37:30,929 datashaper.workflow.workflow INFO executing verb chunk
16:37:31,147 datashaper.workflow.workflow INFO executing verb select
16:37:31,150 datashaper.workflow.workflow INFO executing verb unroll
16:37:31,154 datashaper.workflow.workflow INFO executing verb rename
16:37:31,158 datashaper.workflow.workflow INFO executing verb genid
16:37:31,166 datashaper.workflow.workflow INFO executing verb unzip
16:37:31,170 datashaper.workflow.workflow INFO executing verb copy
16:37:31,174 datashaper.workflow.workflow INFO executing verb filter
16:37:31,188 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_text_units.parquet
16:37:31,438 graphrag.index.run INFO Running workflow: create_base_extracted_entities...
16:37:31,438 graphrag.index.run INFO dependencies for create_base_extracted_entities: ['create_base_text_units']
16:37:31,439 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
16:37:31,472 datashaper.workflow.workflow INFO executing verb entity_extract
16:37:31,497 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=http://192.168.8.118:11434/v1
16:37:31,703 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for llama3.1:8b: TPM=0, RPM=0
16:37:31,703 graphrag.index.llm.load_llm INFO create concurrency limiter for llama3.1:8b: 25
16:37:37,738 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:37:37,741 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 6.01600000000326. input_tokens=1936, output_tokens=49
16:37:39,70 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:37:39,72 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.312000000005355. input_tokens=1936, output_tokens=55
16:37:39,367 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:37:39,367 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 7.625. input_tokens=1936, output_tokens=114
16:37:41,772 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:37:41,774 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 10.030999999988126. input_tokens=1936, output_tokens=56
16:37:42,780 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:37:42,781 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 11.030999999988126. input_tokens=1935, output_tokens=109
16:37:44,67 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:37:44,67 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 12.327999999994063. input_tokens=1936, output_tokens=94
16:37:45,3 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:37:45,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 13.281999999991967. input_tokens=1936, output_tokens=313
16:37:46,453 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:37:46,454 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 14.702999999994063. input_tokens=1935, output_tokens=105
16:37:47,435 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:37:47,435 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 15.687999999994645. input_tokens=1936, output_tokens=89
16:37:47,713 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:37:47,714 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 15.985000000000582. input_tokens=1936, output_tokens=375
16:37:50,688 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:37:50,689 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 18.922000000005937. input_tokens=1935, output_tokens=86
16:37:50,765 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:37:50,767 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 18.985000000000582. input_tokens=1936, output_tokens=84
16:37:52,251 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:37:52,251 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:37:52,253 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 20.5. input_tokens=1936, output_tokens=57
16:37:52,254 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 20.5. input_tokens=1936, output_tokens=201
16:37:52,614 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:37:52,615 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 20.828000000008615. input_tokens=1935, output_tokens=58
16:37:55,664 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:37:55,666 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 23.906000000002678. input_tokens=1936, output_tokens=91
16:37:56,342 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:37:56,343 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.547000000005937. input_tokens=1936, output_tokens=116
16:37:56,403 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:37:56,404 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 24.639999999999418. input_tokens=1935, output_tokens=25
16:37:58,353 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:37:58,355 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 26.562000000005355. input_tokens=1936, output_tokens=30
16:38:00,146 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:00,148 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 28.375. input_tokens=1936, output_tokens=73
16:38:02,155 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:02,157 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 30.375. input_tokens=1936, output_tokens=196
16:38:02,195 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:02,197 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 30.406000000002678. input_tokens=1936, output_tokens=314
16:38:08,661 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:08,662 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 36.89100000000326. input_tokens=1936, output_tokens=98
16:38:10,573 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:10,574 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 38.79700000000594. input_tokens=1936, output_tokens=67
16:38:10,592 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:10,594 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 38.8130000000092. input_tokens=1936, output_tokens=583
16:38:15,956 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:15,956 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 36.89100000000326. input_tokens=1935, output_tokens=108
16:38:17,305 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:17,307 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.937999999994645. input_tokens=1936, output_tokens=108
16:38:17,346 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:17,347 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.60899999999674. input_tokens=1936, output_tokens=180
16:38:21,236 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:21,237 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.17200000000594. input_tokens=1936, output_tokens=112
16:38:23,54 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:23,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 40.28200000000652. input_tokens=1936, output_tokens=195
16:38:26,274 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:26,275 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 44.5. input_tokens=1935, output_tokens=343
16:38:29,851 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:29,852 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 42.421000000002095. input_tokens=1936, output_tokens=192
16:38:31,365 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:31,366 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 46.35900000001129. input_tokens=1936, output_tokens=352
16:38:33,501 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:33,508 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 47.062999999994645. input_tokens=1936, output_tokens=277
16:38:36,124 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:36,125 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 43.85900000001129. input_tokens=1936, output_tokens=54
16:38:37,384 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:37,385 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 45.125. input_tokens=1936, output_tokens=46
16:38:37,950 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:37,951 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 47.187000000005355. input_tokens=1936, output_tokens=166
16:38:40,69 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:40,70 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 52.35899999999674. input_tokens=1936, output_tokens=334
16:38:41,628 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:41,629 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 49.01599999998871. input_tokens=1936, output_tokens=70
16:38:43,690 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:43,691 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 53.0. input_tokens=1936, output_tokens=323
16:38:46,982 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:46,983 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 51.312000000005355. input_tokens=1936, output_tokens=230
16:38:48,459 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:48,460 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 50.11000000000058. input_tokens=1936, output_tokens=102
16:38:50,241 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:50,242 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 50.094000000011874. input_tokens=1936, output_tokens=83
16:38:50,532 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:50,533 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 54.125. input_tokens=1936, output_tokens=273
16:38:52,213 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:52,214 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 55.875. input_tokens=1936, output_tokens=301
16:38:54,742 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:54,743 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 44.17200000000594. input_tokens=1936, output_tokens=27
16:38:55,844 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:55,845 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 45.25. input_tokens=1936, output_tokens=39
16:38:56,266 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:38:56,267 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 54.11000000000058. input_tokens=1936, output_tokens=179
16:39:00,542 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:00,544 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.57799999999406. input_tokens=34, output_tokens=49
16:39:01,420 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:01,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.062999999994645. input_tokens=34, output_tokens=32
16:39:04,489 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:04,490 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 55.828000000008615. input_tokens=1936, output_tokens=253
16:39:06,97 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:06,97 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.85899999999674. input_tokens=34, output_tokens=146
16:39:07,660 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:07,661 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 41.39100000000326. input_tokens=34, output_tokens=63
16:39:09,32 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:09,34 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 51.73399999999674. input_tokens=34, output_tokens=342
16:39:09,601 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:09,603 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 67.40600000000268. input_tokens=1936, output_tokens=468
16:39:10,444 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:10,445 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.937000000005355. input_tokens=34, output_tokens=32
16:39:11,644 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:11,645 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.280999999988126. input_tokens=34, output_tokens=106
16:39:11,679 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:11,681 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 48.625. input_tokens=34, output_tokens=297
16:39:13,453 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:13,453 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.062000000005355. input_tokens=34, output_tokens=80
16:39:14,745 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:14,745 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.79700000000594. input_tokens=34, output_tokens=95
16:39:17,278 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:17,279 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.63999999999942. input_tokens=34, output_tokens=117
16:39:18,765 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:18,766 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 48.90699999999197. input_tokens=34, output_tokens=402
16:39:19,195 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:19,196 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.125. input_tokens=34, output_tokens=168
16:39:20,261 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:20,262 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.28199999999197. input_tokens=34, output_tokens=61
16:39:21,906 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:21,907 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.4369999999908. input_tokens=34, output_tokens=68
16:39:21,944 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:21,944 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 31.702999999994063. input_tokens=34, output_tokens=25
16:39:22,873 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:22,873 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 32.328000000008615. input_tokens=34, output_tokens=30
16:39:23,516 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:23,516 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.812999999994645. input_tokens=34, output_tokens=167
16:39:24,5 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:24,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 31.796999999991385. input_tokens=34, output_tokens=71
16:39:25,482 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:25,483 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 29.218000000008033. input_tokens=34, output_tokens=68
16:39:28,249 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:28,250 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 32.40600000000268. input_tokens=34, output_tokens=162
16:39:29,550 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:29,552 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 29.01600000000326. input_tokens=34, output_tokens=121
16:39:29,818 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:29,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.07799999999406. input_tokens=34, output_tokens=244
16:39:31,736 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:31,737 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 25.625. input_tokens=34, output_tokens=90
16:39:32,58 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:32,60 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 27.562999999994645. input_tokens=34, output_tokens=109
16:39:33,588 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:33,589 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 24.547000000005937. input_tokens=34, output_tokens=68
16:39:34,200 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:34,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 26.531000000002678. input_tokens=34, output_tokens=106
16:39:34,839 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:34,841 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 58.71899999999732. input_tokens=34, output_tokens=887
16:39:39,75 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:39,76 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 27.422000000005937. input_tokens=34, output_tokens=201
16:39:40,615 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:40,617 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 31.01600000000326. input_tokens=34, output_tokens=317
16:39:43,571 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:43,574 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.125. input_tokens=34, output_tokens=432
16:39:45,552 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:45,553 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 32.11000000000058. input_tokens=34, output_tokens=173
16:39:47,849 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:47,850 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 30.578000000008615. input_tokens=34, output_tokens=46
16:39:48,245 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:48,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 46.828000000008615. input_tokens=34, output_tokens=787
16:39:50,285 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:50,287 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 31.531000000002678. input_tokens=34, output_tokens=90
16:39:51,974 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:51,976 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 31.718000000008033. input_tokens=34, output_tokens=70
16:39:54,865 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:54,867 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 43.187000000005355. input_tokens=34, output_tokens=579
16:39:55,520 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:55,522 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.780999999988126. input_tokens=34, output_tokens=394
16:39:55,551 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:55,554 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.61000000000058. input_tokens=34, output_tokens=138
16:39:57,160 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:57,161 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.296999999991385. input_tokens=34, output_tokens=72
16:39:57,167 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:39:57,168 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.65600000000268. input_tokens=34, output_tokens=72
16:40:01,3 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:01,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.0. input_tokens=34, output_tokens=171
16:40:03,425 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:03,426 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.953999999997905. input_tokens=34, output_tokens=238
16:40:04,857 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:04,859 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 42.953000000008615. input_tokens=34, output_tokens=349
16:40:05,606 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:05,607 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.046000000002095. input_tokens=34, output_tokens=38
16:40:05,646 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:05,648 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 46.45299999999406. input_tokens=34, output_tokens=655
16:40:07,965 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:07,967 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.71899999999732. input_tokens=34, output_tokens=115
16:40:11,278 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:11,280 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 41.45299999999406. input_tokens=34, output_tokens=170
16:40:12,633 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:12,635 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.046999999991385. input_tokens=34, output_tokens=147
16:40:14,430 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:14,431 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.59399999999732. input_tokens=1936, output_tokens=26
16:40:15,751 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:15,753 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 43.70299999999406. input_tokens=1936, output_tokens=236
16:40:16,305 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:16,306 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 42.11000000000058. input_tokens=1936, output_tokens=100
16:40:16,592 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:16,594 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 44.84399999999732. input_tokens=1936, output_tokens=257
16:40:18,85 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:18,87 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.01600000000326. input_tokens=1936, output_tokens=89
16:40:19,901 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:19,903 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 36.32799999999406. input_tokens=1936, output_tokens=97
16:40:22,900 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:22,902 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 35.046999999991385. input_tokens=1936, output_tokens=202
16:40:24,625 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:24,626 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.062000000005355. input_tokens=1937, output_tokens=246
16:40:26,368 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:26,370 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 34.39100000000326. input_tokens=1936, output_tokens=26
16:40:27,201 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:27,202 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 36.90600000000268. input_tokens=1936, output_tokens=85
16:40:28,57 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:28,58 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 32.53200000000652. input_tokens=1937, output_tokens=30
16:40:29,271 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:29,274 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.71799999999348. input_tokens=1936, output_tokens=46
16:40:34,160 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:34,162 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.296999999991385. input_tokens=1936, output_tokens=325
16:40:36,812 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:36,814 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 56.187999999994645. input_tokens=34, output_tokens=690
16:40:38,895 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:38,897 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 50.655999999988126. input_tokens=1936, output_tokens=598
16:40:38,978 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:38,980 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 41.812000000005355. input_tokens=1936, output_tokens=314
16:40:45,285 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:45,287 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 48.125. input_tokens=1936, output_tokens=246
16:40:48,64 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:48,65 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 44.63999999999942. input_tokens=1935, output_tokens=216
16:40:50,350 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:50,352 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 49.34300000000803. input_tokens=1936, output_tokens=294
16:40:52,145 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:52,145 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 46.5. input_tokens=1936, output_tokens=64
16:40:52,603 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:52,604 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 47.75. input_tokens=1936, output_tokens=297
16:40:52,645 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:52,646 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 47.046999999991385. input_tokens=1937, output_tokens=148
16:40:57,433 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:40:57,435 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 46.15700000000652. input_tokens=1936, output_tokens=112
16:41:00,706 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:00,708 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 52.75. input_tokens=1936, output_tokens=225
16:41:03,115 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:03,116 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 48.687000000005355. input_tokens=1936, output_tokens=241
16:41:05,75 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:05,77 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 52.437000000005355. input_tokens=1936, output_tokens=274
16:41:07,422 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:07,424 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 49.32799999999406. input_tokens=1936, output_tokens=41
16:41:08,222 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:08,223 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 48.328000000008615. input_tokens=1936, output_tokens=27
16:41:09,613 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:09,614 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 46.719000000011874. input_tokens=1936, output_tokens=49
16:41:10,116 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:10,119 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 54.35900000001129. input_tokens=1936, output_tokens=326
16:41:11,418 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:11,419 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 54.82799999999406. input_tokens=1936, output_tokens=184
16:41:13,951 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:13,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 57.63999999999942. input_tokens=1936, output_tokens=291
16:41:17,114 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:17,115 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 49.062000000005355. input_tokens=1936, output_tokens=66
16:41:19,201 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:19,204 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 52.82799999999406. input_tokens=1935, output_tokens=212
16:41:21,347 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:21,349 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 56.71800000000803. input_tokens=1936, output_tokens=268
16:41:23,458 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:23,460 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 56.26600000000326. input_tokens=1936, output_tokens=285
16:41:29,469 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:29,470 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 52.64100000000326. input_tokens=1936, output_tokens=242
16:41:31,126 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:31,127 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 52.219000000011874. input_tokens=1936, output_tokens=226
16:41:33,653 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:33,654 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:33,657 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 59.48399999999674. input_tokens=1936, output_tokens=362
16:41:33,660 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 64.39100000000326. input_tokens=1936, output_tokens=445
16:41:37,164 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:37,165 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 58.187999999994645. input_tokens=1936, output_tokens=90
16:41:38,876 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:38,877 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 46.73500000000058. input_tokens=1937, output_tokens=64
16:41:39,610 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:39,612 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 49.26600000000326. input_tokens=1936, output_tokens=110
16:41:43,957 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:43,958 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 51.36000000000058. input_tokens=1936, output_tokens=207
16:41:46,785 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:46,787 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 54.14100000000326. input_tokens=1936, output_tokens=251
16:41:48,593 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:48,595 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 60.51600000000326. input_tokens=1936, output_tokens=388
16:41:57,751 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:57,753 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 57.046999999991385. input_tokens=1936, output_tokens=235
16:41:59,211 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:59,213 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 56.09399999999732. input_tokens=1935, output_tokens=227
16:41:59,879 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:59,880 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 54.812999999994645. input_tokens=34, output_tokens=26
16:41:59,963 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:41:59,965 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 62.53100000000268. input_tokens=1936, output_tokens=343
16:42:02,451 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:02,453 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 54.21899999999732. input_tokens=34, output_tokens=99
16:42:03,933 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:03,936 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 53.812999999994645. input_tokens=34, output_tokens=58
16:42:04,881 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:04,882 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 55.26599999998871. input_tokens=34, output_tokens=202
16:42:06,419 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:06,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 55.0. input_tokens=34, output_tokens=97
16:42:13,873 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:13,875 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 56.75. input_tokens=34, output_tokens=326
16:42:15,694 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:15,695 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 56.5. input_tokens=34, output_tokens=26
16:42:16,958 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:16,960 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 63.01600000000326. input_tokens=34, output_tokens=476
16:42:18,329 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:18,331 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 56.98500000000058. input_tokens=34, output_tokens=62
16:42:27,851 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:27,852 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 64.38999999999942. input_tokens=34, output_tokens=30
16:42:28,639 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:28,640 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 59.15699999999197. input_tokens=34, output_tokens=46
16:42:32,150 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:32,152 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 58.48399999999674. input_tokens=1936, output_tokens=97
16:42:33,9 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:33,11 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 85.57799999999406. input_tokens=34, output_tokens=886
16:42:38,912 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:38,914 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 67.78100000000268. input_tokens=34, output_tokens=390
16:42:39,966 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:39,968 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 66.29700000000594. input_tokens=34, output_tokens=307
16:42:40,756 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:40,757 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 63.59399999999732. input_tokens=34, output_tokens=314
16:42:41,301 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:41,303 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 62.42200000000594. input_tokens=34, output_tokens=78
16:42:52,375 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:52,376 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 68.40600000000268. input_tokens=34, output_tokens=121
16:42:52,749 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:52,750 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 65.95300000000861. input_tokens=34, output_tokens=113
16:42:54,137 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:54,138 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 65.54699999999139. input_tokens=34, output_tokens=60
16:42:54,746 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:54,748 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 75.125. input_tokens=34, output_tokens=238
16:42:55,556 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:55,558 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 57.79700000000594. input_tokens=34, output_tokens=101
16:42:56,353 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:42:56,355 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 57.13999999999942. input_tokens=34, output_tokens=74
16:43:00,292 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:43:00,294 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 60.32799999999406. input_tokens=34, output_tokens=139
16:43:06,297 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:43:06,299 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 63.86000000000058. input_tokens=34, output_tokens=317
16:43:11,417 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:43:11,418 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 67.48399999999674. input_tokens=34, output_tokens=214
16:43:19,984 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:43:19,986 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 75.09300000000803. input_tokens=34, output_tokens=27
16:43:21,205 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:43:21,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 74.79700000000594. input_tokens=34, output_tokens=49
16:43:21,267 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:43:21,269 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 81.38999999999942. input_tokens=34, output_tokens=446
16:43:30,635 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:43:30,637 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 73.67199999999139. input_tokens=34, output_tokens=389
16:43:41,464 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:43:41,467 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 87.59399999999732. input_tokens=34, output_tokens=483
16:43:42,19 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:43:42,21 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 86.32799999999406. input_tokens=34, output_tokens=456
16:43:42,676 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:43:42,677 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 84.34399999999732. input_tokens=34, output_tokens=82
16:43:45,479 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n \n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity\'s attributes and activities\nFormat each entity as ("entity"<|><entity_name><|><entity_type><|><entity_description>)\n \n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as ("relationship"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n \n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n \n4. When finished, output <|COMPLETE|>\n \n######################\n-Examples-\n######################\nExample 1:\nEntity_types: ORGANIZATION,PERSON\nText:\nThe Verdantis\'s Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n######################\nOutput:\n("entity"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n##\n("entity"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n##\n("entity"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis\'s money supply)\n##\n("relationship"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n<|COMPLETE|>\n\n######################\nExample 2:\nEntity_types: ORGANIZATION\nText:\nTechGlobal\'s (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation\'s debut on the public markets isn\'t indicative of how other newly listed companies may perform.\n\nTechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n######################\nOutput:\n("entity"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n##\n("entity"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n##\n("relationship"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n<|COMPLETE|>\n\n######################\nExample 3:\nEntity_types: ORGANIZATION,GEO,PERSON\nText:\nFive Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n\nThe swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n\nThe exchange initiated in Firuzabad\'s capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n\nThey were welcomed by senior Aurelian officials and are now on their way to Aurelia\'s capital, Cashion.\n\nThe Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia\'s Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n######################\nOutput:\n("entity"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n##\n("entity"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n##\n("entity"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n##\n##\n("entity"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n##\n("entity"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n##\n("entity"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n##\n("entity"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia\'s Alhamia Prison)\n##\n("entity"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n##\n("entity"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n##\n("entity"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n##\n("relationship"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n##\n("relationship"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n<|COMPLETE|>\n\n######################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: far more resilient. The difficulty of getting to Mars is what makes it resilient. So, but in going through the these various explanations of why don\'t we see the aliens, why one of them is that they fail to pass these great filters, these key hurdles, and one of them is that they fail to pass these great filters, these key hurdles. And one of those hurdles is being a multi-planet species. So if you\'re a multi-planet species, then if something would happen, whether that was a natural catastrophe or a man-made catastrophe, at least the other planet would probably still be around. So you\'re not like, you don\'t have all the eggs in one basket. And once you are sort of a two-planet species, you can obviously extend to, extend life halves to the asteroid belt, maybe to the moons of Jupiter and Saturn, and ultimately to other star systems. But if you can\'t even get to another planet, you know, definitely not getting to\n######################\nOutput:'}
16:43:48,203 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:43:48,205 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 79.56200000000536. input_tokens=34, output_tokens=238
16:43:50,934 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:43:50,936 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 83.0789999999979. input_tokens=34, output_tokens=367
16:43:51,782 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:43:51,784 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 78.78100000000268. input_tokens=34, output_tokens=252
16:43:56,291 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:43:56,294 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 76.32799999999406. input_tokens=34, output_tokens=207
16:43:56,317 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:43:56,319 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 84.17200000000594. input_tokens=34, output_tokens=538
16:43:58,773 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:43:58,774 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 77.46799999999348. input_tokens=34, output_tokens=55
16:43:58,848 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:43:58,850 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 78.09300000000803. input_tokens=34, output_tokens=240
16:43:59,658 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:43:59,661 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 80.75. input_tokens=34, output_tokens=415
16:44:01,177 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:01,178 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 68.79700000000594. input_tokens=34, output_tokens=156
16:44:06,878 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:06,880 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 74.14099999998871. input_tokens=34, output_tokens=367
16:44:08,39 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:08,41 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 73.90600000000268. input_tokens=34, output_tokens=417
16:44:09,49 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:09,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 74.31299999999464. input_tokens=34, output_tokens=427
16:44:14,858 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:14,860 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 68.5460000000021. input_tokens=1936, output_tokens=237
16:44:18,854 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:18,856 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 82.5. input_tokens=34, output_tokens=510
16:44:20,877 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:20,879 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 80.59399999999732. input_tokens=34, output_tokens=505
16:44:23,610 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:23,613 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 88.06200000000536. input_tokens=34, output_tokens=910
16:44:28,228 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:28,230 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 76.81200000000536. input_tokens=34, output_tokens=440
16:44:29,935 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:29,937 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 68.71899999999732. input_tokens=1936, output_tokens=234
16:44:31,924 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:31,925 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 71.93799999999464. input_tokens=1936, output_tokens=312
16:44:34,787 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:34,788 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 53.32799999999406. input_tokens=1936, output_tokens=104
16:44:35,884 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:35,887 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 74.61000000000058. input_tokens=1936, output_tokens=317
16:44:35,928 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:35,929 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 53.25. input_tokens=1936, output_tokens=38
16:44:39,639 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:39,641 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 69.0. input_tokens=1936, output_tokens=275
16:44:42,738 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:42,739 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 51.812000000005355. input_tokens=1936, output_tokens=76
16:44:45,198 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:45,201 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 58.21899999999732. input_tokens=1936, output_tokens=288
16:44:46,454 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:46,457 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 64.4380000000092. input_tokens=1936, output_tokens=408
16:44:52,581 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:52,583 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 56.29700000000594. input_tokens=34, output_tokens=259
16:44:54,177 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:54,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 62.39100000000326. input_tokens=1935, output_tokens=383
16:44:55,962 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:44:55,964 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 59.64100000000326. input_tokens=1936, output_tokens=313
16:45:04,324 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:04,326 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 76.10899999999674. input_tokens=1936, output_tokens=630
16:45:14,384 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:14,386 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 75.61000000000058. input_tokens=1935, output_tokens=185
16:45:17,672 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:17,673 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 70.78100000000268. input_tokens=1936, output_tokens=88
16:45:18,112 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:18,114 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 79.26600000000326. input_tokens=1936, output_tokens=275
16:45:20,195 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:20,197 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 72.15600000000268. input_tokens=1936, output_tokens=49
16:45:22,104 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:22,107 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 82.43700000000536. input_tokens=1936, output_tokens=367
16:45:25,212 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:25,214 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 70.36000000000058. input_tokens=1936, output_tokens=151
16:45:28,153 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:28,155 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 86.96799999999348. input_tokens=1936, output_tokens=461
16:45:30,626 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:30,628 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 69.75. input_tokens=1936, output_tokens=106
16:45:33,66 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:33,67 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 84.01499999999942. input_tokens=1936, output_tokens=402
16:45:34,546 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:34,548 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 66.31299999999464. input_tokens=1936, output_tokens=107
16:45:35,178 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:35,180 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 76.3289999999979. input_tokens=1936, output_tokens=318
16:45:38,24 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:38,26 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 68.09299999999348. input_tokens=1936, output_tokens=96
16:45:38,86 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:38,87 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 66.15600000000268. input_tokens=1936, output_tokens=88
16:45:40,793 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:40,795 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 64.85899999999674. input_tokens=1936, output_tokens=103
16:45:41,466 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:41,468 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 66.67200000000594. input_tokens=1937, output_tokens=196
16:45:43,812 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:43,814 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 67.92200000000594. input_tokens=1936, output_tokens=174
16:45:43,854 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:43,856 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 80.23399999999674. input_tokens=1936, output_tokens=431
16:45:52,985 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:52,987 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 73.35900000001129. input_tokens=1936, output_tokens=315
16:45:55,426 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:55,428 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 72.68799999999464. input_tokens=1936, output_tokens=353
16:45:57,659 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:45:57,662 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 72.46899999999732. input_tokens=1936, output_tokens=345
16:46:00,384 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:00,385 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 66.20299999999406. input_tokens=1936, output_tokens=67
16:46:00,614 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:00,616 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 74.15600000000268. input_tokens=1936, output_tokens=384
16:46:06,596 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:06,598 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 62.28100000000268. input_tokens=1935, output_tokens=205
16:46:09,396 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:09,397 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 55.01499999999942. input_tokens=1937, output_tokens=274
16:46:11,237 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:11,239 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 53.578000000008615. input_tokens=1936, output_tokens=92
16:46:11,638 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:11,641 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 75.67199999999139. input_tokens=1936, output_tokens=347
16:46:11,775 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:11,777 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 79.1869999999908. input_tokens=1936, output_tokens=455
16:46:19,617 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:19,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 57.51600000000326. input_tokens=1936, output_tokens=40
16:46:22,339 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:22,341 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 62.14100000000326. input_tokens=1936, output_tokens=152
16:46:22,873 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:22,876 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 57.65600000000268. input_tokens=1936, output_tokens=150
16:46:25,346 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:25,347 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 57.203000000008615. input_tokens=1936, output_tokens=164
16:46:29,345 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:29,347 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 71.23399999999674. input_tokens=1936, output_tokens=370
16:46:31,61 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:31,63 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 57.98500000000058. input_tokens=1936, output_tokens=197
16:46:35,172 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:35,174 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 57.15700000000652. input_tokens=1936, output_tokens=124
16:46:36,726 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:36,728 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 61.546000000002095. input_tokens=1936, output_tokens=207
16:46:37,637 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:37,640 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 67.0. input_tokens=34, output_tokens=438
16:46:38,63 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:38,66 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 63.53100000000268. input_tokens=1936, output_tokens=375
16:46:39,895 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:39,897 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 58.4369999999908. input_tokens=34, output_tokens=92
16:46:41,820 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:41,821 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 57.96899999999732. input_tokens=34, output_tokens=38
16:46:45,269 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:45,271 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 64.48399999999674. input_tokens=34, output_tokens=329
16:46:45,550 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:45,553 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 67.46899999999732. input_tokens=34, output_tokens=399
16:46:46,682 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:46,683 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 51.25. input_tokens=34, output_tokens=54
16:46:50,516 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:50,518 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 66.70299999999406. input_tokens=34, output_tokens=477
16:46:51,291 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:51,293 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 58.296999999991385. input_tokens=34, output_tokens=390
16:46:52,900 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:52,902 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 52.280999999988126. input_tokens=1936, output_tokens=230
16:46:56,78 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:56,80 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 58.40600000000268. input_tokens=34, output_tokens=425
16:46:59,186 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:46:59,188 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 58.79700000000594. input_tokens=34, output_tokens=373
16:47:01,754 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:01,756 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 55.15699999999197. input_tokens=34, output_tokens=471
16:47:02,461 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:02,462 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 51.21899999999732. input_tokens=34, output_tokens=284
16:47:03,509 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:03,510 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 51.73500000000058. input_tokens=34, output_tokens=70
16:47:04,648 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:04,650 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 53.01499999999942. input_tokens=34, output_tokens=196
16:47:05,687 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:05,689 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 56.28200000000652. input_tokens=34, output_tokens=520
16:47:05,967 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:05,968 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 43.625. input_tokens=34, output_tokens=49
16:47:09,665 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:09,666 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.312999999994645. input_tokens=34, output_tokens=174
16:47:11,294 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:11,295 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.23399999999674. input_tokens=34, output_tokens=71
16:47:12,441 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:12,444 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 52.82799999999406. input_tokens=34, output_tokens=381
16:47:14,343 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:14,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.61000000000058. input_tokens=34, output_tokens=86
16:47:15,556 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:15,558 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 46.203999999997905. input_tokens=34, output_tokens=428
16:47:16,662 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:16,664 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 53.796999999991385. input_tokens=34, output_tokens=526
16:47:18,122 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:18,123 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 38.219000000011874. input_tokens=34, output_tokens=63
16:47:20,71 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:20,72 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 38.25. input_tokens=34, output_tokens=88
16:47:20,740 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:20,742 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 43.10900000001129. input_tokens=34, output_tokens=286
16:47:21,203 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:21,205 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 43.125. input_tokens=34, output_tokens=261
16:47:22,536 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:22,537 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.98399999999674. input_tokens=34, output_tokens=66
16:47:23,694 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:23,696 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 38.42200000000594. input_tokens=34, output_tokens=143
16:47:24,784 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:24,786 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 49.60899999999674. input_tokens=34, output_tokens=585
16:47:31,363 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:31,365 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.078000000008615. input_tokens=34, output_tokens=353
16:47:34,440 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:34,443 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 47.76499999999942. input_tokens=34, output_tokens=553
16:47:35,576 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:35,579 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 45.04700000000594. input_tokens=34, output_tokens=541
16:47:36,671 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:36,674 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 43.76600000000326. input_tokens=34, output_tokens=496
16:47:42,234 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:42,236 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 43.046000000002095. input_tokens=34, output_tokens=346
16:47:44,457 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:44,458 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.953000000008615. input_tokens=34, output_tokens=100
16:47:46,672 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:46,674 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.20299999999406. input_tokens=34, output_tokens=450
16:47:53,270 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:53,272 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 48.625. input_tokens=34, output_tokens=395
16:47:55,437 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:55,439 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 49.46899999999732. input_tokens=34, output_tokens=40
16:47:57,488 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:47:57,490 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 47.828000000008615. input_tokens=34, output_tokens=90
16:48:02,748 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:48:02,750 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 51.453000000008615. input_tokens=34, output_tokens=246
16:48:03,624 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:48:03,627 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 57.92200000000594. input_tokens=34, output_tokens=717
16:48:11,626 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:48:11,627 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 57.28100000000268. input_tokens=34, output_tokens=354
16:48:14,365 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:48:14,367 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 61.92200000000594. input_tokens=34, output_tokens=509
16:48:16,437 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:48:16,439 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 59.76600000000326. input_tokens=34, output_tokens=80
16:48:23,601 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:48:23,602 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 68.0460000000021. input_tokens=34, output_tokens=445
16:48:28,607 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:48:28,609 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 68.53100000000268. input_tokens=1936, output_tokens=161
16:48:30,668 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:48:30,670 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 72.54699999999139. input_tokens=34, output_tokens=492
16:48:39,126 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:48:39,128 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 77.92199999999139. input_tokens=1935, output_tokens=245
16:48:49,552 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:48:49,554 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 87.01600000000326. input_tokens=1936, output_tokens=168
16:49:00,664 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:49:00,666 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 99.92199999999139. input_tokens=34, output_tokens=618
16:49:07,621 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:49:07,623 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 103.92200000000594. input_tokens=1936, output_tokens=181
16:49:26,666 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:49:26,667 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 115.29699999999139. input_tokens=1936, output_tokens=167
16:49:32,665 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:49:32,668 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 127.875. input_tokens=1936, output_tokens=423
16:49:37,841 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:49:37,842 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 122.26600000000326. input_tokens=1936, output_tokens=27
16:49:41,682 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:49:41,684 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 125.0. input_tokens=34, output_tokens=156
16:49:51,593 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:49:51,595 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 137.14100000000326. input_tokens=1935, output_tokens=464
16:49:56,104 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
16:50:01,768 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
16:50:34,427 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:50:34,428 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 167.75. input_tokens=1936, output_tokens=26
16:50:38,405 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:50:38,406 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 173.9369999999908. input_tokens=1936, output_tokens=89
16:50:38,863 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:50:38,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 176.625. input_tokens=1936, output_tokens=176
16:50:38,885 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:50:38,887 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 165.61000000000058. input_tokens=1936, output_tokens=92
16:50:53,105 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:50:53,107 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 177.6710000000021. input_tokens=1936, output_tokens=183
16:50:56,773 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:50:56,775 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 165.13999999999942. input_tokens=1936, output_tokens=98
16:50:56,835 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:50:56,838 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 174.09399999999732. input_tokens=1936, output_tokens=217
16:50:57,506 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n \n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity\'s attributes and activities\nFormat each entity as ("entity"<|><entity_name><|><entity_type><|><entity_description>)\n \n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as ("relationship"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n \n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n \n4. When finished, output <|COMPLETE|>\n \n######################\n-Examples-\n######################\nExample 1:\nEntity_types: ORGANIZATION,PERSON\nText:\nThe Verdantis\'s Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n######################\nOutput:\n("entity"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n##\n("entity"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n##\n("entity"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis\'s money supply)\n##\n("relationship"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n<|COMPLETE|>\n\n######################\nExample 2:\nEntity_types: ORGANIZATION\nText:\nTechGlobal\'s (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation\'s debut on the public markets isn\'t indicative of how other newly listed companies may perform.\n\nTechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n######################\nOutput:\n("entity"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n##\n("entity"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n##\n("relationship"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n<|COMPLETE|>\n\n######################\nExample 3:\nEntity_types: ORGANIZATION,GEO,PERSON\nText:\nFive Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n\nThe swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n\nThe exchange initiated in Firuzabad\'s capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n\nThey were welcomed by senior Aurelian officials and are now on their way to Aurelia\'s capital, Cashion.\n\nThe Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia\'s Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n######################\nOutput:\n("entity"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n##\n("entity"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n##\n("entity"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n##\n##\n("entity"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n##\n("entity"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n##\n("entity"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n##\n("entity"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia\'s Alhamia Prison)\n##\n("entity"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n##\n("entity"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n##\n("entity"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n##\n("relationship"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n##\n("relationship"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n<|COMPLETE|>\n\n######################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: and the organic and the organic. Anyway so sorry to rudely interrupt. So there\'s a selection that Nolan has passed with flying colors. So everything including that it\'s a BCI friendly home, all of that. So what is the process of the surgery, the implantation in the first moment when he gets to use the system. The end-to-end, you know, we say patient-in-to-patient out is anywhere between two to four hours, in particular case for Nolan, it was about three and a half hours, and there\'s many steps leading to, you know, the actual robot insertion, right? So there\'s anesthesia, induction, and we do intra-op CT imaging to make sure that we\'re, you know, drilling the hole in the right location. And this is also pre-plan beforehand. Someone goes through, someone like Nolan would go through FMRI and then they can think about wiggling their hand, you know, obviously due to their injury, it\'s\n######################\nOutput:'}
16:51:03,649 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n \n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity\'s attributes and activities\nFormat each entity as ("entity"<|><entity_name><|><entity_type><|><entity_description>)\n \n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as ("relationship"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n \n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n \n4. When finished, output <|COMPLETE|>\n \n######################\n-Examples-\n######################\nExample 1:\nEntity_types: ORGANIZATION,PERSON\nText:\nThe Verdantis\'s Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n######################\nOutput:\n("entity"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n##\n("entity"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n##\n("entity"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis\'s money supply)\n##\n("relationship"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n<|COMPLETE|>\n\n######################\nExample 2:\nEntity_types: ORGANIZATION\nText:\nTechGlobal\'s (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation\'s debut on the public markets isn\'t indicative of how other newly listed companies may perform.\n\nTechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n######################\nOutput:\n("entity"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n##\n("entity"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n##\n("relationship"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n<|COMPLETE|>\n\n######################\nExample 3:\nEntity_types: ORGANIZATION,GEO,PERSON\nText:\nFive Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n\nThe swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n\nThe exchange initiated in Firuzabad\'s capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n\nThey were welcomed by senior Aurelian officials and are now on their way to Aurelia\'s capital, Cashion.\n\nThe Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia\'s Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n######################\nOutput:\n("entity"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n##\n("entity"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n##\n("entity"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n##\n##\n("entity"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n##\n("entity"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n##\n("entity"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n##\n("entity"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia\'s Alhamia Prison)\n##\n("entity"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n##\n("entity"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n##\n("entity"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n##\n("relationship"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n##\n("relationship"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n<|COMPLETE|>\n\n######################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: is a very, very thick layer that surrounds the brain. That gets actually resected in a process called directomy. And that then exposed the Pia and the brain that you want to insert. And by the time it\'s been around anywhere between one to one and a half hours, robot comes in, does this thing, placement of the target, inserting of the thread. That takes anywhere between 20 to 40 minutes in the particular case for Nolan, it was just under or just over 30 minutes, and then after that the surgeon comes in, there\'s a couple other steps of like actually inserting the derail substitute layer to protect the thread as well as the brain and then yeah screw in the implant and then skin flat and then suture and then you\'re out. So when Nolan woke up, what was that like? Was the recovery like and what was the first time he was able to use it? So he was actually immediately after the surgery, you know, like an\n######################\nOutput:'}
16:51:14,378 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n \n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity\'s attributes and activities\nFormat each entity as ("entity"<|><entity_name><|><entity_type><|><entity_description>)\n \n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as ("relationship"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n \n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n \n4. When finished, output <|COMPLETE|>\n \n######################\n-Examples-\n######################\nExample 1:\nEntity_types: ORGANIZATION,PERSON\nText:\nThe Verdantis\'s Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n######################\nOutput:\n("entity"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n##\n("entity"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n##\n("entity"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis\'s money supply)\n##\n("relationship"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n<|COMPLETE|>\n\n######################\nExample 2:\nEntity_types: ORGANIZATION\nText:\nTechGlobal\'s (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation\'s debut on the public markets isn\'t indicative of how other newly listed companies may perform.\n\nTechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n######################\nOutput:\n("entity"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n##\n("entity"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n##\n("relationship"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n<|COMPLETE|>\n\n######################\nExample 3:\nEntity_types: ORGANIZATION,GEO,PERSON\nText:\nFive Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n\nThe swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n\nThe exchange initiated in Firuzabad\'s capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n\nThey were welcomed by senior Aurelian officials and are now on their way to Aurelia\'s capital, Cashion.\n\nThe Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia\'s Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n######################\nOutput:\n("entity"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n##\n("entity"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n##\n("entity"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n##\n##\n("entity"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n##\n("entity"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n##\n("entity"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n##\n("entity"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia\'s Alhamia Prison)\n##\n("entity"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n##\n("entity"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n##\n("entity"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n##\n("relationship"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n##\n("relationship"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n<|COMPLETE|>\n\n######################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: ulate that. You know, obviously there have been other, other, you know, as you mentioned, pioneers that have participated in these groundbreaking BCI, you know, investigational early feasibility studies. So we\'re obviously standing in the shoulders of the giants here, you know, we\'re not the first ones to actually put electrodes in a human, human brain. But, I mean, just leading up to the surgery, there was, I definitely could not sleep. There\'s just, it\'s the first time that you\'re working in a completely new environment. We had a lot of confidence based on our bench top testing, or pre-clinical R&D studies that the mechanism, the threads, the insertion, all that stuff is very safe and that it\'s obviously ready for doing this in a human, but there\'s still a lot of unknown, unknown about can the needle actually insert? I mean, we brought something like 40 needles just in case they break. And we ended up\n######################\nOutput:'}
16:51:16,445 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n \n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity\'s attributes and activities\nFormat each entity as ("entity"<|><entity_name><|><entity_type><|><entity_description>)\n \n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as ("relationship"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n \n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n \n4. When finished, output <|COMPLETE|>\n \n######################\n-Examples-\n######################\nExample 1:\nEntity_types: ORGANIZATION,PERSON\nText:\nThe Verdantis\'s Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n######################\nOutput:\n("entity"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n##\n("entity"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n##\n("entity"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis\'s money supply)\n##\n("relationship"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n<|COMPLETE|>\n\n######################\nExample 2:\nEntity_types: ORGANIZATION\nText:\nTechGlobal\'s (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation\'s debut on the public markets isn\'t indicative of how other newly listed companies may perform.\n\nTechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n######################\nOutput:\n("entity"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n##\n("entity"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n##\n("relationship"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n<|COMPLETE|>\n\n######################\nExample 3:\nEntity_types: ORGANIZATION,GEO,PERSON\nText:\nFive Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n\nThe swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n\nThe exchange initiated in Firuzabad\'s capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n\nThey were welcomed by senior Aurelian officials and are now on their way to Aurelia\'s capital, Cashion.\n\nThe Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia\'s Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n######################\nOutput:\n("entity"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n##\n("entity"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n##\n("entity"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n##\n##\n("entity"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n##\n("entity"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n##\n("entity"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n##\n("entity"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia\'s Alhamia Prison)\n##\n("entity"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n##\n("entity"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n##\n("entity"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n##\n("relationship"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n##\n("relationship"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n<|COMPLETE|>\n\n######################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: I mean, we brought something like 40 needles just in case they break. And we ended up using only one, but I mean, that was a level of just complete unknown, right, because it\'s a very, very different environment. And I mean, that\'s why we do clinical trial in the first place to be able to test these things out. So extreme nervousness and just,, many, many sleepless night leading up to the surgery and definitely the day before the surgery, and it was an early morning surgery, like we started at 7 in the morning, and by the time it was around 1030, it was, it was, everything was done. But, I mean, first time seeing that, well, number one, just huge relief, that this thing is, you know, doing what it\'s supposed to do. And two, I mean, just immense amount of gratitude for Nolan and his family. And then many others that have applied and that we\n######################\nOutput:'}
16:51:18,894 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:51:18,895 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 175.29699999999139. input_tokens=1936, output_tokens=82
16:51:21,765 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:51:21,767 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 173.15699999999197. input_tokens=1936, output_tokens=135
16:51:25,700 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:51:25,702 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 145.03100000000268. input_tokens=1936, output_tokens=100
16:51:26,457 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:51:26,460 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 167.32800000000861. input_tokens=1936, output_tokens=228
16:51:26,680 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:51:26,682 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 176.01600000000326. input_tokens=1936, output_tokens=266
16:51:34,303 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:51:34,304 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 164.75. input_tokens=1935, output_tokens=256
16:51:41,224 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:51:41,226 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 153.60899999999674. input_tokens=1936, output_tokens=160
16:51:44,609 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:51:44,610 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:51:44,611 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 131.93700000000536. input_tokens=1936, output_tokens=220
16:51:44,613 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 137.95300000000861. input_tokens=1936, output_tokens=225
16:51:53,988 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:51:53,991 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 136.15600000000268. input_tokens=1936, output_tokens=355
16:51:56,486 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:51:56,488 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 124.89100000000326. input_tokens=1936, output_tokens=314
16:51:58,293 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:51:58,296 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 136.60899999999674. input_tokens=1935, output_tokens=524
16:52:46,242 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:52:46,249 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 168.6090000000113. input_tokens=34, output_tokens=4000
16:52:53,194 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:52:53,196 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 134.32799999999406. input_tokens=1935, output_tokens=103
16:52:54,289 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:52:54,291 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 135.89100000000326. input_tokens=1936, output_tokens=148
16:52:56,971 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:52:56,974 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 173.93700000000536. input_tokens=34, output_tokens=222
16:52:59,33 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:52:59,35 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 125.93799999999464. input_tokens=1936, output_tokens=116
16:53:01,377 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:01,380 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 142.5. input_tokens=1936, output_tokens=207
16:53:05,666 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:05,668 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 128.89100000000326. input_tokens=1936, output_tokens=218
16:53:08,250 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:08,252 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 153.81200000000536. input_tokens=1936, output_tokens=515
16:53:09,895 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:09,897 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 133.0619999999908. input_tokens=1936, output_tokens=253
16:53:12,406 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:12,408 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 126.90599999998813. input_tokens=1936, output_tokens=133
16:53:16,296 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:16,298 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 137.01600000000326. input_tokens=1936, output_tokens=384
16:53:18,102 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:18,104 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 120.46800000000803. input_tokens=1936, output_tokens=196
16:53:19,485 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:19,488 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 123.78100000000268. input_tokens=1937, output_tokens=222
16:53:22,52 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:22,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 123.15700000000652. input_tokens=1936, output_tokens=210
16:53:24,251 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:24,253 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 117.57799999999406. input_tokens=1936, output_tokens=24
16:53:26,86 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:26,87 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 120.39100000000326. input_tokens=1936, output_tokens=163
16:53:27,522 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:27,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 125.75. input_tokens=1935, output_tokens=202
16:53:30,669 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:30,669 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 106.04699999999139. input_tokens=1936, output_tokens=80
16:53:31,52 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:31,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 109.8289999999979. input_tokens=1935, output_tokens=104
16:53:31,460 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:31,461 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 93.17200000000594. input_tokens=1936, output_tokens=20
16:53:32,298 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:32,308 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 118.0. input_tokens=1937, output_tokens=220
16:53:33,439 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:33,440 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 126.98399999999674. input_tokens=1936, output_tokens=346
16:53:36,93 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:36,96 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 49.84399999999732. input_tokens=34, output_tokens=167
16:53:37,489 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:37,491 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 101.0. input_tokens=34, output_tokens=187
16:53:39,263 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:39,265 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 105.26599999998871. input_tokens=34, output_tokens=334
16:53:42,154 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:42,156 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 117.53099999998813. input_tokens=34, output_tokens=445
16:53:45,642 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:45,645 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 52.45299999999406. input_tokens=34, output_tokens=350
16:53:47,134 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:47,136 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 48.09399999999732. input_tokens=1936, output_tokens=177
16:53:48,888 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:48,890 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 43.21899999999732. input_tokens=34, output_tokens=26
16:53:49,884 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:49,886 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 55.59399999999732. input_tokens=34, output_tokens=422
16:53:50,274 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:50,276 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 42.01499999999942. input_tokens=34, output_tokens=58
16:53:50,668 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:50,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.76600000000326. input_tokens=34, output_tokens=27
16:53:51,729 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:51,731 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.312000000005355. input_tokens=34, output_tokens=58
16:53:53,511 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:53,512 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.40699999999197. input_tokens=34, output_tokens=80
16:53:56,403 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:56,405 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 55.01499999999942. input_tokens=34, output_tokens=432
16:53:56,673 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:56,675 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.39100000000326. input_tokens=34, output_tokens=273
16:53:58,8 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:58,9 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.95299999999406. input_tokens=34, output_tokens=70
16:53:58,233 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:58,235 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.96800000000803. input_tokens=34, output_tokens=65
16:53:58,556 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:58,558 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.062999999994645. input_tokens=34, output_tokens=222
16:53:59,1 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:53:59,5 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 62.03199999999197. input_tokens=34, output_tokens=755
16:54:00,514 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:00,516 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.421999999991385. input_tokens=34, output_tokens=96
16:54:03,30 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:03,32 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 31.561999999990803. input_tokens=34, output_tokens=116
16:54:06,145 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:06,148 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.48399999999674. input_tokens=34, output_tokens=342
16:54:06,731 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:06,733 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.421000000002095. input_tokens=34, output_tokens=167
16:54:07,686 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:07,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.625. input_tokens=34, output_tokens=387
16:54:10,479 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:10,481 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 32.98399999999674. input_tokens=34, output_tokens=122
16:54:12,820 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:12,823 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.375. input_tokens=34, output_tokens=282
16:54:14,351 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:14,354 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 38.26499999999942. input_tokens=34, output_tokens=287
16:54:15,387 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:15,390 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 47.86000000000058. input_tokens=34, output_tokens=709
16:54:16,395 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:16,398 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 34.25. input_tokens=1935, output_tokens=109
16:54:17,283 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:17,286 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 38.03100000000268. input_tokens=34, output_tokens=250
16:54:17,508 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:17,510 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 30.375. input_tokens=34, output_tokens=84
16:54:19,349 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:19,352 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.703000000008615. input_tokens=34, output_tokens=215
16:54:20,47 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:20,49 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.172000000005937. input_tokens=1935, output_tokens=148
16:54:22,491 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:22,493 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 32.60900000001129. input_tokens=34, output_tokens=220
16:54:26,888 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:26,891 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.61000000000058. input_tokens=34, output_tokens=417
16:54:28,312 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:28,314 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 31.90700000000652. input_tokens=34, output_tokens=61
16:54:31,294 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:31,296 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.78100000000268. input_tokens=34, output_tokens=408
16:54:35,123 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:35,125 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 43.39100000000326. input_tokens=34, output_tokens=682
16:54:35,797 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:35,798 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.78100000000268. input_tokens=34, output_tokens=195
16:54:36,546 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:36,547 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 38.312999999994645. input_tokens=34, output_tokens=52
16:54:38,240 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:38,241 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.23400000001129. input_tokens=34, output_tokens=24
16:54:38,872 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:38,874 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 38.35900000001129. input_tokens=34, output_tokens=23
16:54:42,52 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:42,54 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 45.375. input_tokens=34, output_tokens=570
16:54:42,528 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:42,530 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.375. input_tokens=34, output_tokens=16
16:54:42,898 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:42,900 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.34299999999348. input_tokens=34, output_tokens=266
16:54:46,968 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:46,969 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.28100000000268. input_tokens=34, output_tokens=116
16:54:47,505 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:47,508 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.48500000000058. input_tokens=34, output_tokens=322
16:54:50,39 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:50,40 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 43.312999999994645. input_tokens=34, output_tokens=263
16:54:55,258 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:54:55,260 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 42.437999999994645. input_tokens=34, output_tokens=280
16:55:02,513 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:55:02,515 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 47.125. input_tokens=1936, output_tokens=27
16:55:04,673 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:55:04,675 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 54.203999999997905. input_tokens=34, output_tokens=348
16:55:19,744 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:55:19,746 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 63.344000000011874. input_tokens=1936, output_tokens=84
16:55:23,89 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:55:23,90 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 65.79700000000594. input_tokens=1936, output_tokens=100
16:55:24,104 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:55:24,107 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 69.75. input_tokens=1936, output_tokens=349
16:55:34,545 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:55:34,546 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 75.18799999999464. input_tokens=34, output_tokens=102
16:55:44,765 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:55:44,766 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 84.70299999999406. input_tokens=1936, output_tokens=152
16:55:55,536 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:55:55,539 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 93.04699999999139. input_tokens=1936, output_tokens=96
16:55:56,430 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:55:56,432 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 98.92200000000594. input_tokens=1935, output_tokens=259
16:56:07,555 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:56:07,558 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 136.89100000000326. input_tokens=34, output_tokens=1899
16:56:13,891 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:56:13,893 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 105.56299999999464. input_tokens=1935, output_tokens=104
16:56:17,639 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:56:17,640 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 106.34399999999732. input_tokens=1937, output_tokens=217
16:56:20,460 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:56:20,462 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 113.57800000000861. input_tokens=1936, output_tokens=329
16:56:24,403 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:56:24,405 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 109.28099999998813. input_tokens=1935, output_tokens=251
16:56:29,600 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:56:29,601 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 111.35899999999674. input_tokens=1936, output_tokens=71
16:56:31,220 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:56:31,222 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 114.68700000000536. input_tokens=1936, output_tokens=202
16:56:35,907 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:56:35,909 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 113.85899999999674. input_tokens=1936, output_tokens=100
16:56:36,544 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:56:36,546 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 114.01600000000326. input_tokens=1936, output_tokens=60
16:56:38,849 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:56:38,851 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 115.95300000000861. input_tokens=1935, output_tokens=111
16:56:40,977 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:56:40,979 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 122.10899999999674. input_tokens=1936, output_tokens=339
16:56:46,334 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:56:46,336 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 130.53100000000268. input_tokens=1935, output_tokens=601
16:56:50,462 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:56:50,463 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 122.95300000000861. input_tokens=1936, output_tokens=192
16:56:54,281 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:56:54,283 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 124.23399999999674. input_tokens=1936, output_tokens=212
16:56:57,41 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:56:57,44 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 112.35899999999674. input_tokens=1936, output_tokens=47
16:56:58,163 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:56:58,165 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 115.65600000000268. input_tokens=1936, output_tokens=195
16:56:58,186 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:56:58,187 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 122.92200000000594. input_tokens=1936, output_tokens=288
16:57:01,791 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:57:01,792 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 98.70299999999406. input_tokens=1936, output_tokens=77
16:57:03,280 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:57:03,282 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 103.53099999998813. input_tokens=1936, output_tokens=176
16:57:06,727 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:57:06,729 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 81.96800000000803. input_tokens=1936, output_tokens=83
16:57:08,513 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:57:08,515 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 93.96899999999732. input_tokens=1936, output_tokens=212
16:57:10,759 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:57:10,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 106.65699999999197. input_tokens=34, output_tokens=386
16:57:10,821 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:57:10,823 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 75.28100000000268. input_tokens=1936, output_tokens=163
16:57:15,850 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:57:15,851 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 79.4210000000021. input_tokens=34, output_tokens=100
16:57:37,973 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:57:37,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 84.07800000000861. input_tokens=1936, output_tokens=222
16:57:40,156 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:57:40,158 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 82.51499999999942. input_tokens=1936, output_tokens=251
16:57:46,826 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:57:46,828 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 86.35899999999674. input_tokens=1935, output_tokens=156
16:57:46,999 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n \n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity\'s attributes and activities\nFormat each entity as ("entity"<|><entity_name><|><entity_type><|><entity_description>)\n \n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as ("relationship"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n \n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n \n4. When finished, output <|COMPLETE|>\n \n######################\n-Examples-\n######################\nExample 1:\nEntity_types: ORGANIZATION,PERSON\nText:\nThe Verdantis\'s Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n######################\nOutput:\n("entity"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n##\n("entity"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n##\n("entity"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis\'s money supply)\n##\n("relationship"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n<|COMPLETE|>\n\n######################\nExample 2:\nEntity_types: ORGANIZATION\nText:\nTechGlobal\'s (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation\'s debut on the public markets isn\'t indicative of how other newly listed companies may perform.\n\nTechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n######################\nOutput:\n("entity"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n##\n("entity"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n##\n("relationship"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n<|COMPLETE|>\n\n######################\nExample 3:\nEntity_types: ORGANIZATION,GEO,PERSON\nText:\nFive Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n\nThe swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n\nThe exchange initiated in Firuzabad\'s capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n\nThey were welcomed by senior Aurelian officials and are now on their way to Aurelia\'s capital, Cashion.\n\nThe Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia\'s Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n######################\nOutput:\n("entity"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n##\n("entity"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n##\n("entity"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n##\n##\n("entity"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n##\n("entity"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n##\n("entity"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n##\n("entity"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia\'s Alhamia Prison)\n##\n("entity"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n##\n("entity"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n##\n("entity"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n##\n("relationship"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n##\n("relationship"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n<|COMPLETE|>\n\n######################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: external camera with this BCI system, you\'re not limited to that. You can have infrared, you can have UV, you can have whatever other spectrum that you you want to see. And whether that gets mapped to some sort of weird conscious experience, I\'ve no idea. But when I, you know, oftentimes I talk to people about the goal of Neuralink being going beyond the limits of our biology. That\'s sort of what I mean. And if you\'re able to control the kind of raw signal, is that when we use our site, we\'re getting the photons, and there\'s not much processing on it. If you\'re able to control that signal, maybe you can do some kind of processing. Maybe you do object detection ahead of time. Yeah, you\'re doing some kind of pre-processing, and there\'s a lot of possibilities to explore that. So it\'s not just increasing sort of thermermermermermermermermermermermermermermer\n######################\nOutput:'}
16:57:53,81 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:57:53,83 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 88.6880000000092. input_tokens=1936, output_tokens=175
16:58:04,289 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:58:04,291 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 94.68799999999464. input_tokens=1935, output_tokens=234
16:58:05,969 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:58:05,971 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 90.06200000000536. input_tokens=1936, output_tokens=191
16:58:06,166 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:58:06,168 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 94.93799999999464. input_tokens=1936, output_tokens=255
16:58:22,205 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:58:22,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 101.23500000000058. input_tokens=1936, output_tokens=78
16:58:24,728 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:58:24,730 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 98.38999999999942. input_tokens=1936, output_tokens=36
16:58:25,430 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:58:25,432 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 106.5789999999979. input_tokens=1936, output_tokens=150
16:58:28,417 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:58:28,420 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 111.875. input_tokens=1936, output_tokens=217
16:58:36,734 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:58:36,736 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 106.26499999999942. input_tokens=1937, output_tokens=95
16:58:39,279 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:58:39,282 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 151.71799999999348. input_tokens=1936, output_tokens=930
16:58:42,670 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:58:42,672 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 104.48399999999674. input_tokens=34, output_tokens=27
16:58:44,874 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:58:44,876 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 107.82800000000861. input_tokens=1936, output_tokens=208
16:58:46,44 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:58:46,46 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 107.875. input_tokens=1935, output_tokens=226
16:58:46,146 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:58:46,147 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 111.85899999999674. input_tokens=1936, output_tokens=307
16:58:47,598 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:58:47,600 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 104.32800000000861. input_tokens=34, output_tokens=99
16:58:49,862 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:58:49,863 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 103.14100000000326. input_tokens=34, output_tokens=93
16:58:51,653 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:58:51,654 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 109.85899999999674. input_tokens=1936, output_tokens=308
16:58:51,898 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:58:51,900 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 101.07799999999406. input_tokens=34, output_tokens=77
16:58:52,848 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:58:52,849 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 97.0. input_tokens=34, output_tokens=40
16:58:54,968 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:58:54,971 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 104.20300000000861. input_tokens=1936, output_tokens=208
16:58:57,752 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:58:57,754 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 77.59399999999732. input_tokens=1936, output_tokens=127
16:59:00,289 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:00,291 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 111.78100000000268. input_tokens=34, output_tokens=454
16:59:00,550 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:00,552 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 71.625. input_tokens=1936, output_tokens=200
16:59:01,657 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:01,659 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 68.57799999999406. input_tokens=34, output_tokens=47
16:59:02,75 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:02,79 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 84.09399999999732. input_tokens=34, output_tokens=328
16:59:02,239 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:02,240 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 56.078000000008615. input_tokens=34, output_tokens=16
16:59:03,324 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:03,326 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 76.5. input_tokens=34, output_tokens=229
16:59:04,455 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:04,456 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.73500000000058. input_tokens=34, output_tokens=48
16:59:06,549 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:06,551 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.34399999999732. input_tokens=34, output_tokens=142
16:59:06,973 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:06,975 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 41.546000000002095. input_tokens=34, output_tokens=60
16:59:08,726 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:08,728 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.312000000005355. input_tokens=34, output_tokens=90
16:59:10,489 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:10,491 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 66.20300000000861. input_tokens=34, output_tokens=376
16:59:12,595 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:12,596 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 29.937000000005355. input_tokens=34, output_tokens=96
16:59:13,454 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:13,456 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 28.593999999997322. input_tokens=34, output_tokens=34
16:59:14,758 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:14,760 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.48500000000058. input_tokens=34, output_tokens=215
16:59:16,100 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:16,102 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 30.062000000005355. input_tokens=34, output_tokens=71
16:59:16,862 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:16,865 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.125. input_tokens=34, output_tokens=383
16:59:21,359 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:21,362 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 75.375. input_tokens=34, output_tokens=793
16:59:21,815 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:21,818 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.67200000000594. input_tokens=34, output_tokens=325
16:59:23,636 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:23,639 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.76599999998871. input_tokens=34, output_tokens=322
16:59:24,20 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:24,22 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.421999999991385. input_tokens=34, output_tokens=369
16:59:25,183 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:25,184 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 32.328999999997905. input_tokens=34, output_tokens=16
16:59:25,818 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:25,820 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.92200000000594. input_tokens=34, output_tokens=130
16:59:28,141 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:28,144 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 30.389999999999418. input_tokens=34, output_tokens=84
16:59:28,799 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:28,801 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.15700000000652. input_tokens=34, output_tokens=237
16:59:30,33 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:30,34 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 35.062999999994645. input_tokens=1936, output_tokens=172
16:59:35,543 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:35,545 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.98399999999674. input_tokens=34, output_tokens=335
16:59:36,98 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:36,100 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.03100000000268. input_tokens=34, output_tokens=281
16:59:36,419 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:36,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 36.125. input_tokens=1935, output_tokens=436
16:59:36,727 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:36,728 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.40600000000268. input_tokens=34, output_tokens=16
16:59:37,18 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:37,19 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.35899999999674. input_tokens=34, output_tokens=355
16:59:38,468 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:38,470 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 31.485000000000582. input_tokens=34, output_tokens=66
16:59:40,342 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:40,344 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 31.610000000000582. input_tokens=34, output_tokens=36
16:59:40,691 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:40,693 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.13999999999942. input_tokens=34, output_tokens=127
16:59:41,96 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:41,98 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.63999999999942. input_tokens=34, output_tokens=149
16:59:42,544 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:42,546 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 29.077999999994063. input_tokens=34, output_tokens=64
16:59:43,642 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:43,643 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 41.405999999988126. input_tokens=34, output_tokens=275
16:59:46,454 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:46,457 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.86000000000058. input_tokens=34, output_tokens=194
16:59:50,164 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:50,166 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 34.062999999994645. input_tokens=1936, output_tokens=261
16:59:56,171 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:56,173 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.796999999991385. input_tokens=34, output_tokens=287
16:59:57,213 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
16:59:57,214 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.34399999999732. input_tokens=34, output_tokens=510
17:00:00,331 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:00:00,334 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 45.578000000008615. input_tokens=34, output_tokens=719
17:00:01,952 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:00:01,954 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.92200000000594. input_tokens=1935, output_tokens=17
17:00:07,522 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:00:07,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 43.88999999999942. input_tokens=1936, output_tokens=373
17:00:07,561 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:00:07,564 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 45.73500000000058. input_tokens=34, output_tokens=412
17:00:17,905 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:00:17,907 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 52.07799999999406. input_tokens=1936, output_tokens=141
17:00:18,504 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:00:18,507 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 53.32799999999406. input_tokens=34, output_tokens=413
17:00:19,194 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:00:19,196 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 50.38999999999942. input_tokens=34, output_tokens=42
17:00:21,267 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:00:21,269 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 53.125. input_tokens=1936, output_tokens=219
17:00:21,990 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:00:21,991 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 46.453000000008615. input_tokens=1936, output_tokens=59
17:00:24,36 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:00:24,38 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 47.625. input_tokens=1936, output_tokens=83
17:00:26,104 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:00:26,106 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 56.062000000005355. input_tokens=34, output_tokens=253
17:00:40,408 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:00:40,411 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 63.687999999994645. input_tokens=1935, output_tokens=181
17:00:44,788 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:00:44,789 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 68.68799999999464. input_tokens=34, output_tokens=340
17:00:57,818 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:00:57,820 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 77.48399999999674. input_tokens=1936, output_tokens=42
17:01:00,775 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:01:00,776 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 80.07799999999406. input_tokens=1936, output_tokens=31
17:01:01,869 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:01:01,870 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 84.84400000001187. input_tokens=1936, output_tokens=266
17:01:15,866 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:01:15,880 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 125.39099999998871. input_tokens=34, output_tokens=1854
17:01:25,331 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:01:25,333 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 106.86000000000058. input_tokens=1936, output_tokens=267
17:01:32,11 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:01:32,13 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 108.36000000000058. input_tokens=1936, output_tokens=27
17:01:34,163 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:01:34,164 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 107.70299999999406. input_tokens=1936, output_tokens=27
17:01:35,145 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:01:35,147 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 104.98399999999674. input_tokens=1936, output_tokens=56
17:01:35,618 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:01:35,619 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 114.51600000000326. input_tokens=1936, output_tokens=219
17:01:39,631 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:01:39,634 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 117.09399999999732. input_tokens=1936, output_tokens=205
17:01:49,232 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:01:49,233 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 112.01499999999942. input_tokens=1936, output_tokens=92
17:01:49,624 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:01:49,626 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 109.28100000000268. input_tokens=1936, output_tokens=88
17:01:50,992 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:01:50,994 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 114.82800000000861. input_tokens=1937, output_tokens=199
17:01:52,860 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:01:52,862 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 110.90600000000268. input_tokens=1936, output_tokens=123
17:02:02,260 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:02:02,262 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 104.36000000000058. input_tokens=1936, output_tokens=122
17:02:04,316 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:02:04,318 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 116.79700000000594. input_tokens=1937, output_tokens=221
17:02:07,267 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:02:07,270 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 108.76499999999942. input_tokens=1936, output_tokens=219
17:02:11,891 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:02:11,893 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 124.32799999999406. input_tokens=1936, output_tokens=292
17:02:12,922 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:02:12,923 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 113.71899999999732. input_tokens=1936, output_tokens=158
17:02:19,158 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:02:19,160 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 113.06299999999464. input_tokens=1937, output_tokens=217
17:02:22,379 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:02:22,381 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 120.39100000000326. input_tokens=1935, output_tokens=359
17:02:24,855 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:02:24,858 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 123.57800000000861. input_tokens=1936, output_tokens=524
17:02:26,709 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:02:26,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 106.29699999999139. input_tokens=1936, output_tokens=216
17:02:35,915 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:02:35,917 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 95.14100000000326. input_tokens=1936, output_tokens=356
17:02:38,36 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:02:38,48 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 100.21899999999732. input_tokens=1936, output_tokens=422
17:02:42,150 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:02:42,152 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 100.28099999998813. input_tokens=1936, output_tokens=174
17:02:45,747 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:02:45,750 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 120.95300000000861. input_tokens=34, output_tokens=744
17:02:45,830 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:02:45,831 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 80.49999999998545. input_tokens=1936, output_tokens=98
17:02:56,200 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:02:56,202 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 100.3120000000199. input_tokens=1936, output_tokens=306
17:03:05,941 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:05,943 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 93.9370000000199. input_tokens=1936, output_tokens=97
17:03:08,800 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:08,802 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 93.65700000000652. input_tokens=1936, output_tokens=65
17:03:10,788 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:10,789 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 95.17199999999139. input_tokens=1935, output_tokens=127
17:03:15,704 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:15,706 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 101.54699999999139. input_tokens=1936, output_tokens=389
17:03:22,122 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:22,123 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 92.89100000000326. input_tokens=1936, output_tokens=239
17:03:24,56 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
17:03:37,394 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:37,395 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 104.53099999998813. input_tokens=34, output_tokens=17
17:03:39,320 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:39,322 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 109.70300000000861. input_tokens=1936, output_tokens=180
17:03:42,118 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:42,120 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 122.48400000001129. input_tokens=34, output_tokens=490
17:03:43,721 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:43,722 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 96.45300000000861. input_tokens=34, output_tokens=61
17:03:46,172 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:46,174 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 115.17199999999139. input_tokens=1936, output_tokens=290
17:03:46,254 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:46,256 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 101.9380000000092. input_tokens=1936, output_tokens=193
17:03:48,157 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:48,159 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 96.26600000000326. input_tokens=1936, output_tokens=82
17:03:50,11 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:50,13 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 107.75000000001455. input_tokens=34, output_tokens=368
17:03:50,641 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:50,643 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 97.71799999999348. input_tokens=1936, output_tokens=126
17:03:52,686 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:52,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 87.8289999999979. input_tokens=34, output_tokens=64
17:03:52,920 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:52,922 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 86.20300000000861. input_tokens=1936, output_tokens=43
17:03:54,669 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:54,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 95.5. input_tokens=34, output_tokens=249
17:03:56,387 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:56,389 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 94.0. input_tokens=34, output_tokens=246
17:03:57,834 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:57,835 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 81.92199999999139. input_tokens=34, output_tokens=114
17:03:58,200 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:58,202 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 76.04700000002049. input_tokens=34, output_tokens=42
17:03:59,967 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:03:59,969 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 74.21899999998277. input_tokens=34, output_tokens=46
17:04:01,921 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:01,924 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 83.875. input_tokens=1936, output_tokens=228
17:04:03,824 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:03,827 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 55.01500000001397. input_tokens=34, output_tokens=27
17:04:04,689 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:04,691 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 53.90600000001723. input_tokens=34, output_tokens=27
17:04:05,559 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:05,562 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 69.35999999998603. input_tokens=1936, output_tokens=202
17:04:10,276 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:10,278 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 84.43700000000536. input_tokens=34, output_tokens=437
17:04:12,269 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:12,270 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:12,272 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 66.32799999997951. input_tokens=34, output_tokens=491
17:04:12,275 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 56.562000000005355. input_tokens=34, output_tokens=351
17:04:12,882 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:12,883 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.485000000015134. input_tokens=34, output_tokens=16
17:04:14,319 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:14,321 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 32.203000000008615. input_tokens=34, output_tokens=61
17:04:14,940 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:14,942 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 52.828000000008615. input_tokens=34, output_tokens=433
17:04:19,708 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:19,711 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.390999999974156. input_tokens=34, output_tokens=341
17:04:21,822 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:21,824 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.64000000001397. input_tokens=34, output_tokens=333
17:04:22,374 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:22,376 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 38.64100000000326. input_tokens=34, output_tokens=376
17:04:24,502 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:24,503 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.344000000011874. input_tokens=34, output_tokens=117
17:04:25,281 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:25,282 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.014999999984866. input_tokens=34, output_tokens=246
17:04:29,514 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:29,515 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:29,516 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.5. input_tokens=34, output_tokens=329
17:04:29,519 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.84299999999348. input_tokens=34, output_tokens=200
17:04:34,410 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:34,413 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 43.76600000000326. input_tokens=34, output_tokens=459
17:04:36,391 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:36,392 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 43.469000000011874. input_tokens=34, output_tokens=303
17:04:38,80 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:38,82 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 43.421999999991385. input_tokens=34, output_tokens=376
17:04:39,495 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:39,497 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 41.65600000001723. input_tokens=34, output_tokens=124
17:04:45,478 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:45,480 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 49.09299999999348. input_tokens=34, output_tokens=464
17:04:51,777 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:51,779 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 53.57799999997951. input_tokens=34, output_tokens=553
17:04:55,132 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:55,134 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 51.312999999994645. input_tokens=1936, output_tokens=89
17:04:55,781 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:55,783 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:55,786 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 53.85899999999674. input_tokens=34, output_tokens=367
17:04:55,790 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 1 retries took 90.59399999998277. input_tokens=34, output_tokens=1950
17:04:57,629 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:04:57,630 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 47.360000000015134. input_tokens=34, output_tokens=66
17:05:03,457 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:05:03,459 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 51.187999999994645. input_tokens=34, output_tokens=195
17:05:04,17 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:05:04,19 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 59.32799999997951. input_tokens=34, output_tokens=310
17:05:10,530 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:05:10,531 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 58.25. input_tokens=34, output_tokens=296
17:05:11,565 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:05:11,567 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 66.01500000001397. input_tokens=34, output_tokens=601
17:05:17,371 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:05:17,372 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 62.421999999991385. input_tokens=1936, output_tokens=197
17:05:18,404 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:05:18,406 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 58.687000000005355. input_tokens=34, output_tokens=36
17:05:23,137 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:05:23,139 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 68.81299999999464. input_tokens=34, output_tokens=281
17:05:37,104 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:05:37,107 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 84.21799999999348. input_tokens=34, output_tokens=607
17:05:39,267 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:05:39,270 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 77.45299999997951. input_tokens=1936, output_tokens=135
17:05:38,804 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:05:38,806 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 78.65599999998813. input_tokens=1936, output_tokens=194
17:05:41,890 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:05:41,892 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 74.59400000001187. input_tokens=1936, output_tokens=80
17:05:42,446 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:05:42,448 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 75.15700000000652. input_tokens=34, output_tokens=17
17:05:43,979 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:05:43,981 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 80.93799999999464. input_tokens=34, output_tokens=242
17:05:45,710 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:05:45,712 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 73.53100000001723. input_tokens=34, output_tokens=84
17:05:54,686 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:05:54,689 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 116.95300000000861. input_tokens=34, output_tokens=1908
17:05:56,208 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:05:56,210 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 80.35900000002584. input_tokens=34, output_tokens=52
17:05:58,38 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:05:58,40 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 83.875. input_tokens=1936, output_tokens=92
17:05:58,705 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:05:58,708 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 81.43799999999464. input_tokens=1936, output_tokens=46
17:06:00,504 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:00,506 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 77.25. input_tokens=1936, output_tokens=106
17:06:05,160 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:05,162 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 71.59400000001187. input_tokens=1936, output_tokens=184
17:06:07,148 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:07,152 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 104.875. input_tokens=34, output_tokens=776
17:06:08,808 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:08,810 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 79.26600000000326. input_tokens=1936, output_tokens=383
17:06:13,343 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:13,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 72.10900000002584. input_tokens=1936, output_tokens=144
17:06:14,9 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:14,12 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 81.10899999999674. input_tokens=1936, output_tokens=533
17:06:16,742 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:16,744 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 83.18700000000536. input_tokens=34, output_tokens=349
17:06:18,36 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:18,37 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 76.23500000001513. input_tokens=34, output_tokens=109
17:06:21,245 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:21,246 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 66.10899999999674. input_tokens=1936, output_tokens=82
17:06:22,159 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:22,162 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 73.86000000001513. input_tokens=1936, output_tokens=204
17:06:25,579 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:25,580 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 76.23499999998603. input_tokens=1936, output_tokens=224
17:06:25,950 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:25,951 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 65.04699999999139. input_tokens=1936, output_tokens=99
17:06:27,845 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:27,847 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 50.79700000002049. input_tokens=1936, output_tokens=33
17:06:28,987 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:28,988 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 50.171999999991385. input_tokens=1936, output_tokens=40
17:06:29,296 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:29,298 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 93.88999999998487. input_tokens=1936, output_tokens=589
17:06:31,353 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:31,355 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 48.905999999988126. input_tokens=1936, output_tokens=30
17:06:32,465 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:32,468 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 76.29700000002049. input_tokens=1936, output_tokens=301
17:06:36,211 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:36,213 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 50.5. input_tokens=1936, output_tokens=108
17:06:37,151 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:37,153 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 53.17200000002049. input_tokens=1936, output_tokens=187
17:06:37,194 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:37,196 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 62.328999999997905. input_tokens=1936, output_tokens=316
17:06:39,981 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:39,982 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 41.937999999994645. input_tokens=1936, output_tokens=14
17:06:43,371 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:43,372 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 47.155999999988126. input_tokens=1936, output_tokens=155
17:06:44,657 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:44,659 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 62.76600000000326. input_tokens=1936, output_tokens=396
17:06:46,705 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:46,707 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 52.01600000000326. input_tokens=1936, output_tokens=236
17:06:50,99 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:50,102 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 51.405999999988126. input_tokens=1936, output_tokens=275
17:06:51,573 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:51,575 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 51.062999999994645. input_tokens=1936, output_tokens=196
17:06:51,958 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:51,959 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 43.15600000001723. input_tokens=34, output_tokens=63
17:06:54,652 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:54,653 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 49.485000000015134. input_tokens=1936, output_tokens=224
17:06:58,52 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:06:58,53 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 50.905999999988126. input_tokens=1936, output_tokens=282
17:07:00,870 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:00,872 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 47.530999999988126. input_tokens=1936, output_tokens=203
17:07:02,189 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:02,191 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 45.437999999994645. input_tokens=1936, output_tokens=176
17:07:04,87 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:04,89 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 46.04700000002049. input_tokens=1936, output_tokens=108
17:07:06,859 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:06,860 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 52.84399999998277. input_tokens=1936, output_tokens=348
17:07:08,123 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:08,125 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 46.875. input_tokens=1936, output_tokens=201
17:07:12,996 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:12,998 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 47.421000000002095. input_tokens=1936, output_tokens=338
17:07:14,568 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:14,570 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 52.40700000000652. input_tokens=1936, output_tokens=393
17:07:16,989 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:16,991 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 51.046000000002095. input_tokens=34, output_tokens=300
17:07:17,699 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:17,700 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 49.85999999998603. input_tokens=1936, output_tokens=278
17:07:17,823 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:17,825 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 48.844000000011874. input_tokens=1935, output_tokens=113
17:07:20,668 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:20,670 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.45299999997951. input_tokens=34, output_tokens=126
17:07:24,129 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:24,131 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 51.671999999991385. input_tokens=34, output_tokens=243
17:07:25,990 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:25,992 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 56.703000000008615. input_tokens=1935, output_tokens=377
17:07:27,303 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:27,305 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 50.155999999988126. input_tokens=1936, output_tokens=160
17:07:28,809 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:28,811 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 57.453000000008615. input_tokens=34, output_tokens=340
17:07:30,229 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:30,231 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 53.030999999988126. input_tokens=1936, output_tokens=66
17:07:32,888 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:32,890 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 46.187000000005355. input_tokens=34, output_tokens=109
17:07:35,303 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:35,304 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 45.203000000008615. input_tokens=34, output_tokens=46
17:07:36,55 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:36,57 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 51.405999999988126. input_tokens=1936, output_tokens=200
17:07:36,869 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:36,872 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 56.89000000001397. input_tokens=1935, output_tokens=254
17:07:38,852 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:38,853 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 47.280999999988126. input_tokens=34, output_tokens=91
17:07:39,766 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:39,768 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 56.39100000000326. input_tokens=1936, output_tokens=311
17:07:40,487 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:40,489 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 45.82799999997951. input_tokens=1936, output_tokens=92
17:07:45,64 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:45,66 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.187999999994645. input_tokens=34, output_tokens=239
17:07:47,290 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:47,292 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 55.32799999997951. input_tokens=34, output_tokens=395
17:07:50,94 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:50,96 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 52.03100000001723. input_tokens=34, output_tokens=410
17:07:50,849 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:50,850 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 43.984000000025844. input_tokens=1936, output_tokens=110
17:07:50,868 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:50,870 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 46.780999999988126. input_tokens=1936, output_tokens=173
17:07:51,827 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:51,829 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 43.703999999997905. input_tokens=34, output_tokens=64
17:07:54,396 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:54,398 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 41.40700000000652. input_tokens=34, output_tokens=155
17:07:56,232 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:56,234 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 38.530999999988126. input_tokens=34, output_tokens=33
17:07:57,286 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:57,288 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.46799999999348. input_tokens=34, output_tokens=40
17:07:58,206 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:07:58,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 41.203999999997905. input_tokens=34, output_tokens=232
17:08:00,25 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:00,27 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.89100000000326. input_tokens=34, output_tokens=30
17:08:02,315 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:02,317 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 47.73399999999674. input_tokens=34, output_tokens=408
17:08:04,21 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:04,22 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.719000000011874. input_tokens=34, output_tokens=74
17:08:04,361 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:04,365 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 62.171999999991385. input_tokens=34, output_tokens=884
17:08:06,266 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:06,267 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.453000000008615. input_tokens=34, output_tokens=92
17:08:07,581 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:07,582 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.687999999994645. input_tokens=34, output_tokens=7
17:08:11,936 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:11,938 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.625. input_tokens=34, output_tokens=205
17:08:12,430 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:12,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 42.203000000008615. input_tokens=34, output_tokens=316
17:08:14,675 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:14,677 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.796999999991385. input_tokens=34, output_tokens=101
17:08:16,470 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:16,472 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 50.469000000011874. input_tokens=34, output_tokens=697
17:08:18,220 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:18,221 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.359000000025844. input_tokens=34, output_tokens=154
17:08:21,538 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:21,541 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 60.875. input_tokens=34, output_tokens=939
17:08:21,935 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:21,936 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 42.171999999991385. input_tokens=34, output_tokens=183
17:08:22,206 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:22,208 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 46.14100000000326. input_tokens=34, output_tokens=384
17:08:23,416 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:23,418 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 42.937000000005355. input_tokens=1936, output_tokens=155
17:08:23,786 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:23,788 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 38.73399999999674. input_tokens=34, output_tokens=73
17:08:25,522 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:25,523 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.65700000000652. input_tokens=34, output_tokens=79
17:08:29,469 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:29,471 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 42.17200000002049. input_tokens=34, output_tokens=330
17:08:31,259 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:31,262 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 41.171999999991385. input_tokens=34, output_tokens=412
17:08:31,332 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:31,334 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.921999999991385. input_tokens=34, output_tokens=82
17:08:31,854 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:31,857 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 41.0. input_tokens=34, output_tokens=370
17:08:35,316 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:35,318 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 43.48399999999674. input_tokens=34, output_tokens=392
17:08:37,285 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:37,287 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 41.04700000002049. input_tokens=34, output_tokens=210
17:08:40,43 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:40,45 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 41.84299999999348. input_tokens=1936, output_tokens=334
17:08:43,603 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:43,605 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 41.296999999991385. input_tokens=34, output_tokens=247
17:08:50,189 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:50,192 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 52.89100000000326. input_tokens=34, output_tokens=712
17:08:50,765 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:50,766 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 46.73399999999674. input_tokens=1936, output_tokens=386
17:08:51,209 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:51,212 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 51.187000000005355. input_tokens=34, output_tokens=613
17:08:54,74 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:54,76 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 49.703999999997905. input_tokens=1937, output_tokens=355
17:08:55,687 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:55,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 48.10899999999674. input_tokens=34, output_tokens=118
17:08:58,847 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:58,850 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 46.40600000001723. input_tokens=34, output_tokens=121
17:08:58,888 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:08:58,889 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 46.953000000008615. input_tokens=1936, output_tokens=204
17:09:00,662 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:00,663 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 46.0. input_tokens=1936, output_tokens=121
17:09:01,381 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:01,382 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 43.171999999991385. input_tokens=34, output_tokens=46
17:09:04,719 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:04,722 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 42.78100000001723. input_tokens=1936, output_tokens=98
17:09:06,338 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:06,339 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 49.875. input_tokens=1936, output_tokens=212
17:09:08,78 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:08,79 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.65700000000652. input_tokens=34, output_tokens=78
17:09:10,392 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:10,393 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 46.594000000011874. input_tokens=1936, output_tokens=58
17:09:11,384 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:11,386 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 49.844000000011874. input_tokens=34, output_tokens=374
17:09:11,442 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:11,444 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 45.905999999988126. input_tokens=1936, output_tokens=41
17:09:12,326 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:12,328 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 66.06299999999464. input_tokens=34, output_tokens=669
17:09:14,330 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:14,331 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 43.062999999994645. input_tokens=34, output_tokens=75
17:09:15,442 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:15,443 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.10999999998603. input_tokens=34, output_tokens=43
17:09:18,120 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:18,122 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 42.796000000002095. input_tokens=1936, output_tokens=64
17:09:18,406 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:18,408 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 56.203000000008615. input_tokens=34, output_tokens=454
17:09:19,717 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:19,718 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 50.25. input_tokens=1936, output_tokens=208
17:09:23,464 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:23,465 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.859000000025844. input_tokens=1936, output_tokens=106
17:09:24,287 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:24,288 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 34.10899999999674. input_tokens=1936, output_tokens=28
17:09:25,237 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:25,240 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 53.375. input_tokens=1936, output_tokens=349
17:09:27,492 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:27,493 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 47.453000000008615. input_tokens=1936, output_tokens=230
17:09:29,981 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:29,982 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 38.765999999974156. input_tokens=1935, output_tokens=97
17:09:31,304 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:31,307 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 54.01600000000326. input_tokens=1936, output_tokens=340
17:09:34,575 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:34,577 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 40.5. input_tokens=1936, output_tokens=192
17:09:38,75 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:38,78 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.219000000011874. input_tokens=1936, output_tokens=188
17:09:42,160 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:42,161 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 41.485000000015134. input_tokens=1936, output_tokens=120
17:09:44,17 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:44,19 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 53.25. input_tokens=1936, output_tokens=533
17:09:46,910 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:46,911 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 42.187999999994645. input_tokens=1936, output_tokens=65
17:09:48,528 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:48,530 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 47.14100000000326. input_tokens=1936, output_tokens=164
17:09:50,773 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:50,775 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 44.437999999994645. input_tokens=1936, output_tokens=101
17:09:51,652 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:51,653 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 43.578000000008615. input_tokens=1936, output_tokens=67
17:09:54,305 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:54,307 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 43.921999999991385. input_tokens=1935, output_tokens=113
17:09:56,479 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:56,481 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 57.59399999998277. input_tokens=1936, output_tokens=638
17:09:58,463 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:09:58,464 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 47.01500000001397. input_tokens=34, output_tokens=118
17:10:05,523 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:10:05,526 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 54.14100000000326. input_tokens=1936, output_tokens=270
17:10:16,227 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:10:16,229 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 61.905999999988126. input_tokens=1936, output_tokens=97
17:10:17,708 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:10:17,710 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 65.39000000001397. input_tokens=1936, output_tokens=191
17:10:24,259 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:10:24,261 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 68.81200000000536. input_tokens=1936, output_tokens=122
17:10:38,388 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:10:38,389 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 79.98399999999674. input_tokens=1936, output_tokens=175
17:10:40,664 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:10:40,666 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 80.95299999997951. input_tokens=1936, output_tokens=193
17:10:45,294 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:10:45,296 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 81.0. input_tokens=1937, output_tokens=50
17:10:45,992 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:10:45,994 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 82.53099999998813. input_tokens=1936, output_tokens=118
17:10:49,256 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:10:49,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 81.76600000000326. input_tokens=1936, output_tokens=88
17:10:52,35 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:10:52,36 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 82.04700000002049. input_tokens=1936, output_tokens=102
17:11:00,229 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:11:00,231 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 95.0. input_tokens=34, output_tokens=556
17:11:10,765 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:11:10,766 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 96.18700000000536. input_tokens=1935, output_tokens=97
17:11:14,246 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:11:14,248 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 102.93700000000536. input_tokens=34, output_tokens=581
17:11:23,856 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:11:23,858 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 101.68799999999464. input_tokens=1936, output_tokens=116
17:11:26,898 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:11:26,900 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 108.82800000000861. input_tokens=34, output_tokens=332
17:11:28,312 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:11:28,314 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 104.29699999999139. input_tokens=1936, output_tokens=130
17:11:28,968 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:11:28,969 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 100.43700000000536. input_tokens=34, output_tokens=22
17:11:31,732 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:11:31,733 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 100.95299999997951. input_tokens=1936, output_tokens=63
17:11:32,725 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:11:32,726 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 105.82799999997951. input_tokens=34, output_tokens=182
17:11:33,445 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:11:33,446 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 101.79699999999139. input_tokens=34, output_tokens=63
17:11:36,202 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:11:36,204 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 101.89100000000326. input_tokens=34, output_tokens=82
17:11:52,166 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:11:52,167 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 113.70299999997951. input_tokens=34, output_tokens=155
17:11:55,716 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n \n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity\'s attributes and activities\nFormat each entity as ("entity"<|><entity_name><|><entity_type><|><entity_description>)\n \n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as ("relationship"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n \n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n \n4. When finished, output <|COMPLETE|>\n \n######################\n-Examples-\n######################\nExample 1:\nEntity_types: ORGANIZATION,PERSON\nText:\nThe Verdantis\'s Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n######################\nOutput:\n("entity"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n##\n("entity"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n##\n("entity"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis\'s money supply)\n##\n("relationship"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n<|COMPLETE|>\n\n######################\nExample 2:\nEntity_types: ORGANIZATION\nText:\nTechGlobal\'s (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation\'s debut on the public markets isn\'t indicative of how other newly listed companies may perform.\n\nTechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n######################\nOutput:\n("entity"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n##\n("entity"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n##\n("relationship"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n<|COMPLETE|>\n\n######################\nExample 3:\nEntity_types: ORGANIZATION,GEO,PERSON\nText:\nFive Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n\nThe swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n\nThe exchange initiated in Firuzabad\'s capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n\nThey were welcomed by senior Aurelian officials and are now on their way to Aurelia\'s capital, Cashion.\n\nThe Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia\'s Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n######################\nOutput:\n("entity"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n##\n("entity"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n##\n("entity"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n##\n##\n("entity"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n##\n("entity"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n##\n("entity"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n##\n("entity"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia\'s Alhamia Prison)\n##\n("entity"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n##\n("entity"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n##\n("entity"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n##\n("relationship"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n##\n("relationship"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n<|COMPLETE|>\n\n######################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: of challenges associated with both of these techniques, and we can sort of wrap it whole into both loop task is that the user themselves doesn\'t get pro-receptive feedback about what they\'re doing. They don\'t, you know, necessarily perceive themself or feel, you know, the mouse under their hand when they\'re using an open loop calibration. They\'re being asked to perform something. Like, imagine if you sort of had your whole right arm numbed and you stuck it in a box and you couldn\'t see it. So you had no visual. the had no appropriate set to feedback about what the position or activity of your arm was. And now you\'re asked, okay, given this thing on the screen that\'s moving from left to right, match that speed. And you basically can try your best to you know invoke whatever that imagined inconsistent in how you do that task. And so that\'s sort of the fundamental challenge of open loop. The challenge with closed loop is that once the\n######################\nOutput:'}
17:12:00,615 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:12:00,616 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 124.14000000001397. input_tokens=1936, output_tokens=228
17:12:18,142 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n \n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity\'s attributes and activities\nFormat each entity as ("entity"<|><entity_name><|><entity_type><|><entity_description>)\n \n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as ("relationship"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n \n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n \n4. When finished, output <|COMPLETE|>\n \n######################\n-Examples-\n######################\nExample 1:\nEntity_types: ORGANIZATION,PERSON\nText:\nThe Verdantis\'s Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n######################\nOutput:\n("entity"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n##\n("entity"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n##\n("entity"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis\'s money supply)\n##\n("relationship"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n<|COMPLETE|>\n\n######################\nExample 2:\nEntity_types: ORGANIZATION\nText:\nTechGlobal\'s (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation\'s debut on the public markets isn\'t indicative of how other newly listed companies may perform.\n\nTechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n######################\nOutput:\n("entity"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n##\n("entity"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n##\n("relationship"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n<|COMPLETE|>\n\n######################\nExample 3:\nEntity_types: ORGANIZATION,GEO,PERSON\nText:\nFive Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n\nThe swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n\nThe exchange initiated in Firuzabad\'s capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n\nThey were welcomed by senior Aurelian officials and are now on their way to Aurelia\'s capital, Cashion.\n\nThe Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia\'s Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n######################\nOutput:\n("entity"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n##\n("entity"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n##\n("entity"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n##\n##\n("entity"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n##\n("entity"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n##\n("entity"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n##\n("entity"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia\'s Alhamia Prison)\n##\n("entity"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n##\n("entity"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n##\n("entity"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n##\n("relationship"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n##\n("relationship"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n<|COMPLETE|>\n\n######################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: can essentially flash the cursor over to the side of the screen and it opens up a window where they can actually sort of adjust or tune exactly the bias of the cursor. So bias maybe for people who aren\'t familiar is just sort of what is the default motion of the cursor if you\'re imagining nothing. And it turns out that that\'s one of the first sort of qualia of the cursor control experience that\'s impacted by neural non-stationarity. Quality of the cursor experience. I mean, I don\'t know how else to describe it. Like, you know, I\'m not the guy moving. It\'s very poetic. I love it. The quality of the cursor experience. Yeah, I mean, it is a joyful, a really pleasant experience. And when it doesn\'t work well, it\'s a very frustrating experience. That\'s actually the art of UX. It\'s like, you have the possibility to frustrate people or the possibility to give them joy. And at the end\n######################\nOutput:'}
17:12:45,285 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:12:45,286 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 149.0470000000205. input_tokens=34, output_tokens=36
17:13:05,546 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': '\n-Goal-\nGiven a text document that is potentially relevant to this activity and a list of entity types, identify all entities of those types from the text and all relationships among the identified entities.\n \n-Steps-\n1. Identify all entities. For each identified entity, extract the following information:\n- entity_name: Name of the entity, capitalized\n- entity_type: One of the following types: [organization,person,geo,event]\n- entity_description: Comprehensive description of the entity\'s attributes and activities\nFormat each entity as ("entity"<|><entity_name><|><entity_type><|><entity_description>)\n \n2. From the entities identified in step 1, identify all pairs of (source_entity, target_entity) that are *clearly related* to each other.\nFor each pair of related entities, extract the following information:\n- source_entity: name of the source entity, as identified in step 1\n- target_entity: name of the target entity, as identified in step 1\n- relationship_description: explanation as to why you think the source entity and the target entity are related to each other\n- relationship_strength: a numeric score indicating strength of the relationship between the source entity and target entity\n Format each relationship as ("relationship"<|><source_entity><|><target_entity><|><relationship_description><|><relationship_strength>)\n \n3. Return output in English as a single list of all the entities and relationships identified in steps 1 and 2. Use **##** as the list delimiter.\n \n4. When finished, output <|COMPLETE|>\n \n######################\n-Examples-\n######################\nExample 1:\nEntity_types: ORGANIZATION,PERSON\nText:\nThe Verdantis\'s Central Institution is scheduled to meet on Monday and Thursday, with the institution planning to release its latest policy decision on Thursday at 1:30 p.m. PDT, followed by a press conference where Central Institution Chair Martin Smith will take questions. Investors expect the Market Strategy Committee to hold its benchmark interest rate steady in a range of 3.5%-3.75%.\n######################\nOutput:\n("entity"<|>CENTRAL INSTITUTION<|>ORGANIZATION<|>The Central Institution is the Federal Reserve of Verdantis, which is setting interest rates on Monday and Thursday)\n##\n("entity"<|>MARTIN SMITH<|>PERSON<|>Martin Smith is the chair of the Central Institution)\n##\n("entity"<|>MARKET STRATEGY COMMITTEE<|>ORGANIZATION<|>The Central Institution committee makes key decisions about interest rates and the growth of Verdantis\'s money supply)\n##\n("relationship"<|>MARTIN SMITH<|>CENTRAL INSTITUTION<|>Martin Smith is the Chair of the Central Institution and will answer questions at a press conference<|>9)\n<|COMPLETE|>\n\n######################\nExample 2:\nEntity_types: ORGANIZATION\nText:\nTechGlobal\'s (TG) stock skyrocketed in its opening day on the Global Exchange Thursday. But IPO experts warn that the semiconductor corporation\'s debut on the public markets isn\'t indicative of how other newly listed companies may perform.\n\nTechGlobal, a formerly public company, was taken private by Vision Holdings in 2014. The well-established chip designer says it powers 85% of premium smartphones.\n######################\nOutput:\n("entity"<|>TECHGLOBAL<|>ORGANIZATION<|>TechGlobal is a stock now listed on the Global Exchange which powers 85% of premium smartphones)\n##\n("entity"<|>VISION HOLDINGS<|>ORGANIZATION<|>Vision Holdings is a firm that previously owned TechGlobal)\n##\n("relationship"<|>TECHGLOBAL<|>VISION HOLDINGS<|>Vision Holdings formerly owned TechGlobal from 2014 until present<|>5)\n<|COMPLETE|>\n\n######################\nExample 3:\nEntity_types: ORGANIZATION,GEO,PERSON\nText:\nFive Aurelians jailed for 8 years in Firuzabad and widely regarded as hostages are on their way home to Aurelia.\n\nThe swap orchestrated by Quintara was finalized when $8bn of Firuzi funds were transferred to financial institutions in Krohaara, the capital of Quintara.\n\nThe exchange initiated in Firuzabad\'s capital, Tiruzia, led to the four men and one woman, who are also Firuzi nationals, boarding a chartered flight to Krohaara.\n\nThey were welcomed by senior Aurelian officials and are now on their way to Aurelia\'s capital, Cashion.\n\nThe Aurelians include 39-year-old businessman Samuel Namara, who has been held in Tiruzia\'s Alhamia Prison, as well as journalist Durke Bataglani, 59, and environmentalist Meggie Tazbah, 53, who also holds Bratinas nationality.\n######################\nOutput:\n("entity"<|>FIRUZABAD<|>GEO<|>Firuzabad held Aurelians as hostages)\n##\n("entity"<|>AURELIA<|>GEO<|>Country seeking to release hostages)\n##\n("entity"<|>QUINTARA<|>GEO<|>Country that negotiated a swap of money in exchange for hostages)\n##\n##\n("entity"<|>TIRUZIA<|>GEO<|>Capital of Firuzabad where the Aurelians were being held)\n##\n("entity"<|>KROHAARA<|>GEO<|>Capital city in Quintara)\n##\n("entity"<|>CASHION<|>GEO<|>Capital city in Aurelia)\n##\n("entity"<|>SAMUEL NAMARA<|>PERSON<|>Aurelian who spent time in Tiruzia\'s Alhamia Prison)\n##\n("entity"<|>ALHAMIA PRISON<|>GEO<|>Prison in Tiruzia)\n##\n("entity"<|>DURKE BATAGLANI<|>PERSON<|>Aurelian journalist who was held hostage)\n##\n("entity"<|>MEGGIE TAZBAH<|>PERSON<|>Bratinas national and environmentalist who was held hostage)\n##\n("relationship"<|>FIRUZABAD<|>AURELIA<|>Firuzabad negotiated a hostage exchange with Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>AURELIA<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>QUINTARA<|>FIRUZABAD<|>Quintara brokered the hostage exchange between Firuzabad and Aurelia<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>ALHAMIA PRISON<|>Samuel Namara was a prisoner at Alhamia prison<|>8)\n##\n("relationship"<|>SAMUEL NAMARA<|>MEGGIE TAZBAH<|>Samuel Namara and Meggie Tazbah were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>DURKE BATAGLANI<|>Samuel Namara and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>DURKE BATAGLANI<|>Meggie Tazbah and Durke Bataglani were exchanged in the same hostage release<|>2)\n##\n("relationship"<|>SAMUEL NAMARA<|>FIRUZABAD<|>Samuel Namara was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>MEGGIE TAZBAH<|>FIRUZABAD<|>Meggie Tazbah was a hostage in Firuzabad<|>2)\n##\n("relationship"<|>DURKE BATAGLANI<|>FIRUZABAD<|>Durke Bataglani was a hostage in Firuzabad<|>2)\n<|COMPLETE|>\n\n######################\n-Real Data-\n######################\nEntity_types: organization,person,geo,event\nText: , I think there\'s a very interesting journey ahead to get us to that same level of 10 BPS performance. It\'s not the case that sort of the tricks that got us from, you know, 4 to 6 BPS and then 6 to 8 BPS are gonna be the ones that get us from 8 to 10. And in my view, the core challenge here is really the labeling problem. It\'s how do you understand at a very, very fine resolution, to work on this problem. What\'s the journey with Nolan on that quest of increasing the BPS on WebGrid? In March, you said that he selected 89,285 targets in WebGrid. So he loves this game. He\'s really serious about improving his performance in this game. So what is that journey of trying to figure out how to improve that performance? How much can that be done on the decoding side? How much can that be done on the calibration side? How\n######################\nOutput:'}
17:13:08,528 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:08,529 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 147.86000000001513. input_tokens=34, output_tokens=64
17:13:11,910 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:11,911 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 174.18799999999464. input_tokens=1936, output_tokens=315
17:13:12,205 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:12,206 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 167.93799999999464. input_tokens=1936, output_tokens=316
17:13:13,196 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:13,197 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 154.81299999999464. input_tokens=1936, output_tokens=321
17:13:15,219 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:15,220 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 143.18700000000536. input_tokens=34, output_tokens=27
17:13:15,589 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:15,590 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 146.32800000000861. input_tokens=34, output_tokens=72
17:13:16,102 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:16,103 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 150.81299999999464. input_tokens=1936, output_tokens=180
17:13:17,706 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:17,707 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 123.4539999999979. input_tokens=34, output_tokens=71
17:13:21,167 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:21,168 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 117.31200000000536. input_tokens=34, output_tokens=157
17:13:22,304 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:22,306 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 156.31299999999464. input_tokens=34, output_tokens=383
17:13:23,336 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:23,337 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 132.57800000000861. input_tokens=34, output_tokens=347
17:13:24,262 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:24,263 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 115.95300000000861. input_tokens=34, output_tokens=81
17:13:24,842 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:24,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 117.93700000000536. input_tokens=34, output_tokens=148
17:13:26,351 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:26,352 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 117.39099999997416. input_tokens=34, output_tokens=74
17:13:28,461 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:28,462 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 116.73400000002584. input_tokens=34, output_tokens=132
17:13:29,275 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:29,276 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 115.82800000000861. input_tokens=34, output_tokens=132
17:13:31,368 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:31,370 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 118.64000000001397. input_tokens=34, output_tokens=207
17:13:32,886 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:32,887 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 152.65600000001723. input_tokens=34, output_tokens=641
17:13:35,65 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:35,66 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 102.89100000000326. input_tokens=34, output_tokens=63
17:13:36,669 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:36,671 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 120.46799999999348. input_tokens=34, output_tokens=225
17:13:36,724 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:36,725 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 96.10999999998603. input_tokens=34, output_tokens=71
17:13:38,479 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:38,481 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 100.78099999998813. input_tokens=1936, output_tokens=207
17:13:41,487 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:41,488 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 34.34399999998277. input_tokens=1936, output_tokens=102
17:13:41,958 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:41,959 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.421999999991385. input_tokens=1936, output_tokens=97
17:13:46,793 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:46,794 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.59299999999348. input_tokens=34, output_tokens=219
17:13:47,711 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:47,712 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 1 retries took 88.31200000000536. input_tokens=1936, output_tokens=499
17:13:48,716 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:48,717 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.812000000005355. input_tokens=34, output_tokens=311
17:13:49,271 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:49,272 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.062999999994645. input_tokens=34, output_tokens=61
17:13:50,162 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:50,163 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.96799999999348. input_tokens=34, output_tokens=132
17:13:53,66 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:53,68 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.46899999998277. input_tokens=34, output_tokens=136
17:13:53,104 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:53,105 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 67.82799999997951. input_tokens=34, output_tokens=559
17:13:54,553 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:54,554 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.85899999999674. input_tokens=34, output_tokens=144
17:13:54,589 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:54,590 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.42200000002049. input_tokens=34, output_tokens=64
17:13:55,916 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:55,917 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.60899999999674. input_tokens=34, output_tokens=76
17:13:56,121 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:56,122 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.85899999999674. input_tokens=1936, output_tokens=18
17:13:56,747 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:56,747 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.405999999988126. input_tokens=34, output_tokens=43
17:13:57,85 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:57,86 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 32.25. input_tokens=34, output_tokens=41
17:13:59,446 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:13:59,447 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.094000000011874. input_tokens=1935, output_tokens=97
17:14:01,444 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:14:01,445 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 45.344000000011874. input_tokens=34, output_tokens=400
17:14:03,692 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:14:03,693 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 34.405999999988126. input_tokens=1936, output_tokens=212
17:14:05,948 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:14:05,949 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 30.89100000000326. input_tokens=34, output_tokens=51
17:14:05,971 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:14:05,972 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 33.078000000008615. input_tokens=1936, output_tokens=112
17:14:08,372 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:14:08,373 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.0. input_tokens=34, output_tokens=221
17:14:10,171 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:14:10,172 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.687000000005355. input_tokens=1937, output_tokens=26
17:14:11,26 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:14:11,27 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 34.29700000002049. input_tokens=1936, output_tokens=76
17:14:13,117 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:14:13,119 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 31.64000000001397. input_tokens=1936, output_tokens=112
17:14:16,903 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:14:16,905 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 30.110000000015134. input_tokens=1936, output_tokens=106
17:14:16,949 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:14:16,950 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 40.28200000000652. input_tokens=1936, output_tokens=270
17:14:21,381 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:14:21,382 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.437000000005355. input_tokens=34, output_tokens=329
17:14:22,679 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:14:22,680 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 34.96899999998277. input_tokens=1936, output_tokens=183
17:14:26,359 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:14:26,360 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.07799999997951. input_tokens=34, output_tokens=198
17:14:32,93 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:14:32,94 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.03100000001723. input_tokens=1936, output_tokens=196
17:14:34,270 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:14:34,272 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 45.546999999991385. input_tokens=34, output_tokens=581
17:14:42,652 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:14:42,654 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 52.485000000015134. input_tokens=34, output_tokens=494
17:14:43,649 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:14:43,650 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 50.54700000002049. input_tokens=1936, output_tokens=173
17:14:45,661 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:14:45,663 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 51.10899999999674. input_tokens=34, output_tokens=69
17:14:54,383 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:14:54,385 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 59.796999999991385. input_tokens=1936, output_tokens=204
17:15:11,366 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:15:11,367 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 75.25. input_tokens=1936, output_tokens=118
17:15:11,428 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:15:11,429 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 75.51600000000326. input_tokens=1936, output_tokens=160
17:15:23,175 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:15:23,175 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 86.42199999999139. input_tokens=1936, output_tokens=119
17:15:39,199 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:15:39,200 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 102.10999999998603. input_tokens=1936, output_tokens=64
17:15:42,891 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:15:42,892 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 103.43700000000536. input_tokens=1935, output_tokens=98
17:15:47,55 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:15:47,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 103.35899999999674. input_tokens=1936, output_tokens=79
17:15:47,615 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:15:47,616 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 106.1710000000021. input_tokens=1936, output_tokens=129
17:16:07,321 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:07,322 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 121.35999999998603. input_tokens=1936, output_tokens=136
17:16:09,686 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:09,687 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 123.73399999999674. input_tokens=1936, output_tokens=237
17:16:23,201 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:23,202 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 132.17199999999139. input_tokens=1936, output_tokens=77
17:16:25,951 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:25,952 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 132.8289999999979. input_tokens=1937, output_tokens=108
17:16:27,651 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:27,653 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 139.28200000000652. input_tokens=1936, output_tokens=432
17:16:28,397 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:28,399 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 131.5. input_tokens=34, output_tokens=92
17:16:28,477 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
17:16:31,89 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:31,90 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 140.9220000000205. input_tokens=1936, output_tokens=400
17:16:31,590 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:31,591 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 128.90600000001723. input_tokens=1936, output_tokens=64
17:16:31,595 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:31,596 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 130.20300000000861. input_tokens=34, output_tokens=69
17:16:33,562 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:33,563 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 136.60899999999674. input_tokens=34, output_tokens=133
17:16:36,145 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:36,146 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 124.06299999999464. input_tokens=1936, output_tokens=92
17:16:37,439 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:37,440 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 123.15599999998813. input_tokens=1936, output_tokens=144
17:16:41,544 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:41,545 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 135.18700000000536. input_tokens=34, output_tokens=287
17:16:43,403 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:43,403 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 117.73500000001513. input_tokens=1935, output_tokens=159
17:16:46,230 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:46,231 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 123.57799999997951. input_tokens=1936, output_tokens=350
17:16:48,533 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:48,535 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 124.875. input_tokens=1936, output_tokens=329
17:16:49,840 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:49,841 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 98.46900000001187. input_tokens=1936, output_tokens=117
17:16:51,446 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:51,447 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 88.28200000000652. input_tokens=34, output_tokens=17
17:16:53,601 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:53,602 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 74.40599999998813. input_tokens=1936, output_tokens=100
17:16:53,809 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:53,810 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 102.375. input_tokens=1936, output_tokens=156
17:16:55,818 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:55,819 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 72.93799999999464. input_tokens=1936, output_tokens=127
17:16:56,817 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:56,818 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 49.5. input_tokens=34, output_tokens=33
17:16:58,234 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:58,235 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 71.17199999999139. input_tokens=34, output_tokens=87
17:16:59,30 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:16:59,32 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 124.64100000000326. input_tokens=1935, output_tokens=381
17:17:00,857 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:00,858 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.655999999988126. input_tokens=34, output_tokens=56
17:17:02,830 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:02,831 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.171999999991385. input_tokens=34, output_tokens=26
17:17:03,896 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:03,897 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.5. input_tokens=34, output_tokens=39
17:17:04,93 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:04,94 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 54.40600000001723. input_tokens=1935, output_tokens=144
17:17:06,9 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:06,11 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.921999999991385. input_tokens=34, output_tokens=75
17:17:07,298 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:07,299 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 79.67199999999139. input_tokens=1936, output_tokens=355
17:17:07,518 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:07,520 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 41.562000000005355. input_tokens=1936, output_tokens=241
17:17:09,72 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:09,73 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.48499999998603. input_tokens=34, output_tokens=78
17:17:11,294 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:11,296 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.139999999984866. input_tokens=34, output_tokens=100
17:17:12,969 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:12,970 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.40600000001723. input_tokens=1936, output_tokens=158
17:17:17,382 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:17,383 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.953000000008615. input_tokens=1935, output_tokens=243
17:17:18,513 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:18,514 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 46.921999999991385. input_tokens=34, output_tokens=384
17:17:19,982 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:19,983 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 38.437999999994645. input_tokens=34, output_tokens=233
17:17:23,607 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:23,608 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 40.20299999997951. input_tokens=1936, output_tokens=103
17:17:24,405 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:24,405 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 34.562999999994645. input_tokens=1937, output_tokens=107
17:17:27,6 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:27,8 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 40.78100000001723. input_tokens=1937, output_tokens=237
17:17:30,237 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:30,238 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.625. input_tokens=34, output_tokens=133
17:17:31,542 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:31,543 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.73399999999674. input_tokens=34, output_tokens=48
17:17:31,856 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:31,857 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 43.32799999997951. input_tokens=34, output_tokens=334
17:17:34,689 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:34,690 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.85899999999674. input_tokens=34, output_tokens=64
17:17:36,356 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:36,358 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.905999999988126. input_tokens=34, output_tokens=444
17:17:37,380 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:37,381 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 38.35899999999674. input_tokens=34, output_tokens=38
17:17:39,952 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:39,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 39.094000000011874. input_tokens=34, output_tokens=103
17:17:40,222 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:40,223 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.39000000001397. input_tokens=34, output_tokens=309
17:17:40,673 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:40,673 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.84299999999348. input_tokens=34, output_tokens=16
17:17:41,7 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:41,8 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.10899999999674. input_tokens=34, output_tokens=22
17:17:41,806 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:41,807 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 43.578000000008615. input_tokens=34, output_tokens=279
17:17:43,119 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:43,120 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.10899999999674. input_tokens=34, output_tokens=35
17:17:44,498 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:44,499 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.405999999988126. input_tokens=34, output_tokens=93
17:17:45,858 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:45,859 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 38.34399999998277. input_tokens=1936, output_tokens=96
17:17:48,51 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:48,52 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.75. input_tokens=34, output_tokens=52
17:17:54,192 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:54,193 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 41.21899999998277. input_tokens=1936, output_tokens=106
17:17:58,794 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:17:58,795 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 49.71799999999348. input_tokens=34, output_tokens=306
17:18:00,490 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:18:00,491 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 43.10899999999674. input_tokens=1936, output_tokens=163
17:18:01,75 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:18:01,77 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 42.562999999994645. input_tokens=34, output_tokens=85
17:18:07,309 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:18:07,310 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 43.703000000008615. input_tokens=1936, output_tokens=86
17:18:11,388 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:18:11,390 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 64.09400000001187. input_tokens=34, output_tokens=706
17:18:13,210 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:18:13,211 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 48.812000000005355. input_tokens=34, output_tokens=240
17:18:14,904 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:18:14,905 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 54.92200000002049. input_tokens=34, output_tokens=413
17:18:16,843 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:18:16,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 45.29700000002049. input_tokens=34, output_tokens=77
17:18:19,236 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:18:19,237 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 52.21899999998277. input_tokens=34, output_tokens=261
17:18:20,699 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:18:20,701 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 50.469000000011874. input_tokens=34, output_tokens=251
17:18:21,480 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:18:21,481 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 46.796999999991385. input_tokens=34, output_tokens=86
17:18:22,354 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:18:22,356 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 50.5. input_tokens=1936, output_tokens=168
17:18:26,853 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:18:26,855 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 49.46899999998277. input_tokens=34, output_tokens=166
17:18:34,120 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:18:34,121 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 54.171000000002095. input_tokens=1936, output_tokens=142
17:18:38,921 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:18:38,923 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 58.70299999997951. input_tokens=1936, output_tokens=205
17:18:53,263 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:18:53,264 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 72.25. input_tokens=1936, output_tokens=191
17:18:55,842 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:18:55,843 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 74.03100000001723. input_tokens=1936, output_tokens=57
17:19:01,955 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:19:01,963 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 81.29700000002049. input_tokens=34, output_tokens=709
17:19:07,826 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:19:07,827 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 83.3289999999979. input_tokens=34, output_tokens=248
17:19:19,165 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:19:19,167 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 96.04699999999139. input_tokens=1936, output_tokens=667
17:19:23,384 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:19:23,385 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 95.34400000001187. input_tokens=34, output_tokens=170
17:19:24,943 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:19:24,945 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 99.09400000001187. input_tokens=1936, output_tokens=395
17:19:29,938 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
17:19:51,192 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:19:51,193 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 112.39100000000326. input_tokens=1936, output_tokens=111
17:19:52,120 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:19:52,121 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 111.0460000000021. input_tokens=34, output_tokens=30
17:19:52,931 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:19:52,932 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 112.43799999999464. input_tokens=1936, output_tokens=167
17:20:01,67 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:20:01,69 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 126.89100000000326. input_tokens=34, output_tokens=295
17:20:13,703 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:20:13,705 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 126.39100000000326. input_tokens=34, output_tokens=92
17:20:15,637 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:20:15,638 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 120.73399999999674. input_tokens=34, output_tokens=75
17:20:16,123 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:20:16,124 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 124.73399999999674. input_tokens=1936, output_tokens=162
17:20:17,619 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:20:17,620 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 120.78099999998813. input_tokens=34, output_tokens=72
17:20:21,948 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:20:21,950 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 128.73499999998603. input_tokens=1935, output_tokens=299
17:20:24,631 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:20:24,632 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 123.93700000000536. input_tokens=1936, output_tokens=141
17:20:33,742 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:20:33,743 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 132.26500000001397. input_tokens=1936, output_tokens=104
17:20:35,297 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:20:35,299 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 136.06200000000536. input_tokens=34, output_tokens=320
17:20:36,365 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
17:20:41,578 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:20:41,579 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 134.71900000001187. input_tokens=1936, output_tokens=27
17:20:47,533 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:20:47,534 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 145.1720000000205. input_tokens=1936, output_tokens=189
17:20:47,550 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:20:47,551 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 133.42199999999139. input_tokens=1936, output_tokens=70
17:21:02,256 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:02,257 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 126.42199999999139. input_tokens=1936, output_tokens=60
17:21:03,310 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:03,311 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 130.04699999999139. input_tokens=1936, output_tokens=167
17:21:05,448 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:05,449 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 117.625. input_tokens=1936, output_tokens=69
17:21:06,523 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:06,524 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 107.36000000001513. input_tokens=1936, output_tokens=73
17:21:07,244 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:07,246 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 148.32800000000861. input_tokens=1936, output_tokens=270
17:21:08,511 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:08,513 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 105.125. input_tokens=1936, output_tokens=116
17:21:09,469 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:09,470 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 104.51500000001397. input_tokens=1936, output_tokens=111
17:21:11,724 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:11,725 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 80.53099999998813. input_tokens=34, output_tokens=81
17:21:12,569 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:12,570 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 79.64100000000326. input_tokens=34, output_tokens=29
17:21:14,809 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:14,810 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 82.68799999999464. input_tokens=1936, output_tokens=81
17:21:15,635 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:15,636 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 74.56200000000536. input_tokens=1937, output_tokens=33
17:21:17,349 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:17,350 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 63.655999999988126. input_tokens=34, output_tokens=95
17:21:17,734 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:17,736 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 135.76599999997416. input_tokens=1936, output_tokens=492
17:21:20,183 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:20,184 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 64.54699999999139. input_tokens=1936, output_tokens=127
17:21:21,692 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:21,693 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 65.56299999999464. input_tokens=34, output_tokens=72
17:21:23,376 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:23,377 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 61.421000000002095. input_tokens=1935, output_tokens=27
17:21:23,928 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:23,929 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 66.31299999999464. input_tokens=1935, output_tokens=94
17:21:27,472 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:27,473 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 52.17200000002049. input_tokens=1935, output_tokens=144
17:21:28,901 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:28,903 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 55.15700000000652. input_tokens=1936, output_tokens=176
17:21:29,487 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:29,488 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 47.905999999988126. input_tokens=1937, output_tokens=27
17:21:30,899 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:30,900 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 43.375. input_tokens=1936, output_tokens=52
17:21:31,284 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:31,285 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 66.64100000000326. input_tokens=1935, output_tokens=281
17:21:35,264 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:35,264 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 47.719000000011874. input_tokens=34, output_tokens=136
17:21:37,305 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:37,307 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 34.0. input_tokens=34, output_tokens=84
17:21:40,612 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:40,613 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.155999999988126. input_tokens=34, output_tokens=140
17:21:41,359 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:41,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.09399999998277. input_tokens=1936, output_tokens=388
17:21:43,948 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:43,949 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.703999999997905. input_tokens=34, output_tokens=57
17:21:47,859 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:47,860 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 41.32799999997951. input_tokens=34, output_tokens=204
17:21:50,928 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:50,929 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 42.421999999991385. input_tokens=1936, output_tokens=197
17:21:53,780 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:21:53,781 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 44.312999999994645. input_tokens=1936, output_tokens=185
17:22:00,793 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:22:00,794 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 48.21799999999348. input_tokens=1936, output_tokens=167
17:22:03,183 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:22:03,185 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 51.453000000008615. input_tokens=34, output_tokens=368
17:22:05,168 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:22:05,169 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 49.530999999988126. input_tokens=34, output_tokens=82
17:22:13,405 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:22:13,407 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 56.04700000002049. input_tokens=1936, output_tokens=55
17:22:19,950 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:22:19,951 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 62.219000000011874. input_tokens=34, output_tokens=275
17:22:26,770 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:22:26,772 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 71.95300000000861. input_tokens=34, output_tokens=611
17:22:32,110 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
17:22:38,124 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:22:38,124 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 77.93700000000536. input_tokens=1936, output_tokens=98
17:22:43,358 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:22:43,359 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 79.98499999998603. input_tokens=1936, output_tokens=83
17:22:43,373 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:22:43,374 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 81.68700000000536. input_tokens=1935, output_tokens=90
17:22:45,359 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:22:45,360 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 77.89099999997416. input_tokens=1936, output_tokens=69
17:22:46,778 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:22:46,780 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 82.84400000001187. input_tokens=34, output_tokens=219
17:22:49,147 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:22:49,148 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 78.25. input_tokens=34, output_tokens=99
17:22:51,576 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:22:51,577 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 82.67199999999139. input_tokens=34, output_tokens=288
17:22:57,33 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:22:57,34 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 81.76600000000326. input_tokens=34, output_tokens=27
17:22:58,672 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:22:58,673 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 87.38999999998487. input_tokens=1936, output_tokens=151
17:22:58,816 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:22:58,817 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 89.32800000000861. input_tokens=34, output_tokens=314
17:23:01,605 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:01,606 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 84.29699999999139. input_tokens=34, output_tokens=135
17:23:03,89 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:03,90 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 82.48400000002584. input_tokens=34, output_tokens=134
17:23:04,58 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:04,59 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 82.70300000000861. input_tokens=34, output_tokens=170
17:23:04,452 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:04,453 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 76.59400000001187. input_tokens=34, output_tokens=55
17:23:04,854 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:04,855 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 73.92199999999139. input_tokens=34, output_tokens=21
17:23:06,682 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:06,683 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 65.89100000000326. input_tokens=34, output_tokens=75
17:23:07,558 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:07,559 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 83.60899999999674. input_tokens=34, output_tokens=241
17:23:09,711 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:09,712 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 66.53100000001723. input_tokens=34, output_tokens=74
17:23:09,731 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:09,732 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 75.95299999997951. input_tokens=34, output_tokens=166
17:23:18,649 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:18,650 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 65.25. input_tokens=1936, output_tokens=27
17:23:20,180 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:20,181 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 60.23399999999674. input_tokens=34, output_tokens=32
17:23:37,259 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:37,260 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 70.48399999999674. input_tokens=34, output_tokens=33
17:23:37,800 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
17:23:41,213 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:41,214 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 63.094000000011874. input_tokens=1936, output_tokens=193
17:23:44,536 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:44,538 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 99.375. input_tokens=1936, output_tokens=412
17:23:47,661 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:47,662 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 62.312000000005355. input_tokens=1936, output_tokens=85
17:23:48,513 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:48,514 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 61.73399999999674. input_tokens=34, output_tokens=27
17:23:49,547 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:49,548 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 66.17199999999139. input_tokens=34, output_tokens=452
17:23:50,673 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:50,674 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 61.514999999984866. input_tokens=34, output_tokens=88
17:23:55,64 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:55,66 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 63.48399999999674. input_tokens=34, output_tokens=248
17:23:56,867 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:56,869 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 58.203000000008615. input_tokens=34, output_tokens=27
17:23:57,354 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:57,355 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 74.0. input_tokens=34, output_tokens=591
17:23:58,528 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:58,529 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 59.719000000011874. input_tokens=34, output_tokens=57
17:23:59,986 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:23:59,987 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 62.95299999997951. input_tokens=34, output_tokens=306
17:24:04,392 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:24:04,393 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 61.296999999991385. input_tokens=1936, output_tokens=100
17:24:07,527 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:24:07,529 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 63.469000000011874. input_tokens=1936, output_tokens=196
17:24:09,222 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:24:09,225 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 67.625. input_tokens=34, output_tokens=324
17:24:12,493 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:24:12,495 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 68.0460000000021. input_tokens=1937, output_tokens=261
17:24:18,211 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:24:18,212 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 71.53100000001723. input_tokens=1936, output_tokens=103
17:24:18,848 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:24:18,850 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 71.28100000001723. input_tokens=1936, output_tokens=31
17:24:19,981 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:24:19,983 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 70.25. input_tokens=34, output_tokens=63
17:24:21,569 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:24:21,570 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 71.85999999998603. input_tokens=34, output_tokens=104
17:24:23,753 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:24:23,754 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 63.562000000005355. input_tokens=1936, output_tokens=40
17:24:25,277 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:24:25,279 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 80.42200000002049. input_tokens=34, output_tokens=400
17:24:26,357 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:24:26,359 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 49.09399999998277. input_tokens=1936, output_tokens=101
17:24:27,934 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:24:27,935 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 46.71899999998277. input_tokens=34, output_tokens=55
17:24:32,644 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:24:32,646 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 48.094000000011874. input_tokens=1936, output_tokens=111
17:24:37,457 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:24:37,460 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 78.79699999999139. input_tokens=34, output_tokens=491
17:24:37,715 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:24:37,717 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 50.04700000002049. input_tokens=1936, output_tokens=108
17:24:39,340 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:24:39,342 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 49.79700000002049. input_tokens=34, output_tokens=63
17:24:40,54 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:24:40,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 51.546999999991385. input_tokens=34, output_tokens=101
17:24:40,548 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:24:40,550 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 45.48399999999674. input_tokens=34, output_tokens=16
17:24:46,746 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:24:46,748 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 56.078000000008615. input_tokens=34, output_tokens=135
17:24:53,993 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:24:53,995 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 57.125. input_tokens=1937, output_tokens=122
17:25:00,941 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:25:00,943 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 63.578000000008615. input_tokens=1936, output_tokens=121
17:25:08,404 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:25:08,406 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 68.42200000002049. input_tokens=1935, output_tokens=17
17:25:10,438 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:25:10,440 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 71.90599999998813. input_tokens=1936, output_tokens=196
17:25:13,653 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:25:13,654 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 66.125. input_tokens=1936, output_tokens=79
17:25:15,497 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:25:15,500 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 71.10899999999674. input_tokens=34, output_tokens=233
17:25:32,207 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:25:32,208 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 79.7039999999979. input_tokens=1936, output_tokens=198
17:25:36,148 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
17:25:52,183 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:25:52,184 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 93.96899999998277. input_tokens=1936, output_tokens=76
17:25:57,143 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:25:57,145 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 98.28100000001723. input_tokens=1936, output_tokens=190
17:26:04,302 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:26:04,303 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 102.73399999999674. input_tokens=1936, output_tokens=90
17:26:10,72 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:26:10,75 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 120.84400000001187. input_tokens=1936, output_tokens=677
17:26:20,80 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:26:20,82 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 120.09400000001187. input_tokens=1936, output_tokens=369
17:26:31,566 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:26:31,568 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 125.20300000000861. input_tokens=1936, output_tokens=120
17:26:36,310 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:26:36,311 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 128.375. input_tokens=34, output_tokens=27
17:26:40,154 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
17:26:48,514 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:26:48,516 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 144.76600000000326. input_tokens=1936, output_tokens=421
17:26:50,172 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:26:50,174 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 144.88999999998487. input_tokens=1936, output_tokens=377
17:26:50,624 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:26:50,625 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 133.15599999998813. input_tokens=1936, output_tokens=91
17:26:50,667 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:26:50,668 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 138.01499999998487. input_tokens=1936, output_tokens=182
17:26:52,815 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:26:52,817 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 132.26600000000326. input_tokens=1936, output_tokens=32
17:26:53,812 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:26:53,813 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 133.75. input_tokens=34, output_tokens=70
17:26:55,637 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:26:55,639 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 128.89100000000326. input_tokens=1936, output_tokens=63
17:26:56,471 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:26:56,472 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 115.51500000001397. input_tokens=1935, output_tokens=27
17:26:58,894 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:26:58,896 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 141.17199999999139. input_tokens=34, output_tokens=304
17:27:01,453 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:01,455 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 127.4539999999979. input_tokens=1937, output_tokens=219
17:27:03,666 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:03,667 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 113.23399999999674. input_tokens=1936, output_tokens=99
17:27:05,270 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:05,272 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 116.875. input_tokens=1936, output_tokens=267
17:27:05,351 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:05,353 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 111.70299999997951. input_tokens=1936, output_tokens=108
17:27:06,885 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:06,887 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 111.39100000000326. input_tokens=1936, output_tokens=125
17:27:08,201 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:08,203 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 96.0. input_tokens=34, output_tokens=111
17:27:10,419 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:10,421 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 151.0779999999795. input_tokens=34, output_tokens=575
17:27:12,409 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:12,411 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 80.21900000001187. input_tokens=34, output_tokens=182
17:27:13,429 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:13,432 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 76.29699999999139. input_tokens=1936, output_tokens=174
17:27:15,175 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:15,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 55.09299999999348. input_tokens=34, output_tokens=30
17:27:16,994 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:16,996 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 66.9210000000021. input_tokens=34, output_tokens=160
17:27:19,75 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:19,77 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 4 retries took 94.84400000001187. input_tokens=34, output_tokens=458
17:27:20,175 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:20,177 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 75.85899999999674. input_tokens=34, output_tokens=345
17:27:22,288 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:22,289 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 33.780999999988126. input_tokens=34, output_tokens=40
17:27:23,648 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:23,650 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 52.094000000011874. input_tokens=1936, output_tokens=237
17:27:26,724 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:26,726 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 36.562999999994645. input_tokens=1935, output_tokens=101
17:27:27,368 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:27,369 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 36.703000000008615. input_tokens=1936, output_tokens=112
17:27:28,236 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:28,237 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.60999999998603. input_tokens=34, output_tokens=60
17:27:30,305 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:30,306 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.5. input_tokens=34, output_tokens=78
17:27:31,530 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:31,531 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.719000000011874. input_tokens=1936, output_tokens=98
17:27:33,824 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:33,825 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 38.187999999994645. input_tokens=34, output_tokens=151
17:27:37,4 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:37,6 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 38.10899999999674. input_tokens=1936, output_tokens=79
17:27:39,55 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:39,56 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.60899999999674. input_tokens=1936, output_tokens=79
17:27:39,75 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:39,77 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 42.60999999998603. input_tokens=1936, output_tokens=267
17:27:40,937 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:40,938 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.655999999988126. input_tokens=34, output_tokens=27
17:27:42,803 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:42,805 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.453000000008615. input_tokens=34, output_tokens=76
17:27:44,482 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:44,483 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.59399999998277. input_tokens=34, output_tokens=17
17:27:44,933 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:44,936 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 68.625. input_tokens=1936, output_tokens=823
17:27:46,298 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:46,300 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 42.625. input_tokens=1936, output_tokens=206
17:27:47,654 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:47,655 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 37.235000000015134. input_tokens=34, output_tokens=67
17:27:49,865 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:49,867 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 37.46799999999348. input_tokens=1936, output_tokens=84
17:27:52,50 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:52,52 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.85899999999674. input_tokens=34, output_tokens=88
17:27:52,840 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:52,842 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.64000000001397. input_tokens=34, output_tokens=282
17:27:53,544 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:53,545 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.10899999999674. input_tokens=34, output_tokens=226
17:27:54,707 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:54,708 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.625. input_tokens=34, output_tokens=65
17:27:57,857 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:57,859 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 40.85999999998603. input_tokens=34, output_tokens=241
17:27:58,987 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:27:58,989 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 35.32799999997951. input_tokens=34, output_tokens=46
17:28:06,410 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:28:06,411 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 39.67200000002049. input_tokens=1936, output_tokens=189
17:28:10,324 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:28:10,326 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 50.14100000000326. input_tokens=34, output_tokens=579
17:28:11,276 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:28:11,278 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 48.985000000015134. input_tokens=34, output_tokens=573
17:28:11,566 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:28:11,567 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.187999999994645. input_tokens=34, output_tokens=200
17:28:12,751 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:28:12,752 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 42.437000000005355. input_tokens=34, output_tokens=49
17:28:21,554 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:28:21,556 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 47.73399999999674. input_tokens=34, output_tokens=32
17:28:27,349 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:28:27,350 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 50.34399999998277. input_tokens=1936, output_tokens=153
17:28:29,665 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:28:29,667 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 58.139999999984866. input_tokens=34, output_tokens=280
17:28:32,784 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:28:32,787 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 53.719000000011874. input_tokens=34, output_tokens=51
17:28:40,87 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:28:40,89 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 61.01500000001397. input_tokens=34, output_tokens=27
17:28:42,456 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:28:42,458 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 61.51600000000326. input_tokens=1936, output_tokens=109
17:28:42,519 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:28:42,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 74.28100000001723. input_tokens=34, output_tokens=452
17:28:45,240 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:28:45,242 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 62.437000000005355. input_tokens=34, output_tokens=201
17:28:45,262 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:28:45,263 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 60.78100000001723. input_tokens=34, output_tokens=105
17:28:46,237 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:28:46,239 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 59.937999999994645. input_tokens=34, output_tokens=33
17:28:48,739 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:28:48,740 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 61.07799999997951. input_tokens=34, output_tokens=86
17:29:00,17 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:29:00,19 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 70.14100000000326. input_tokens=1936, output_tokens=71
17:29:01,16 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:29:01,18 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 76.07800000000861. input_tokens=34, output_tokens=241
17:29:02,325 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:29:02,326 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 69.48499999998603. input_tokens=1936, output_tokens=65
17:29:03,540 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:29:03,542 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 68.82799999997951. input_tokens=1936, output_tokens=26
17:29:03,622 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:29:03,624 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 70.07800000000861. input_tokens=34, output_tokens=73
17:29:14,254 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:29:14,256 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 76.39000000001397. input_tokens=1936, output_tokens=27
17:29:15,370 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:29:15,371 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 76.39000000001397. input_tokens=1936, output_tokens=42
17:29:19,485 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:29:19,487 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 87.43799999999464. input_tokens=1936, output_tokens=212
17:29:25,643 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:29:25,645 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 79.21900000001187. input_tokens=1936, output_tokens=213
17:29:26,968 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:29:26,970 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 76.64000000001397. input_tokens=1936, output_tokens=250
17:29:27,546 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:29:27,548 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 75.98399999999674. input_tokens=34, output_tokens=70
17:29:31,120 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:29:31,121 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 78.375. input_tokens=34, output_tokens=100
17:29:44,564 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
17:29:47,794 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:29:47,796 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 96.51499999998487. input_tokens=34, output_tokens=361
17:29:49,509 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:29:49,511 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 87.95300000000861. input_tokens=1936, output_tokens=149
17:29:49,925 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:29:49,927 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 80.25. input_tokens=34, output_tokens=79
17:29:51,150 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:29:51,152 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 71.06299999999464. input_tokens=34, output_tokens=62
17:29:51,731 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:29:51,733 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 69.28099999998813. input_tokens=34, output_tokens=63
17:29:53,793 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:29:53,795 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 86.43700000000536. input_tokens=1935, output_tokens=187
17:29:56,682 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:29:56,684 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 83.89100000000326. input_tokens=1936, output_tokens=205
17:29:59,309 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:29:59,310 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 76.78099999998813. input_tokens=34, output_tokens=178
17:30:01,491 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:01,493 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 76.25. input_tokens=1936, output_tokens=234
17:30:02,241 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:02,243 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 76.01500000001397. input_tokens=1936, output_tokens=158
17:30:03,589 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:03,591 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 78.32800000000861. input_tokens=1936, output_tokens=234
17:30:05,699 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:05,702 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 63.375. input_tokens=34, output_tokens=87
17:30:08,18 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:08,20 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 68.0. input_tokens=34, output_tokens=165
17:30:09,318 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:09,319 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 65.78200000000652. input_tokens=1936, output_tokens=45
17:30:09,939 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:09,940 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 55.671999999991385. input_tokens=1936, output_tokens=17
17:30:10,567 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:10,569 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 69.56299999999464. input_tokens=1937, output_tokens=191
17:30:13,511 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:13,513 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 54.03100000001723. input_tokens=1936, output_tokens=71
17:30:17,0 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:17,2 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 73.375. input_tokens=1936, output_tokens=268
17:30:18,178 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:18,179 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 62.812999999994645. input_tokens=1936, output_tokens=285
17:30:19,434 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:19,435 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 53.796999999991385. input_tokens=1935, output_tokens=197
17:30:23,913 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:23,916 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 56.95299999997951. input_tokens=34, output_tokens=189
17:30:26,278 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:26,280 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 55.15700000000652. input_tokens=1936, output_tokens=188
17:30:31,631 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:31,632 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 43.844000000011874. input_tokens=1936, output_tokens=75
17:30:31,669 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:31,670 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 64.125. input_tokens=1936, output_tokens=226
17:30:34,12 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:34,14 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 44.078000000008615. input_tokens=1936, output_tokens=88
17:30:34,596 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:34,598 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 45.078000000008615. input_tokens=1936, output_tokens=141
17:30:36,497 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:36,499 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 44.76500000001397. input_tokens=1936, output_tokens=93
17:30:39,327 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:39,330 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 48.171999999991385. input_tokens=34, output_tokens=305
17:30:42,239 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:42,241 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 45.562000000005355. input_tokens=1936, output_tokens=55
17:30:43,76 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:43,78 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 49.28200000000652. input_tokens=1936, output_tokens=204
17:30:44,991 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:44,995 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 116.25. input_tokens=34, output_tokens=1214
17:30:46,612 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:46,614 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 47.296999999991385. input_tokens=34, output_tokens=75
17:30:49,12 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:49,13 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 47.51600000000326. input_tokens=1936, output_tokens=95
17:30:51,395 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:51,397 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 45.703000000008615. input_tokens=1936, output_tokens=93
17:30:52,946 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:52,947 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 49.35999999998603. input_tokens=1936, output_tokens=205
17:30:53,789 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:53,791 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 51.546999999991385. input_tokens=1936, output_tokens=257
17:30:56,521 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:56,522 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 47.203000000008615. input_tokens=34, output_tokens=95
17:30:58,723 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:58,724 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 48.796999999991385. input_tokens=1936, output_tokens=99
17:30:59,582 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:30:59,583 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 46.062999999994645. input_tokens=34, output_tokens=26
17:31:01,978 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:01,979 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 44.98499999998603. input_tokens=1936, output_tokens=95
17:31:02,272 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:02,274 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 54.26600000000326. input_tokens=1937, output_tokens=331
17:31:06,619 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:06,621 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 56.046000000002095. input_tokens=34, output_tokens=194
17:31:06,642 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:06,643 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 48.453000000008615. input_tokens=34, output_tokens=27
17:31:07,405 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:07,406 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 47.969000000011874. input_tokens=34, output_tokens=42
17:31:07,426 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:07,428 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 41.139999999984866. input_tokens=34, output_tokens=16
17:31:09,825 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:09,826 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 45.90700000000652. input_tokens=34, output_tokens=62
17:31:11,758 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:11,759 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 40.094000000011874. input_tokens=1936, output_tokens=65
17:31:12,60 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:12,61 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 38.046999999991385. input_tokens=1936, output_tokens=26
17:31:12,739 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:12,740 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 38.140999999974156. input_tokens=1936, output_tokens=28
17:31:13,32 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:13,33 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 36.53200000000652. input_tokens=34, output_tokens=28
17:31:15,865 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:15,867 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.23399999999674. input_tokens=34, output_tokens=165
17:31:30,216 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:30,218 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 47.969000000011874. input_tokens=1936, output_tokens=111
17:31:31,371 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:31,372 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 46.375. input_tokens=34, output_tokens=40
17:31:32,475 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:32,477 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 49.405999999988126. input_tokens=1935, output_tokens=138
17:31:37,37 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:37,40 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 50.437000000005355. input_tokens=34, output_tokens=179
17:31:37,684 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:37,685 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 46.280999999988126. input_tokens=34, output_tokens=16
17:31:38,131 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:38,132 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 49.125. input_tokens=1936, output_tokens=172
17:31:41,548 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:41,550 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 62.21799999999348. input_tokens=1936, output_tokens=523
17:31:43,790 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:43,792 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 50.0. input_tokens=34, output_tokens=177
17:31:58,293 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:31:58,295 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 65.34299999999348. input_tokens=34, output_tokens=288
17:32:07,143 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:07,143 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 68.40600000001723. input_tokens=1845, output_tokens=110
17:32:10,557 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:10,558 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 68.57800000000861. input_tokens=34, output_tokens=17
17:32:11,874 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:11,876 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 72.2960000000021. input_tokens=34, output_tokens=87
17:32:13,486 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:13,487 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 66.85999999998603. input_tokens=34, output_tokens=63
17:32:16,918 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:16,920 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 70.28099999998813. input_tokens=34, output_tokens=135
17:32:19,436 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:19,439 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "Process" with 0 retries took 82.90599999998813. input_tokens=1936, output_tokens=537
17:32:20,516 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:20,518 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 78.23399999999674. input_tokens=34, output_tokens=401
17:32:25,188 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:25,190 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 77.78099999998813. input_tokens=34, output_tokens=233
17:32:27,340 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:27,342 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 75.57800000000861. input_tokens=34, output_tokens=87
17:32:27,422 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:27,424 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 79.98399999999674. input_tokens=34, output_tokens=435
17:32:29,390 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:29,391 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 76.64100000000326. input_tokens=34, output_tokens=74
17:32:30,274 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:30,276 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 80.45300000000861. input_tokens=34, output_tokens=401
17:32:31,931 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:31,933 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 76.06299999999464. input_tokens=34, output_tokens=65
17:32:41,132 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:41,135 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 70.92199999999139. input_tokens=34, output_tokens=70
17:32:43,256 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:43,257 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:43,259 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 90.23399999999674. input_tokens=34, output_tokens=263
17:32:43,260 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 71.89100000000326. input_tokens=34, output_tokens=82
17:32:44,694 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:44,695 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 72.21900000001187. input_tokens=34, output_tokens=51
17:32:47,975 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:47,978 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 70.93799999999464. input_tokens=34, output_tokens=174
17:32:48,256 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:48,259 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 96.20300000000861. input_tokens=34, output_tokens=526
17:32:53,272 graphrag.index.reporting.file_workflow_callbacks INFO Error Invoking LLM details={'input': 'MANY entities and relationships were missed in the last extraction. Remember to ONLY emit entities that match any of the previously extracted types. Add them below using the same format:\n'}
17:32:53,299 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:53,302 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 75.60899999999674. input_tokens=34, output_tokens=340
17:32:53,319 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:53,321 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 71.78200000000652. input_tokens=34, output_tokens=211
17:32:55,347 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:55,349 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 57.04700000002049. input_tokens=34, output_tokens=22
17:32:57,182 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:57,184 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 46.625. input_tokens=34, output_tokens=26
17:32:57,614 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:57,615 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 50.48399999999674. input_tokens=34, output_tokens=65
17:32:57,823 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:57,824 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 74.03200000000652. input_tokens=34, output_tokens=71
17:32:58,205 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:58,207 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 46.328999999997905. input_tokens=34, output_tokens=28
17:32:59,617 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:32:59,618 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 42.703000000008615. input_tokens=34, output_tokens=67
17:33:01,568 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:01,569 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 42.14100000000326. input_tokens=34, output_tokens=126
17:33:06,110 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:06,113 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 52.625. input_tokens=34, output_tokens=300
17:33:06,780 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:06,783 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 88.64100000000326. input_tokens=34, output_tokens=607
17:33:07,887 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:07,889 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 42.703000000008615. input_tokens=34, output_tokens=211
17:33:10,429 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:10,431 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 49.921999999991385. input_tokens=34, output_tokens=419
17:33:12,210 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:12,213 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 0 retries took 44.875. input_tokens=34, output_tokens=304
17:33:15,414 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:15,417 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "extract-continuation-0" with 5 retries took 12.125. input_tokens=34, output_tokens=538
17:33:15,444 datashaper.workflow.workflow INFO executing verb merge_graphs
17:33:16,104 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_extracted_entities.parquet
17:33:16,482 graphrag.index.run INFO Running workflow: create_summarized_entities...
17:33:16,482 graphrag.index.run INFO dependencies for create_summarized_entities: ['create_base_extracted_entities']
17:33:16,483 graphrag.index.run INFO read table from storage: create_base_extracted_entities.parquet
17:33:16,531 datashaper.workflow.workflow INFO executing verb summarize_descriptions
17:33:18,65 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:18,68 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.3439999999827705. input_tokens=157, output_tokens=39
17:33:18,134 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:18,137 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.437000000005355. input_tokens=154, output_tokens=46
17:33:18,403 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:18,405 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 1.687999999994645. input_tokens=166, output_tokens=51
17:33:18,764 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:18,766 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.062000000005355. input_tokens=201, output_tokens=73
17:33:18,965 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:18,967 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 2.234000000025844. input_tokens=166, output_tokens=35
17:33:19,963 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:19,965 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.234000000025844. input_tokens=144, output_tokens=69
17:33:20,36 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:20,38 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.327999999979511. input_tokens=218, output_tokens=84
17:33:20,190 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:20,191 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 3.437999999994645. input_tokens=175, output_tokens=61
17:33:21,288 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:21,290 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.562000000005355. input_tokens=147, output_tokens=62
17:33:21,671 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:21,673 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.905999999988126. input_tokens=248, output_tokens=123
17:33:21,693 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:21,695 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 4.9539999999979045. input_tokens=163, output_tokens=69
17:33:21,816 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:21,817 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.046999999991385. input_tokens=171, output_tokens=78
17:33:22,488 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:22,489 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 5.671999999991385. input_tokens=144, output_tokens=28
17:33:22,834 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:22,835 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.094000000011874. input_tokens=166, output_tokens=70
17:33:23,156 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:23,158 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.375. input_tokens=130, output_tokens=28
17:33:23,341 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:23,343 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.578000000008615. input_tokens=169, output_tokens=75
17:33:23,681 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:23,683 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.905999999988126. input_tokens=159, output_tokens=92
17:33:23,717 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:23,719 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.922000000020489. input_tokens=133, output_tokens=14
17:33:24,612 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:24,614 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.812999999994645. input_tokens=143, output_tokens=65
17:33:24,762 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:24,763 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.98399999999674. input_tokens=212, output_tokens=87
17:33:24,796 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:24,798 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.98399999999674. input_tokens=152, output_tokens=47
17:33:25,121 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:25,123 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.34299999999348. input_tokens=130, output_tokens=12
17:33:25,264 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:25,266 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.562000000005355. input_tokens=202, output_tokens=66
17:33:26,89 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:26,90 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.297000000020489. input_tokens=185, output_tokens=63
17:33:26,256 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:26,258 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.5. input_tokens=181, output_tokens=67
17:33:26,520 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:26,522 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.125. input_tokens=146, output_tokens=16
17:33:26,591 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:26,592 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.53100000001723. input_tokens=174, output_tokens=66
17:33:26,777 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:26,778 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.01600000000326. input_tokens=134, output_tokens=22
17:33:26,933 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:26,935 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.796999999991385. input_tokens=152, output_tokens=77
17:33:27,706 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:27,707 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.73499999998603. input_tokens=147, output_tokens=50
17:33:28,221 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:28,223 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.172000000020489. input_tokens=153, output_tokens=67
17:33:28,377 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:28,378 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.187000000005355. input_tokens=148, output_tokens=65
17:33:29,526 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:29,528 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.235000000015134. input_tokens=150, output_tokens=84
17:33:29,633 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:29,635 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.937000000005355. input_tokens=143, output_tokens=58
17:33:30,132 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:30,133 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.469000000011874. input_tokens=161, output_tokens=88
17:33:31,537 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:31,540 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 12.577999999979511. input_tokens=143, output_tokens=237
17:33:32,92 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:32,94 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.28100000001723. input_tokens=166, output_tokens=116
17:33:32,517 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:32,519 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.671999999991385. input_tokens=154, output_tokens=114
17:33:32,686 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:32,688 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.34399999998277. input_tokens=135, output_tokens=24
17:33:33,558 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:33,560 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 11.062999999994645. input_tokens=226, output_tokens=187
17:33:33,753 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:33,756 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.59299999999348. input_tokens=152, output_tokens=103
17:33:34,596 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:34,598 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.875. input_tokens=145, output_tokens=88
17:33:34,908 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:34,908 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 11.219000000011874. input_tokens=142, output_tokens=113
17:33:35,18 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:35,19 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:35,20 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.219000000011874. input_tokens=136, output_tokens=17
17:33:35,21 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.422000000020489. input_tokens=162, output_tokens=65
17:33:35,611 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:35,613 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.48499999998603. input_tokens=146, output_tokens=30
17:33:35,700 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:35,702 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.937999999994645. input_tokens=174, output_tokens=88
17:33:36,43 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:36,45 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.952999999979511. input_tokens=157, output_tokens=41
17:33:36,311 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:36,313 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.046999999991385. input_tokens=136, output_tokens=28
17:33:36,424 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:36,425 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 11.155999999988126. input_tokens=180, output_tokens=58
17:33:37,145 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:37,147 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.219000000011874. input_tokens=138, output_tokens=29
17:33:37,718 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:37,720 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.937000000005355. input_tokens=149, output_tokens=66
17:33:37,743 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:37,745 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.046000000002095. input_tokens=142, output_tokens=26
17:33:37,855 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:37,856 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 11.265999999974156. input_tokens=146, output_tokens=81
17:33:38,569 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:38,570 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.35999999998603. input_tokens=177, output_tokens=36
17:33:39,383 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:39,384 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.85899999999674. input_tokens=149, output_tokens=72
17:33:39,454 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:39,455 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.812999999994645. input_tokens=151, output_tokens=39
17:33:40,89 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:40,90 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 11.719000000011874. input_tokens=164, output_tokens=110
17:33:40,269 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:40,271 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 13.73399999999674. input_tokens=263, output_tokens=211
17:33:40,468 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:40,470 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.922000000020489. input_tokens=145, output_tokens=43
17:33:41,259 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:41,261 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 11.125. input_tokens=151, output_tokens=83
17:33:41,777 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:41,779 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.219000000011874. input_tokens=167, output_tokens=20
17:33:42,0 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:42,1 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.48399999999674. input_tokens=160, output_tokens=79
17:33:42,635 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:42,636 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.953000000008615. input_tokens=186, output_tokens=99
17:33:43,455 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:43,457 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.844000000011874. input_tokens=153, output_tokens=66
17:33:43,630 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:43,632 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.875. input_tokens=173, output_tokens=84
17:33:44,128 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:44,130 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 12.030999999988126. input_tokens=209, output_tokens=188
17:33:44,217 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:44,220 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.312000000005355. input_tokens=163, output_tokens=70
17:33:44,571 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:44,573 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.546999999991385. input_tokens=167, output_tokens=47
17:33:45,242 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:45,245 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.64000000001397. input_tokens=143, output_tokens=50
17:33:45,359 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:45,360 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.327999999979511. input_tokens=202, output_tokens=76
17:33:45,804 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:45,806 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.10899999999674. input_tokens=159, output_tokens=71
17:33:46,77 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:46,79 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.65700000000652. input_tokens=143, output_tokens=30
17:33:46,99 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:46,101 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.796999999991385. input_tokens=156, output_tokens=38
17:33:46,707 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:46,708 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.98499999998603. input_tokens=144, output_tokens=27
17:33:47,403 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:47,405 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.25. input_tokens=154, output_tokens=72
17:33:47,560 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:47,562 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.703000000008615. input_tokens=155, output_tokens=35
17:33:47,681 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:47,683 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 11.64100000000326. input_tokens=188, output_tokens=143
17:33:47,961 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:47,963 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.578000000008615. input_tokens=133, output_tokens=15
17:33:48,206 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:48,208 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.75. input_tokens=131, output_tokens=20
17:33:48,793 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:48,794 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.514999999984866. input_tokens=141, output_tokens=24
17:33:48,922 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:48,923 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 11.171999999991385. input_tokens=157, output_tokens=132
17:33:49,248 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:49,249 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.155999999988126. input_tokens=169, output_tokens=59
17:33:49,266 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:49,267 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.687000000005355. input_tokens=203, output_tokens=84
17:33:49,424 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:49,426 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.155999999988126. input_tokens=131, output_tokens=20
17:33:49,671 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:49,672 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.030999999988126. input_tokens=133, output_tokens=9
17:33:50,520 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:50,521 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 10.046999999991385. input_tokens=149, output_tokens=77
17:33:50,726 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:50,727 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.73499999998603. input_tokens=151, output_tokens=67
17:33:50,798 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:50,799 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.014999999984866. input_tokens=172, output_tokens=69
17:33:51,282 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:51,284 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.828000000008615. input_tokens=178, output_tokens=74
17:33:51,639 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:51,641 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.0. input_tokens=146, output_tokens=48
17:33:52,93 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:52,95 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.969000000011874. input_tokens=159, output_tokens=60
17:33:52,359 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:52,361 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.140999999974156. input_tokens=155, output_tokens=72
17:33:52,486 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:52,489 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.905999999988126. input_tokens=156, output_tokens=55
17:33:52,960 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:52,961 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.1560000000172295. input_tokens=144, output_tokens=24
17:33:52,979 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:52,982 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.73499999998603. input_tokens=161, output_tokens=60
17:33:53,572 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:53,575 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.219000000011874. input_tokens=140, output_tokens=67
17:33:53,576 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:53,579 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 6.875. input_tokens=138, output_tokens=25
17:33:54,341 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:54,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.234000000025844. input_tokens=139, output_tokens=62
17:33:54,885 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:54,887 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.328000000008615. input_tokens=153, output_tokens=59
17:33:55,101 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:55,102 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.702999999979511. input_tokens=162, output_tokens=69
17:33:55,408 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:55,410 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 9.328000000008615. input_tokens=158, output_tokens=137
17:33:55,586 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:55,589 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.9060000000172295. input_tokens=135, output_tokens=55
17:33:56,38 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:56,40 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.827999999979511. input_tokens=141, output_tokens=40
17:33:56,206 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:56,208 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.407000000006519. input_tokens=164, output_tokens=35
17:33:56,365 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:56,367 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 8.405999999988126. input_tokens=165, output_tokens=67
17:33:56,554 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:56,556 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.312999999994645. input_tokens=146, output_tokens=25
17:33:56,866 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:33:56,869 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "summarize" with 0 retries took 7.953000000008615. input_tokens=158, output_tokens=71
17:33:56,972 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_summarized_entities.parquet
17:33:57,357 graphrag.index.run INFO Running workflow: create_base_entity_graph...
17:33:57,358 graphrag.index.run INFO dependencies for create_base_entity_graph: ['create_summarized_entities']
17:33:57,359 graphrag.index.run INFO read table from storage: create_summarized_entities.parquet
17:33:57,401 datashaper.workflow.workflow INFO executing verb cluster_graph
17:33:58,99 datashaper.workflow.workflow INFO executing verb select
17:33:58,107 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_entity_graph.parquet
17:33:58,508 graphrag.index.run INFO Running workflow: create_final_entities...
17:33:58,508 graphrag.index.run INFO dependencies for create_final_entities: ['create_base_entity_graph']
17:33:58,509 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
17:33:58,555 datashaper.workflow.workflow INFO executing verb unpack_graph
17:33:58,895 datashaper.workflow.workflow INFO executing verb rename
17:33:58,910 datashaper.workflow.workflow INFO executing verb select
17:33:58,923 datashaper.workflow.workflow INFO executing verb dedupe
17:33:58,940 datashaper.workflow.workflow INFO executing verb rename
17:33:58,954 datashaper.workflow.workflow INFO executing verb filter
17:33:58,998 datashaper.workflow.workflow INFO executing verb text_split
17:33:59,25 datashaper.workflow.workflow INFO executing verb drop
17:33:59,41 datashaper.workflow.workflow INFO executing verb merge
17:33:59,258 datashaper.workflow.workflow INFO executing verb text_embed
17:33:59,261 graphrag.llm.openai.create_openai_client INFO Creating OpenAI client base_url=None
17:33:59,710 graphrag.index.llm.load_llm INFO create TPM/RPM limiter for text-embedding-3-small: TPM=0, RPM=0
17:33:59,710 graphrag.index.llm.load_llm INFO create concurrency limiter for text-embedding-3-small: 25
17:33:59,773 graphrag.index.verbs.text.embed.strategies.openai INFO embedding 721 inputs via 721 snippets using 46 batches. max_batch_size=16, max_tokens=8191
17:34:00,300 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,310 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,347 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,359 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,363 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,379 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,391 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,403 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,419 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,426 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.625. input_tokens=698, output_tokens=0
17:34:00,455 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,458 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,460 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,475 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6719999999913853. input_tokens=105, output_tokens=0
17:34:00,501 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7030000000086147. input_tokens=618, output_tokens=0
17:34:00,529 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.75. input_tokens=571, output_tokens=0
17:34:00,557 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,572 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,573 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,622 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,624 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,625 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,627 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,628 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,629 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,630 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,631 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,633 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,635 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:00,642 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8280000000086147. input_tokens=501, output_tokens=0
17:34:00,673 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8279999999795109. input_tokens=480, output_tokens=0
17:34:00,734 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9379999999946449. input_tokens=307, output_tokens=0
17:34:00,763 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9060000000172295. input_tokens=332, output_tokens=0
17:34:00,796 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9839999999967404. input_tokens=350, output_tokens=0
17:34:00,823 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9849999999860302. input_tokens=308, output_tokens=0
17:34:00,925 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.125. input_tokens=425, output_tokens=0
17:34:00,953 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1099999999860302. input_tokens=438, output_tokens=0
17:34:00,988 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.187999999994645. input_tokens=267, output_tokens=0
17:34:01,30 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2190000000118744. input_tokens=334, output_tokens=0
17:34:01,66 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.25. input_tokens=413, output_tokens=0
17:34:01,98 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.25. input_tokens=352, output_tokens=0
17:34:01,127 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2809999999881256. input_tokens=253, output_tokens=0
17:34:01,158 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.360000000015134. input_tokens=352, output_tokens=0
17:34:01,191 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.3439999999827705. input_tokens=249, output_tokens=0
17:34:01,218 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.422000000020489. input_tokens=766, output_tokens=0
17:34:01,245 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.4210000000020955. input_tokens=574, output_tokens=0
17:34:01,273 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.437999999994645. input_tokens=764, output_tokens=0
17:34:01,303 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:01,340 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:01,374 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.562000000005355. input_tokens=523, output_tokens=0
17:34:01,402 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5780000000086147. input_tokens=370, output_tokens=0
17:34:01,433 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.5939999999827705. input_tokens=358, output_tokens=0
17:34:01,496 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0460000000020955. input_tokens=642, output_tokens=0
17:34:01,536 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9690000000118744. input_tokens=245, output_tokens=0
17:34:01,600 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:01,679 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.125. input_tokens=254, output_tokens=0
17:34:01,707 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:01,708 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:01,732 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:01,847 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.8590000000258442. input_tokens=280, output_tokens=0
17:34:01,880 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1559999999881256. input_tokens=243, output_tokens=0
17:34:01,921 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:01,922 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:01,924 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:01,926 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:01,926 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:01,926 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:01,928 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:01,929 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:01,931 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:01,932 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:01,933 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:01,943 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0160000000032596. input_tokens=294, output_tokens=0
17:34:01,970 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.6560000000172295. input_tokens=13, output_tokens=0
17:34:01,973 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:02,8 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:02,9 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:02,14 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7030000000086147. input_tokens=358, output_tokens=0
17:34:02,47 httpx INFO HTTP Request: POST https://api.openai.com/v1/embeddings "HTTP/1.1 200 OK"
17:34:02,54 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.7339999999967404. input_tokens=423, output_tokens=0
17:34:02,82 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1570000000065193. input_tokens=476, output_tokens=0
17:34:02,116 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.562000000005355. input_tokens=329, output_tokens=0
17:34:02,175 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1089999999967404. input_tokens=223, output_tokens=0
17:34:02,206 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.875. input_tokens=226, output_tokens=0
17:34:02,231 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 0.9379999999946449. input_tokens=254, output_tokens=0
17:34:02,264 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1719999999913853. input_tokens=156, output_tokens=0
17:34:02,296 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.2339999999967404. input_tokens=213, output_tokens=0
17:34:02,345 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0150000000139698. input_tokens=201, output_tokens=0
17:34:02,370 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.6559999999881256. input_tokens=364, output_tokens=0
17:34:02,399 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.0940000000118744. input_tokens=287, output_tokens=0
17:34:02,425 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.437000000005355. input_tokens=403, output_tokens=0
17:34:02,472 graphrag.llm.base.rate_limiting_llm INFO perf - llm.embedding "Process" with 0 retries took 1.1400000000139698. input_tokens=288, output_tokens=0
17:34:02,546 datashaper.workflow.workflow INFO executing verb drop
17:34:02,565 datashaper.workflow.workflow INFO executing verb filter
17:34:02,603 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_entities.parquet
17:34:03,166 graphrag.index.run INFO Running workflow: create_final_nodes...
17:34:03,178 graphrag.index.run INFO dependencies for create_final_nodes: ['create_base_entity_graph']
17:34:03,179 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
17:34:03,222 datashaper.workflow.workflow INFO executing verb layout_graph
17:34:04,605 datashaper.workflow.workflow INFO executing verb unpack_graph
17:34:05,9 datashaper.workflow.workflow INFO executing verb unpack_graph
17:34:05,435 datashaper.workflow.workflow INFO executing verb filter
17:34:05,536 datashaper.workflow.workflow INFO executing verb drop
17:34:05,555 datashaper.workflow.workflow INFO executing verb select
17:34:05,575 datashaper.workflow.workflow INFO executing verb rename
17:34:05,594 datashaper.workflow.workflow INFO executing verb join
17:34:05,632 datashaper.workflow.workflow INFO executing verb convert
17:34:05,707 datashaper.workflow.workflow INFO executing verb rename
17:34:05,713 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_nodes.parquet
17:34:06,165 graphrag.index.run INFO Running workflow: create_final_communities...
17:34:06,166 graphrag.index.run INFO dependencies for create_final_communities: ['create_base_entity_graph']
17:34:06,167 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
17:34:06,224 datashaper.workflow.workflow INFO executing verb unpack_graph
17:34:06,687 datashaper.workflow.workflow INFO executing verb unpack_graph
17:34:07,77 datashaper.workflow.workflow INFO executing verb aggregate_override
17:34:07,106 datashaper.workflow.workflow INFO executing verb join
17:34:07,152 datashaper.workflow.workflow INFO executing verb join
17:34:07,195 datashaper.workflow.workflow INFO executing verb concat
17:34:07,226 datashaper.workflow.workflow INFO executing verb filter
17:34:07,709 datashaper.workflow.workflow INFO executing verb aggregate_override
17:34:07,756 datashaper.workflow.workflow INFO executing verb join
17:34:07,791 datashaper.workflow.workflow INFO executing verb filter
17:34:07,844 datashaper.workflow.workflow INFO executing verb fill
17:34:07,875 datashaper.workflow.workflow INFO executing verb merge
17:34:07,924 datashaper.workflow.workflow INFO executing verb copy
17:34:07,955 datashaper.workflow.workflow INFO executing verb select
17:34:07,960 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_communities.parquet
17:34:08,383 graphrag.index.run INFO Running workflow: join_text_units_to_entity_ids...
17:34:08,383 graphrag.index.run INFO dependencies for join_text_units_to_entity_ids: ['create_final_entities']
17:34:08,384 graphrag.index.run INFO read table from storage: create_final_entities.parquet
17:34:08,520 datashaper.workflow.workflow INFO executing verb select
17:34:08,552 datashaper.workflow.workflow INFO executing verb unroll
17:34:08,588 datashaper.workflow.workflow INFO executing verb aggregate_override
17:34:08,604 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_entity_ids.parquet
17:34:09,51 graphrag.index.run INFO Running workflow: create_final_relationships...
17:34:09,51 graphrag.index.run INFO dependencies for create_final_relationships: ['create_final_nodes', 'create_base_entity_graph']
17:34:09,51 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
17:34:09,80 graphrag.index.run INFO read table from storage: create_base_entity_graph.parquet
17:34:09,145 datashaper.workflow.workflow INFO executing verb unpack_graph
17:34:09,495 datashaper.workflow.workflow INFO executing verb filter
17:34:09,595 datashaper.workflow.workflow INFO executing verb rename
17:34:09,622 datashaper.workflow.workflow INFO executing verb filter
17:34:09,723 datashaper.workflow.workflow INFO executing verb drop
17:34:09,782 datashaper.workflow.workflow INFO executing verb compute_edge_combined_degree
17:34:09,820 datashaper.workflow.workflow INFO executing verb convert
17:34:09,881 datashaper.workflow.workflow INFO executing verb convert
17:34:09,889 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_relationships.parquet
17:34:10,336 graphrag.index.run INFO Running workflow: join_text_units_to_relationship_ids...
17:34:10,336 graphrag.index.run INFO dependencies for join_text_units_to_relationship_ids: ['create_final_relationships']
17:34:10,337 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
17:34:10,422 datashaper.workflow.workflow INFO executing verb select
17:34:10,457 datashaper.workflow.workflow INFO executing verb unroll
17:34:10,498 datashaper.workflow.workflow INFO executing verb aggregate_override
17:34:10,549 datashaper.workflow.workflow INFO executing verb select
17:34:10,553 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table join_text_units_to_relationship_ids.parquet
17:34:10,985 graphrag.index.run INFO Running workflow: create_final_community_reports...
17:34:11,1 graphrag.index.run INFO dependencies for create_final_community_reports: ['create_final_nodes', 'create_final_relationships']
17:34:11,10 graphrag.index.run INFO read table from storage: create_final_nodes.parquet
17:34:11,24 graphrag.index.run INFO read table from storage: create_final_relationships.parquet
17:34:11,104 datashaper.workflow.workflow INFO executing verb prepare_community_reports_nodes
17:34:11,170 datashaper.workflow.workflow INFO executing verb prepare_community_reports_edges
17:34:11,215 datashaper.workflow.workflow INFO executing verb restore_community_hierarchy
17:34:11,261 datashaper.workflow.workflow INFO executing verb prepare_community_reports
17:34:11,262 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=2 => 721
17:34:11,334 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=1 => 721
17:34:11,685 graphrag.index.verbs.graph.report.prepare_community_reports INFO Number of nodes at level=0 => 721
17:34:11,938 datashaper.workflow.workflow INFO executing verb create_community_reports
17:34:23,130 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:34:24,143 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:34:24,145 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 12.078000000008615. input_tokens=2033, output_tokens=372
17:34:26,472 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:34:27,896 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:34:32,113 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:34:38,622 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:34:38,642 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:34:40,738 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:34:44,313 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:34:49,306 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:34:52,346 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:34:55,858 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:34:56,620 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:35:02,118 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:35:07,617 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:35:09,671 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:35:09,709 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:35:16,358 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:35:18,107 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:35:21,690 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:35:26,245 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:35:26,247 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 74.1710000000021. input_tokens=2033, output_tokens=293
17:35:28,436 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:35:30,357 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:35:34,668 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:35:40,1 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:35:42,737 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:35:44,678 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:35:44,680 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:35:44,707 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:35:44,707 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 86
17:35:49,865 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:35:52,288 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:35:52,289 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:35:52,293 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:35:52,293 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 85
17:35:58,304 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:35:58,365 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:35:58,366 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:35:58,369 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:35:58,369 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 88
17:36:03,269 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:36:03,270 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:36:03,273 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:36:03,273 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 89
17:36:05,821 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:36:05,823 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:36:05,826 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:36:05,826 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 90
17:36:11,112 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:36:11,114 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:36:11,120 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:36:11,120 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 87
17:36:14,174 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:36:14,175 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:36:14,179 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:36:14,179 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 84
17:36:15,612 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:36:15,615 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:36:15,621 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:36:15,622 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 92
17:36:24,814 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:36:31,760 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:36:32,648 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:36:38,855 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:36:42,613 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:36:46,971 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:36:49,121 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:36:55,358 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:36:58,957 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:37:02,465 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:37:04,459 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:37:04,521 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:37:11,916 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:37:16,507 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:37:17,892 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:37:20,81 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:37:26,856 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:37:29,774 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:37:34,450 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:37:35,917 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:37:38,365 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:37:45,781 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:37:48,200 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:37:51,9 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:37:52,866 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:37:55,997 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:38:03,9 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:38:05,976 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:38:07,166 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:38:09,829 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:38:16,199 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:38:18,685 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:38:21,726 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:38:23,304 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:38:29,914 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:38:32,43 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:38:34,867 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:38:41,965 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:38:46,436 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:38:49,939 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:38:52,569 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:38:57,843 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:39:01,636 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:39:05,390 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:39:06,568 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:39:06,570 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 170.7039999999979. input_tokens=2219, output_tokens=573
17:39:09,297 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:39:17,658 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:39:25,143 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:39:25,524 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:39:25,901 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:39:31,621 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:39:37,769 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:39:42,627 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:39:44,325 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:39:46,325 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:39:52,468 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:39:56,185 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:39:57,864 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:40:03,917 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:40:08,978 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:40:09,975 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:40:11,349 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:40:16,469 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:40:24,602 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:40:26,632 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:40:29,9 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:40:33,152 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:40:39,684 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:40:39,768 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:40:42,571 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:40:44,157 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:40:52,197 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:40:52,198 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:40:52,200 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:40:52,200 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 26
17:40:54,638 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:40:56,802 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:40:58,434 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:41:05,332 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:41:10,820 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:41:10,822 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:41:10,824 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:41:10,824 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 20
17:41:11,672 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:41:11,673 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:41:11,675 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:41:11,675 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 22
17:41:12,419 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:41:12,420 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:41:12,423 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:41:12,423 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 18
17:41:20,845 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:41:20,846 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:41:20,848 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:41:20,848 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 21
17:41:23,962 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:41:23,963 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:41:23,965 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:41:23,965 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 19
17:41:27,518 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:41:27,519 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:41:27,521 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:41:27,521 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 24
17:41:29,220 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:41:29,221 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:41:29,223 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:41:29,224 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 23
17:41:32,303 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:41:32,305 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:41:32,306 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:41:32,306 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 34
17:41:38,488 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:41:38,489 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:41:38,492 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:41:38,492 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 28
17:41:40,442 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:41:40,443 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:41:40,446 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:41:40,446 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 30
17:41:43,419 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:41:43,420 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:41:43,421 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:41:43,422 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 25
17:41:45,40 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:41:45,41 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:41:45,43 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:41:45,43 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 33
17:41:52,319 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:41:52,320 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:41:52,322 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:41:52,322 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 29
17:41:54,780 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:41:54,781 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:41:54,784 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:41:54,784 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 31
17:41:58,251 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:41:58,252 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:41:58,254 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:41:58,254 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 32
17:42:05,308 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:42:05,309 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:42:05,311 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:42:05,311 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 36
17:42:08,527 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:42:08,528 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:42:08,531 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:42:08,531 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 35
17:42:10,476 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:42:10,477 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:42:10,480 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:42:10,480 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 39
17:42:11,916 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:42:11,917 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:42:11,920 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:42:11,920 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 27
17:42:20,98 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:42:23,454 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:42:25,73 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:42:25,74 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:42:25,77 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:42:25,77 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 41
17:42:32,212 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:42:32,213 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:42:32,215 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:42:32,215 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 42
17:42:34,765 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:42:34,767 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:42:34,769 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:42:34,769 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 40
17:42:37,347 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:42:44,577 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:42:46,952 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:42:46,954 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:42:46,956 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:42:46,956 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 38
17:42:49,126 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:42:52,211 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:42:59,209 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:43:02,879 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:43:04,237 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:43:05,984 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:43:16,140 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:43:17,824 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:43:17,828 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:43:20,463 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:43:30,833 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:43:33,696 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:43:42,665 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:43:44,162 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:43:49,396 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:43:56,595 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:43:57,471 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:43:58,387 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:44:02,447 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:44:12,517 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:44:15,164 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:44:16,646 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:44:16,686 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:44:25,63 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:44:27,256 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:44:31,177 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:44:36,522 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:44:39,547 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:44:43,716 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:44:50,404 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:44:52,542 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:44:57,507 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:44:57,890 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:45:06,249 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:45:11,246 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:45:11,286 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:45:13,600 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:45:19,15 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:45:25,996 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:45:29,37 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:45:32,873 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:45:33,727 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:45:33,728 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:45:33,730 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:45:33,730 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 43
17:45:37,478 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:45:41,249 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:45:46,50 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:45:47,327 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:45:51,923 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:45:56,836 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:45:58,100 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:45:58,884 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:46:04,633 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:46:08,778 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:46:12,320 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:46:13,923 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:46:20,837 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:46:26,40 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:46:28,61 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:46:30,465 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:46:40,877 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:46:40,897 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:46:44,981 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:46:45,958 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:46:57,212 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:47:00,6 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:47:03,268 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:47:10,726 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:47:12,369 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:47:12,371 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:47:12,375 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:47:12,375 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 44
17:47:13,701 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:47:18,811 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:47:25,106 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:47:25,107 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:47:25,110 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:47:25,111 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 45
17:47:28,574 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:47:30,981 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:47:38,181 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:47:40,778 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:47:40,780 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:47:40,783 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:47:40,783 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 46
17:47:41,806 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:47:45,323 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:47:45,324 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:47:45,328 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:47:45,328 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 47
17:47:52,395 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:47:52,396 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:47:52,400 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:47:52,400 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 48
17:47:55,797 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:47:55,799 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:47:55,802 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:47:55,802 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 49
17:47:58,322 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:47:58,323 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:47:58,327 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:47:58,327 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 54
17:48:09,117 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:48:09,118 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:48:09,122 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:48:09,122 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 51
17:48:09,997 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:48:09,998 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:48:10,2 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:48:10,2 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 52
17:48:11,162 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:48:11,164 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:48:11,169 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:48:11,169 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 55
17:48:16,350 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:48:16,352 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:48:16,355 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:48:16,355 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 50
17:48:22,700 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:48:22,702 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:48:22,705 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:48:22,706 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 59
17:48:28,85 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:48:28,87 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:48:28,92 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:48:28,92 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 56
17:48:28,958 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:48:28,959 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:48:28,963 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:48:28,963 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 53
17:48:31,640 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:48:31,641 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:48:31,645 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:48:31,645 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 57
17:48:40,516 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:48:40,518 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:48:40,522 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:48:40,522 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 61
17:48:42,319 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:48:42,321 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:48:42,323 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:48:42,325 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 60
17:48:44,48 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:48:44,50 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:48:44,54 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:48:44,54 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 62
17:48:46,92 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:48:53,999 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:48:58,505 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:48:58,507 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:48:58,510 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:48:58,510 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 63
17:49:00,303 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:49:09,552 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:49:09,554 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:49:09,558 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:49:09,558 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 65
17:49:11,102 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:49:11,104 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:49:11,108 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:49:11,108 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 58
17:49:15,862 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:49:15,864 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:49:15,867 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:49:15,868 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 66
17:49:22,493 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:49:22,495 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:49:22,500 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:49:22,500 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 67
17:49:26,231 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:49:26,233 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:49:26,237 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:49:26,237 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 64
17:49:28,947 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:49:30,723 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:49:36,41 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:49:52,967 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:49:52,968 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:49:52,968 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:49:52,969 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:49:53,159 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:50:01,110 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:50:01,992 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:50:05,935 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:50:07,7 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:50:16,559 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:50:20,39 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:50:22,70 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:50:29,421 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:50:30,126 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:50:37,883 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:50:43,730 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:50:45,15 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:50:46,375 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:50:51,555 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:50:59,185 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:51:00,977 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:51:01,699 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:51:06,556 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:51:12,119 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:51:16,276 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:51:20,546 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:51:22,101 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:51:29,822 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:51:33,747 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:51:33,748 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:51:33,751 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:51:33,751 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 68
17:51:37,989 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:51:39,5 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:51:44,371 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:51:53,486 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:51:53,504 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:51:58,303 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:52:02,91 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:52:08,932 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:52:11,146 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:52:14,537 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:52:17,137 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:52:22,944 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:52:22,945 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:52:22,947 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:52:22,947 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 69
17:52:25,23 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:52:33,286 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:52:33,287 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:52:33,289 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:52:33,289 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 72
17:52:37,440 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:52:39,845 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:52:39,846 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:52:39,848 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:52:39,848 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 73
17:52:40,734 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:52:40,736 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:52:40,739 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:52:40,739 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 70
17:52:48,844 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:52:48,846 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:52:48,849 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:52:48,849 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 77
17:52:51,334 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:52:51,335 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:52:51,337 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:52:51,337 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 76
17:52:59,18 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:52:59,20 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:52:59,22 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:52:59,22 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 74
17:53:02,736 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:53:02,737 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:53:02,739 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:53:02,739 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 78
17:53:04,811 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:53:04,813 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:53:04,815 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:53:04,816 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 71
17:53:06,426 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:53:06,427 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:53:06,429 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:53:06,429 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 75
17:53:13,208 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:53:13,209 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:53:13,211 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:53:13,211 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 80
17:53:16,61 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:53:16,62 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:53:16,65 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:53:16,65 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 82
17:53:20,559 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:53:20,560 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:53:20,563 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:53:20,563 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 79
17:53:26,137 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:53:26,138 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:53:26,141 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:53:26,141 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 81
17:53:26,164 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:53:26,165 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:53:26,167 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:53:26,167 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 83
17:53:35,214 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:53:40,76 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:53:43,25 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:53:43,47 graphrag.llm.base.rate_limiting_llm INFO perf - llm.chat "create_community_report" with 0 retries took 16.85899999999674. input_tokens=3281, output_tokens=561
17:53:45,190 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:53:50,546 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:53:59,356 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:54:00,294 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:54:02,573 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:54:04,974 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:54:13,595 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:54:18,275 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:54:21,794 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:54:25,641 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:54:28,482 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:54:36,551 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:54:39,985 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:54:44,403 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:54:52,493 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:54:53,756 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:54:55,832 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:54:59,250 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:55:05,270 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:55:05,272 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
17:55:10,643 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:55:12,758 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:55:14,769 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:55:20,415 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:55:26,407 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:55:27,504 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:55:33,622 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:55:37,139 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:55:40,386 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:55:43,339 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:55:48,837 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:55:49,642 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:55:54,870 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:56:02,439 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:56:02,816 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:56:09,55 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:56:11,805 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:56:16,671 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:56:20,296 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:56:22,108 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:56:27,526 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:56:30,391 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:56:35,504 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:56:39,893 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:56:45,303 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:56:46,682 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:56:47,784 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:56:47,785 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:56:47,787 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:56:47,787 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 14
17:56:50,263 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:57:00,802 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:57:02,190 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:57:02,191 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:57:02,193 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:57:02,193 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 1
17:57:07,115 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:57:07,117 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:57:07,119 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:57:07,119 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 15
17:57:10,377 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:57:12,141 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:57:12,143 graphrag.llm.openai.utils INFO Warning: Error decoding faulty json, attempting repair
17:57:12,144 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:57:12,147 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:57:12,147 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 12
17:57:21,250 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:57:21,252 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:57:21,255 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:57:21,255 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 17
17:57:25,822 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:57:25,823 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:57:25,825 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:57:25,825 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 11
17:57:27,35 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:57:27,37 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:57:27,39 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:57:27,39 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 16
17:57:29,352 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:57:35,478 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:57:35,479 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:57:35,481 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:57:35,481 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 7
17:57:41,914 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:57:41,916 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:57:41,918 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:57:41,918 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 5
17:57:45,290 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:57:45,291 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:57:45,293 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:57:45,293 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 4
17:57:46,230 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:57:46,230 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:57:46,232 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:57:46,232 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 13
17:57:48,356 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:57:48,357 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:57:48,359 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:57:48,359 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 8
17:57:55,592 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:57:55,593 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:57:55,595 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:57:55,595 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 10
17:57:59,89 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:57:59,90 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:57:59,93 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:57:59,93 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 6
17:58:02,118 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:58:02,119 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:58:02,121 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:58:02,121 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 3
17:58:03,32 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:58:03,33 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:58:03,36 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:58:03,36 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 9
17:58:11,233 httpx INFO HTTP Request: POST http://192.168.8.118:11434/v1/chat/completions "HTTP/1.1 200 OK"
17:58:11,235 graphrag.index.graph.extractors.community_reports.community_reports_extractor ERROR error generating community report
Traceback (most recent call last):
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\index\graph\extractors\community_reports\community_reports_extractor.py", line 58, in __call__
    await self._llm(
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\json_parsing_llm.py", line 34, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_token_replacing_llm.py", line 37, in __call__
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_history_tracking_llm.py", line 33, in __call__
    output = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\caching_llm.py", line 96, in __call__
    result = await self._delegate(input, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 177, in __call__
    result, start = await execute_with_retry()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 159, in execute_with_retry
    async for attempt in retryer:
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 166, in __anext__
    do = await self.iter(retry_state=self._retry_state)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\asyncio\__init__.py", line 153, in iter
    result = await action(retry_state)
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\_utils.py", line 99, in inner
    return call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\tenacity\__init__.py", line 398, in <lambda>
    self._add_action_func(lambda rs: rs.outcome.result())
                                     ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 449, in result
    return self.__get_result()
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Python312\Lib\concurrent\futures\_base.py", line 401, in __get_result
    raise self._exception
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 165, in execute_with_retry
    return await do_attempt(), start
           ^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\rate_limiting_llm.py", line 147, in do_attempt
    return await self._delegate(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\base\base_llm.py", line 48, in __call__
    return await self._invoke_json(input, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Ken Bailey\AppData\Roaming\Python\Python312\site-packages\graphrag\llm\openai\openai_chat_llm.py", line 88, in _invoke_json
    raise RuntimeError(FAILED_TO_CREATE_JSON_ERROR)
RuntimeError: Failed to generate valid JSON output
17:58:11,237 graphrag.index.reporting.file_workflow_callbacks INFO Community Report Extraction Error details=None
17:58:11,237 graphrag.index.verbs.graph.report.strategies.graph_intelligence.run_graph_intelligence WARNING No report found for community: 2
17:58:11,276 datashaper.workflow.workflow INFO executing verb window
17:58:11,279 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_community_reports.parquet
17:58:11,533 graphrag.index.run INFO Running workflow: create_final_text_units...
17:58:11,533 graphrag.index.run INFO dependencies for create_final_text_units: ['join_text_units_to_relationship_ids', 'join_text_units_to_entity_ids', 'create_base_text_units']
17:58:11,533 graphrag.index.run INFO read table from storage: join_text_units_to_relationship_ids.parquet
17:58:11,550 graphrag.index.run INFO read table from storage: join_text_units_to_entity_ids.parquet
17:58:11,568 graphrag.index.run INFO read table from storage: create_base_text_units.parquet
17:58:11,607 datashaper.workflow.workflow INFO executing verb select
17:58:11,624 datashaper.workflow.workflow INFO executing verb rename
17:58:11,641 datashaper.workflow.workflow INFO executing verb join
17:58:11,662 datashaper.workflow.workflow INFO executing verb join
17:58:11,685 datashaper.workflow.workflow INFO executing verb aggregate_override
17:58:11,723 datashaper.workflow.workflow INFO executing verb select
17:58:11,727 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_text_units.parquet
17:58:11,977 graphrag.index.run INFO Running workflow: create_base_documents...
17:58:11,977 graphrag.index.run INFO dependencies for create_base_documents: ['create_final_text_units']
17:58:11,977 graphrag.index.run INFO read table from storage: create_final_text_units.parquet
17:58:12,28 datashaper.workflow.workflow INFO executing verb unroll
17:58:12,47 datashaper.workflow.workflow INFO executing verb select
17:58:12,64 datashaper.workflow.workflow INFO executing verb rename
17:58:12,82 datashaper.workflow.workflow INFO executing verb join
17:58:12,106 datashaper.workflow.workflow INFO executing verb aggregate_override
17:58:12,126 datashaper.workflow.workflow INFO executing verb join
17:58:12,148 datashaper.workflow.workflow INFO executing verb rename
17:58:12,166 datashaper.workflow.workflow INFO executing verb convert
17:58:12,189 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_base_documents.parquet
17:58:12,431 graphrag.index.run INFO Running workflow: create_final_documents...
17:58:12,431 graphrag.index.run INFO dependencies for create_final_documents: ['create_base_documents']
17:58:12,431 graphrag.index.run INFO read table from storage: create_base_documents.parquet
17:58:12,487 datashaper.workflow.workflow INFO executing verb rename
17:58:12,492 graphrag.index.emit.parquet_table_emitter INFO emitting parquet table create_final_documents.parquet
